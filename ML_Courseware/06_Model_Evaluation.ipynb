{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Module 6: Model Evaluation and Validation\n",
                "\n",
                "---\n",
                "\n",
                "Building a model is only half the work. This module focuses on rigorously **evaluating and validating** models to ensure they generalize well to unseen data. We cover evaluation metrics, cross-validation, the bias-variance tradeoff, and hyperparameter tuning.\n",
                "\n",
                "---\n",
                "\n",
                "## Table of Contents\n",
                "\n",
                "1. [Confusion Matrix and Derived Metrics](#1.-Confusion-Matrix-and-Derived-Metrics)\n",
                "2. [ROC Curve and AUC](#2.-ROC-Curve-and-AUC)\n",
                "3. [Cross-Validation](#3.-Cross-Validation)\n",
                "4. [Bias-Variance Tradeoff](#4.-Bias-Variance-Tradeoff)\n",
                "5. [Hyperparameter Tuning](#5.-Hyperparameter-Tuning)\n",
                "6. [Exercises](#6.-Exercises)\n",
                "7. [Summary and Further Reading](#7.-Summary-and-Further-Reading)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "from sklearn.datasets import load_breast_cancer, make_classification\n",
                "from sklearn.model_selection import (train_test_split, cross_val_score,\n",
                "                                     StratifiedKFold, GridSearchCV,\n",
                "                                     RandomizedSearchCV, learning_curve, validation_curve)\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.svm import SVC\n",
                "from sklearn.tree import DecisionTreeClassifier\n",
                "from sklearn.ensemble import RandomForestClassifier\n",
                "from sklearn.neighbors import KNeighborsClassifier\n",
                "from sklearn.metrics import (confusion_matrix, accuracy_score, precision_score,\n",
                "                              recall_score, f1_score, roc_curve, roc_auc_score,\n",
                "                              classification_report, precision_recall_curve)\n",
                "\n",
                "plt.style.use('seaborn-v0_8-whitegrid')\n",
                "np.random.seed(42)\n",
                "\n",
                "# Load and prepare data\n",
                "cancer = load_breast_cancer()\n",
                "X, y = cancer.data, cancer.target\n",
                "X_train, X_test, y_train, y_test = train_test_split(\n",
                "    X, y, test_size=0.2, random_state=42, stratify=y\n",
                ")\n",
                "scaler = StandardScaler()\n",
                "X_train_s = scaler.fit_transform(X_train)\n",
                "X_test_s = scaler.transform(X_test)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 1. Confusion Matrix and Derived Metrics\n",
                "\n",
                "The confusion matrix is the foundation of classification evaluation. It shows how predictions map to actual labels:\n",
                "\n",
                "```\n",
                "                     Predicted\n",
                "                  Neg        Pos\n",
                "Actual  Neg   [ TN      |   FP ]\n",
                "        Pos   [ FN      |   TP ]\n",
                "```\n",
                "\n",
                "| Metric | Formula | Interpretation |\n",
                "|--------|---------|---------------|\n",
                "| **Accuracy** | (TP + TN) / Total | Overall correct predictions |\n",
                "| **Precision** | TP / (TP + FP) | Of predicted positives, how many are correct? |\n",
                "| **Recall (Sensitivity)** | TP / (TP + FN) | Of actual positives, how many did we find? |\n",
                "| **F1 Score** | 2 * P * R / (P + R) | Harmonic mean of precision and recall |\n",
                "| **Specificity** | TN / (TN + FP) | Of actual negatives, how many did we identify? |"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train a model for evaluation\n",
                "lr = LogisticRegression(max_iter=5000, random_state=42)\n",
                "lr.fit(X_train_s, y_train)\n",
                "y_pred = lr.predict(X_test_s)\n",
                "\n",
                "# Confusion matrix\n",
                "cm = confusion_matrix(y_test, y_pred)\n",
                "tn, fp, fn, tp = cm.ravel()\n",
                "\n",
                "print(\"CONFUSION MATRIX\")\n",
                "print(\"=\" * 40)\n",
                "print(f\"                Predicted\")\n",
                "print(f\"             Neg     Pos\")\n",
                "print(f\"Actual Neg   {tn:>4}    {fp:>4}\")\n",
                "print(f\"       Pos   {fn:>4}    {tp:>4}\")\n",
                "\n",
                "print(f\"\\nDERIVED METRICS\")\n",
                "print(\"=\" * 40)\n",
                "print(f\"Accuracy:    {accuracy_score(y_test, y_pred):.4f}\")\n",
                "print(f\"Precision:   {precision_score(y_test, y_pred):.4f}\")\n",
                "print(f\"Recall:      {recall_score(y_test, y_pred):.4f}\")\n",
                "print(f\"F1 Score:    {f1_score(y_test, y_pred):.4f}\")\n",
                "print(f\"Specificity: {tn / (tn + fp):.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize the confusion matrix\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "# Raw counts\n",
                "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
                "            xticklabels=cancer.target_names, yticklabels=cancer.target_names,\n",
                "            square=True, linewidths=1, annot_kws={'size': 16})\n",
                "axes[0].set_xlabel('Predicted', fontsize=13)\n",
                "axes[0].set_ylabel('Actual', fontsize=13)\n",
                "axes[0].set_title('Confusion Matrix (Counts)', fontsize=14, fontweight='bold')\n",
                "\n",
                "# Normalized (percentages)\n",
                "cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
                "sns.heatmap(cm_norm, annot=True, fmt='.2%', cmap='Blues', ax=axes[1],\n",
                "            xticklabels=cancer.target_names, yticklabels=cancer.target_names,\n",
                "            square=True, linewidths=1, annot_kws={'size': 14})\n",
                "axes[1].set_xlabel('Predicted', fontsize=13)\n",
                "axes[1].set_ylabel('Actual', fontsize=13)\n",
                "axes[1].set_title('Confusion Matrix (Normalized)', fontsize=14, fontweight='bold')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### When to Use Which Metric?\n",
                "\n",
                "- **Accuracy** is appropriate when classes are balanced.\n",
                "- **Precision** matters when false positives are costly (e.g., spam detection — do not flag legitimate email).\n",
                "- **Recall** matters when false negatives are costly (e.g., cancer detection — do not miss a positive case).\n",
                "- **F1 Score** provides a balanced measure when both false positives and false negatives are important."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 2. ROC Curve and AUC\n",
                "\n",
                "The **ROC (Receiver Operating Characteristic) curve** plots the True Positive Rate (recall) against the False Positive Rate at various classification thresholds.\n",
                "\n",
                "The **AUC (Area Under the Curve)** summarizes the curve into a single number:\n",
                "- AUC = 1.0: perfect classifier\n",
                "- AUC = 0.5: random classifier (no discrimination)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Get probability scores (not just binary predictions)\n",
                "y_proba = lr.predict_proba(X_test_s)[:, 1]\n",
                "\n",
                "# ROC curve\n",
                "fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n",
                "auc_score = roc_auc_score(y_test, y_proba)\n",
                "\n",
                "# Precision-Recall curve\n",
                "precision_vals, recall_vals, pr_thresholds = precision_recall_curve(y_test, y_proba)\n",
                "\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
                "\n",
                "# ROC Curve\n",
                "ax = axes[0]\n",
                "ax.plot(fpr, tpr, linewidth=2.5, color='#2196F3', label=f'Logistic Regression (AUC = {auc_score:.3f})')\n",
                "ax.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random classifier (AUC = 0.5)')\n",
                "ax.fill_between(fpr, tpr, alpha=0.1, color='#2196F3')\n",
                "ax.set_xlabel('False Positive Rate', fontsize=13)\n",
                "ax.set_ylabel('True Positive Rate (Recall)', fontsize=13)\n",
                "ax.set_title('ROC Curve', fontsize=14, fontweight='bold')\n",
                "ax.legend(fontsize=10, loc='lower right')\n",
                "\n",
                "# Precision-Recall Curve\n",
                "ax = axes[1]\n",
                "ax.plot(recall_vals, precision_vals, linewidth=2.5, color='#FF5722')\n",
                "ax.fill_between(recall_vals, precision_vals, alpha=0.1, color='#FF5722')\n",
                "ax.set_xlabel('Recall', fontsize=13)\n",
                "ax.set_ylabel('Precision', fontsize=13)\n",
                "ax.set_title('Precision-Recall Curve', fontsize=14, fontweight='bold')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Compare ROC curves of multiple classifiers\n",
                "classifiers = {\n",
                "    'Logistic Regression': LogisticRegression(max_iter=5000, random_state=42),\n",
                "    'KNN (K=5)': KNeighborsClassifier(n_neighbors=5),\n",
                "    'Decision Tree': DecisionTreeClassifier(max_depth=5, random_state=42),\n",
                "    'SVM (RBF)': SVC(kernel='rbf', probability=True, random_state=42),\n",
                "}\n",
                "\n",
                "fig, ax = plt.subplots(figsize=(10, 8))\n",
                "colors = ['#2196F3', '#FF5722', '#4CAF50', '#9C27B0']\n",
                "\n",
                "for (name, clf), color in zip(classifiers.items(), colors):\n",
                "    clf.fit(X_train_s, y_train)\n",
                "    y_prob = clf.predict_proba(X_test_s)[:, 1]\n",
                "    fpr_c, tpr_c, _ = roc_curve(y_test, y_prob)\n",
                "    auc_c = roc_auc_score(y_test, y_prob)\n",
                "    ax.plot(fpr_c, tpr_c, linewidth=2, color=color, label=f'{name} (AUC = {auc_c:.3f})')\n",
                "\n",
                "ax.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random (AUC = 0.5)')\n",
                "ax.set_xlabel('False Positive Rate', fontsize=13)\n",
                "ax.set_ylabel('True Positive Rate', fontsize=13)\n",
                "ax.set_title('ROC Curves — Classifier Comparison', fontsize=14, fontweight='bold')\n",
                "ax.legend(fontsize=11)\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 3. Cross-Validation\n",
                "\n",
                "A single train-test split can give unreliable results because it depends on which samples end up in each set. **Cross-validation** addresses this by evaluating the model on multiple different splits.\n",
                "\n",
                "**K-Fold Cross-Validation:**\n",
                "1. Split data into K equal folds.\n",
                "2. For each fold, train on K-1 folds and evaluate on the remaining fold.\n",
                "3. Average the K results."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 5-Fold and 10-Fold cross-validation\n",
                "models = {\n",
                "    'Logistic Regression': LogisticRegression(max_iter=5000, random_state=42),\n",
                "    'KNN (K=5)': KNeighborsClassifier(n_neighbors=5),\n",
                "    'Decision Tree': DecisionTreeClassifier(max_depth=5, random_state=42),\n",
                "    'SVM (RBF)': SVC(kernel='rbf', random_state=42),\n",
                "}\n",
                "\n",
                "print(\"5-Fold Stratified Cross-Validation Results\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "cv_results = []\n",
                "for name, model in models.items():\n",
                "    scores = cross_val_score(model, X_train_s, y_train,\n",
                "                            cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
                "                            scoring='accuracy')\n",
                "    cv_results.append({\n",
                "        'Model': name,\n",
                "        'Mean Accuracy': scores.mean(),\n",
                "        'Std': scores.std(),\n",
                "        'Fold Scores': scores\n",
                "    })\n",
                "    print(f\"  {name:>25s}: {scores.mean():.4f} (+/- {scores.std():.4f})  Folds: {np.round(scores, 3)}\")\n",
                "\n",
                "# Visualize as boxplot\n",
                "fig, ax = plt.subplots(figsize=(10, 6))\n",
                "fold_data = [r['Fold Scores'] for r in cv_results]\n",
                "bp = ax.boxplot(fold_data, labels=[r['Model'] for r in cv_results],\n",
                "                patch_artist=True)\n",
                "colors = ['#2196F3', '#FF5722', '#4CAF50', '#9C27B0']\n",
                "for patch, color in zip(bp['boxes'], colors):\n",
                "    patch.set_facecolor(color)\n",
                "    patch.set_alpha(0.6)\n",
                "ax.set_ylabel('Accuracy', fontsize=13)\n",
                "ax.set_title('Cross-Validation Accuracy Distribution', fontsize=14, fontweight='bold')\n",
                "plt.xticks(rotation=15)\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 4. Bias-Variance Tradeoff\n",
                "\n",
                "Understanding the bias-variance tradeoff is crucial for building models that generalize well.\n",
                "\n",
                "- **Bias**: Error from incorrect assumptions. High bias leads to underfitting.\n",
                "- **Variance**: Error from sensitivity to fluctuations in training data. High variance leads to overfitting.\n",
                "- **Goal**: Find the sweet spot that minimizes total error (bias + variance).\n",
                "\n",
                "**Learning curves** help diagnose whether a model suffers from high bias or high variance."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Learning curves for different model complexities\n",
                "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
                "\n",
                "models_lc = [\n",
                "    ('High Bias (Logistic Reg)', LogisticRegression(max_iter=5000, random_state=42)),\n",
                "    ('Good Fit (SVM RBF)', SVC(kernel='rbf', random_state=42)),\n",
                "    ('High Variance (Deep Tree)', DecisionTreeClassifier(max_depth=None, random_state=42)),\n",
                "]\n",
                "\n",
                "for idx, (name, model) in enumerate(models_lc):\n",
                "    train_sizes, train_scores, val_scores = learning_curve(\n",
                "        model, X_train_s, y_train,\n",
                "        train_sizes=np.linspace(0.1, 1.0, 10),\n",
                "        cv=5, scoring='accuracy', n_jobs=-1\n",
                "    )\n",
                "    \n",
                "    train_mean = train_scores.mean(axis=1)\n",
                "    train_std = train_scores.std(axis=1)\n",
                "    val_mean = val_scores.mean(axis=1)\n",
                "    val_std = val_scores.std(axis=1)\n",
                "    \n",
                "    ax = axes[idx]\n",
                "    ax.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1, color='#2196F3')\n",
                "    ax.fill_between(train_sizes, val_mean - val_std, val_mean + val_std, alpha=0.1, color='#FF5722')\n",
                "    ax.plot(train_sizes, train_mean, 'o-', color='#2196F3', linewidth=2, label='Training score')\n",
                "    ax.plot(train_sizes, val_mean, 's-', color='#FF5722', linewidth=2, label='Validation score')\n",
                "    ax.set_xlabel('Training Set Size', fontsize=12)\n",
                "    ax.set_ylabel('Accuracy', fontsize=12)\n",
                "    ax.set_title(name, fontsize=13, fontweight='bold')\n",
                "    ax.legend(fontsize=10)\n",
                "    ax.set_ylim(0.85, 1.02)\n",
                "\n",
                "plt.suptitle('Learning Curves — Diagnosing Bias vs Variance', fontsize=15, fontweight='bold')\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(\"How to read learning curves:\")\n",
                "print(\"  - High Bias: both curves converge at a low score — model is too simple.\")\n",
                "print(\"  - Good Fit: both curves converge at a high score — model is appropriate.\")\n",
                "print(\"  - High Variance: large gap between curves — model memorizes training data.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Validation curve: how a hyperparameter affects performance\n",
                "param_range = np.arange(1, 25)\n",
                "train_scores_vc, val_scores_vc = validation_curve(\n",
                "    DecisionTreeClassifier(random_state=42), X_train_s, y_train,\n",
                "    param_name='max_depth', param_range=param_range,\n",
                "    cv=5, scoring='accuracy', n_jobs=-1\n",
                ")\n",
                "\n",
                "fig, ax = plt.subplots(figsize=(12, 6))\n",
                "ax.plot(param_range, train_scores_vc.mean(axis=1), 'o-', color='#2196F3',\n",
                "        linewidth=2, label='Training score')\n",
                "ax.plot(param_range, val_scores_vc.mean(axis=1), 's-', color='#FF5722',\n",
                "        linewidth=2, label='Validation score')\n",
                "ax.fill_between(param_range,\n",
                "                train_scores_vc.mean(axis=1) - train_scores_vc.std(axis=1),\n",
                "                train_scores_vc.mean(axis=1) + train_scores_vc.std(axis=1),\n",
                "                alpha=0.1, color='#2196F3')\n",
                "ax.fill_between(param_range,\n",
                "                val_scores_vc.mean(axis=1) - val_scores_vc.std(axis=1),\n",
                "                val_scores_vc.mean(axis=1) + val_scores_vc.std(axis=1),\n",
                "                alpha=0.1, color='#FF5722')\n",
                "ax.set_xlabel('max_depth', fontsize=13)\n",
                "ax.set_ylabel('Accuracy', fontsize=13)\n",
                "ax.set_title('Validation Curve — Decision Tree max_depth', fontsize=14, fontweight='bold')\n",
                "ax.legend(fontsize=11)\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "best_depth = param_range[np.argmax(val_scores_vc.mean(axis=1))]\n",
                "print(f\"Optimal max_depth: {best_depth}\")\n",
                "print(\"Beyond this depth, validation accuracy plateaus or drops while training accuracy keeps rising — overfitting.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 5. Hyperparameter Tuning\n",
                "\n",
                "Most ML algorithms have hyperparameters that must be set before training. Finding the best combination is called **hyperparameter tuning**.\n",
                "\n",
                "| Method | How It Works | Pros | Cons |\n",
                "|--------|-------------|------|------|\n",
                "| **GridSearchCV** | Tries every combination | Thorough | Slow for many parameters |\n",
                "| **RandomizedSearchCV** | Samples random combinations | Faster | May miss the best |\n",
                "\n",
                "Both use cross-validation internally to evaluate each combination."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# GridSearchCV for SVM\n",
                "param_grid = {\n",
                "    'C': [0.01, 0.1, 1, 10, 100],\n",
                "    'gamma': ['scale', 'auto', 0.01, 0.1, 1],\n",
                "    'kernel': ['rbf', 'linear']\n",
                "}\n",
                "\n",
                "grid_search = GridSearchCV(\n",
                "    SVC(random_state=42),\n",
                "    param_grid,\n",
                "    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
                "    scoring='accuracy',\n",
                "    n_jobs=-1,\n",
                "    verbose=0\n",
                ")\n",
                "\n",
                "grid_search.fit(X_train_s, y_train)\n",
                "\n",
                "print(\"GRID SEARCH RESULTS — SVM\")\n",
                "print(\"=\" * 50)\n",
                "print(f\"Best parameters:     {grid_search.best_params_}\")\n",
                "print(f\"Best CV accuracy:    {grid_search.best_score_:.4f}\")\n",
                "print(f\"Test set accuracy:   {grid_search.score(X_test_s, y_test):.4f}\")\n",
                "\n",
                "# Show top 5 parameter combinations\n",
                "results = pd.DataFrame(grid_search.cv_results_)\n",
                "top5 = results.nsmallest(5, 'rank_test_score')[['params', 'mean_test_score', 'std_test_score', 'rank_test_score']]\n",
                "print(\"\\nTop 5 Configurations:\")\n",
                "for _, row in top5.iterrows():\n",
                "    print(f\"  Rank {int(row['rank_test_score'])}:  {row['mean_test_score']:.4f} +/- {row['std_test_score']:.4f}  {row['params']}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# RandomizedSearchCV for Random Forest\n",
                "from scipy.stats import randint, uniform\n",
                "\n",
                "param_distributions = {\n",
                "    'n_estimators': randint(50, 300),\n",
                "    'max_depth': randint(3, 20),\n",
                "    'min_samples_split': randint(2, 15),\n",
                "    'min_samples_leaf': randint(1, 10),\n",
                "    'max_features': ['sqrt', 'log2', None]\n",
                "}\n",
                "\n",
                "random_search = RandomizedSearchCV(\n",
                "    RandomForestClassifier(random_state=42),\n",
                "    param_distributions,\n",
                "    n_iter=50,\n",
                "    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
                "    scoring='accuracy',\n",
                "    n_jobs=-1,\n",
                "    random_state=42,\n",
                "    verbose=0\n",
                ")\n",
                "\n",
                "random_search.fit(X_train_s, y_train)\n",
                "\n",
                "print(\"RANDOMIZED SEARCH RESULTS — Random Forest\")\n",
                "print(\"=\" * 50)\n",
                "print(f\"Best parameters:     {random_search.best_params_}\")\n",
                "print(f\"Best CV accuracy:    {random_search.best_score_:.4f}\")\n",
                "print(f\"Test set accuracy:   {random_search.score(X_test_s, y_test):.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 6. Exercises\n",
                "\n",
                "### Exercise 1: Evaluate on an Imbalanced Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Exercise 1: Create an imbalanced dataset and evaluate a classifier\n",
                "#\n",
                "# 1. Generate imbalanced data using make_classification with weights=[0.9, 0.1]\n",
                "# 2. Split into train/test\n",
                "# 3. Train a LogisticRegression\n",
                "# 4. Compute accuracy, precision, recall, and F1\n",
                "# 5. Why is accuracy misleading here? Which metric is more informative?\n",
                "# 6. Plot the confusion matrix and ROC curve\n",
                "\n",
                "X_imb, y_imb = make_classification(n_samples=1000, weights=[0.9, 0.1],\n",
                "                                    random_state=42, n_features=10)\n",
                "\n",
                "# Your code here:\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Exercise 2: Cross-Validation Comparison"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Exercise 2: Using the Breast Cancer dataset:\n",
                "# 1. Run 10-fold cross-validation for at least 3 different classifiers\n",
                "# 2. Use 'f1' as the scoring metric instead of 'accuracy'\n",
                "# 3. Print mean and std for each model\n",
                "# 4. Create a bar chart comparing mean F1 scores with error bars (std)\n",
                "\n",
                "# Your code here:\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Exercise 3: Hyperparameter Tuning for KNN"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Exercise 3: Use GridSearchCV to find the best hyperparameters for KNN:\n",
                "# Search over:\n",
                "#   - n_neighbors: [1, 3, 5, 7, 9, 11, 15, 20]\n",
                "#   - weights: ['uniform', 'distance']\n",
                "#   - metric: ['euclidean', 'manhattan']\n",
                "# \n",
                "# Report the best parameters, best CV score, and test accuracy.\n",
                "\n",
                "# Your code here:\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 7. Summary and Further Reading\n",
                "\n",
                "### What We Covered\n",
                "\n",
                "- **Confusion Matrix**: The foundation of classification evaluation (TP, TN, FP, FN).\n",
                "- **Precision, Recall, F1**: Metrics that go beyond accuracy, especially for imbalanced data.\n",
                "- **ROC Curve and AUC**: Threshold-independent evaluation of classifier performance.\n",
                "- **Cross-Validation**: Multiple train/test splits for more reliable evaluation.\n",
                "- **Bias-Variance Tradeoff**: Diagnosing underfitting and overfitting with learning curves.\n",
                "- **Hyperparameter Tuning**: GridSearchCV and RandomizedSearchCV to find optimal model settings.\n",
                "\n",
                "### Recommended Reading\n",
                "\n",
                "- [Scikit-learn Model Evaluation Guide](https://scikit-learn.org/stable/modules/model_evaluation.html)\n",
                "- [Scikit-learn Cross-Validation Guide](https://scikit-learn.org/stable/modules/cross_validation.html)\n",
                "- Chapter 2 of Aurélien Géron, *Hands-On Machine Learning* (end-to-end project with evaluation)\n",
                "- Chapter 5 of *ISLR* — Resampling Methods\n",
                "\n",
                "### Next Module\n",
                "\n",
                "In **Module 7: Unsupervised Learning**, we will explore algorithms that work with unlabeled data: K-Means clustering, hierarchical clustering, DBSCAN, and dimensionality reduction with PCA and t-SNE.\n",
                "\n",
                "---"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.9.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}