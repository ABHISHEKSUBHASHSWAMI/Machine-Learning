{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Module 7: Unsupervised Learning\n",
                "\n",
                "---\n",
                "\n",
                "Unsupervised learning discovers hidden patterns in data **without labeled outputs**. This module covers the two major categories: **clustering** (grouping similar data points) and **dimensionality reduction** (compressing high-dimensional data into fewer dimensions while preserving structure).\n",
                "\n",
                "---\n",
                "\n",
                "## Table of Contents\n",
                "\n",
                "1. [K-Means Clustering](#1.-K-Means-Clustering)\n",
                "2. [Hierarchical Clustering](#2.-Hierarchical-Clustering)\n",
                "3. [DBSCAN](#3.-DBSCAN)\n",
                "4. [Dimensionality Reduction: PCA](#4.-Dimensionality-Reduction:-PCA)\n",
                "5. [Visualization with t-SNE](#5.-Visualization-with-t-SNE)\n",
                "6. [Exercises](#6.-Exercises)\n",
                "7. [Summary and Further Reading](#7.-Summary-and-Further-Reading)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "from sklearn.datasets import make_blobs, make_moons, load_iris, load_digits\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
                "from sklearn.decomposition import PCA\n",
                "from sklearn.manifold import TSNE\n",
                "from sklearn.metrics import silhouette_score, silhouette_samples\n",
                "from scipy.cluster.hierarchy import dendrogram, linkage\n",
                "\n",
                "plt.style.use('seaborn-v0_8-whitegrid')\n",
                "np.random.seed(42)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 1. K-Means Clustering\n",
                "\n",
                "K-Means partitions data into **K clusters** by iteratively:\n",
                "1. Assigning each point to the nearest centroid.\n",
                "2. Updating centroids as the mean of all assigned points.\n",
                "3. Repeating until convergence.\n",
                "\n",
                "The key challenge: you must choose K in advance."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate synthetic data with 4 clusters\n",
                "X_blobs, y_true = make_blobs(n_samples=300, centers=4, cluster_std=0.8, random_state=42)\n",
                "\n",
                "# Apply K-Means\n",
                "kmeans = KMeans(n_clusters=4, random_state=42, n_init=10)\n",
                "y_kmeans = kmeans.fit_predict(X_blobs)\n",
                "\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "# Before clustering (unlabeled)\n",
                "axes[0].scatter(X_blobs[:, 0], X_blobs[:, 1], c='gray', s=30, alpha=0.6, edgecolors='white', linewidth=0.3)\n",
                "axes[0].set_title('Raw Data (No Labels)', fontsize=14, fontweight='bold')\n",
                "axes[0].set_xlabel('Feature 1')\n",
                "axes[0].set_ylabel('Feature 2')\n",
                "\n",
                "# After clustering\n",
                "scatter = axes[1].scatter(X_blobs[:, 0], X_blobs[:, 1], c=y_kmeans, cmap='viridis',\n",
                "                          s=30, alpha=0.7, edgecolors='white', linewidth=0.3)\n",
                "axes[1].scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],\n",
                "                c='red', marker='X', s=200, edgecolors='black', linewidth=1, label='Centroids')\n",
                "axes[1].set_title('K-Means Clustering (K=4)', fontsize=14, fontweight='bold')\n",
                "axes[1].set_xlabel('Feature 1')\n",
                "axes[1].set_ylabel('Feature 2')\n",
                "axes[1].legend(fontsize=11)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# The Elbow Method: choosing the optimal K\n",
                "inertias = []\n",
                "silhouette_scores = []\n",
                "K_range = range(2, 11)\n",
                "\n",
                "for k in K_range:\n",
                "    km = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
                "    km.fit(X_blobs)\n",
                "    inertias.append(km.inertia_)\n",
                "    silhouette_scores.append(silhouette_score(X_blobs, km.labels_))\n",
                "\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "# Elbow plot\n",
                "axes[0].plot(K_range, inertias, 'o-', linewidth=2, color='#2196F3', markersize=8)\n",
                "axes[0].axvline(x=4, color='red', linestyle='--', alpha=0.7, label='Elbow at K=4')\n",
                "axes[0].set_xlabel('Number of Clusters (K)', fontsize=13)\n",
                "axes[0].set_ylabel('Inertia (Within-Cluster Sum of Squares)', fontsize=13)\n",
                "axes[0].set_title('Elbow Method', fontsize=14, fontweight='bold')\n",
                "axes[0].legend(fontsize=11)\n",
                "\n",
                "# Silhouette score\n",
                "axes[1].plot(K_range, silhouette_scores, 's-', linewidth=2, color='#FF5722', markersize=8)\n",
                "axes[1].axvline(x=4, color='red', linestyle='--', alpha=0.7, label='Best K=4')\n",
                "axes[1].set_xlabel('Number of Clusters (K)', fontsize=13)\n",
                "axes[1].set_ylabel('Silhouette Score', fontsize=13)\n",
                "axes[1].set_title('Silhouette Score', fontsize=14, fontweight='bold')\n",
                "axes[1].legend(fontsize=11)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(\"The Elbow Method: look for the 'bend' where adding more clusters gives diminishing returns.\")\n",
                "print(\"Silhouette Score: ranges from -1 to 1. Higher is better (well-separated clusters).\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 2. Hierarchical Clustering\n",
                "\n",
                "Hierarchical clustering builds a tree (dendrogram) of cluster merges. It does not require specifying K in advance — you can cut the tree at any level to get the desired number of clusters.\n",
                "\n",
                "**Agglomerative (bottom-up)**: Start with each point as its own cluster, then merge the closest pairs."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Dendrogram\n",
                "X_small = X_blobs[:50]  # use a subset for readable dendrogram\n",
                "\n",
                "linkage_matrix = linkage(X_small, method='ward')\n",
                "\n",
                "fig, ax = plt.subplots(figsize=(16, 6))\n",
                "dendrogram(linkage_matrix, ax=ax, leaf_rotation=90, leaf_font_size=8,\n",
                "           color_threshold=5)\n",
                "ax.set_title('Hierarchical Clustering Dendrogram', fontsize=14, fontweight='bold')\n",
                "ax.set_xlabel('Sample Index', fontsize=13)\n",
                "ax.set_ylabel('Distance', fontsize=13)\n",
                "ax.axhline(y=5, color='red', linestyle='--', alpha=0.7, label='Cut threshold')\n",
                "ax.legend(fontsize=11)\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(\"The dendrogram shows how clusters are merged at each step.\")\n",
                "print(\"Cutting at a specific height determines the number of clusters.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Agglomerative Clustering on the full dataset\n",
                "agg = AgglomerativeClustering(n_clusters=4, linkage='ward')\n",
                "y_agg = agg.fit_predict(X_blobs)\n",
                "\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "axes[0].scatter(X_blobs[:, 0], X_blobs[:, 1], c=y_kmeans, cmap='viridis',\n",
                "                s=30, alpha=0.7, edgecolors='white', linewidth=0.3)\n",
                "axes[0].set_title('K-Means (K=4)', fontsize=14, fontweight='bold')\n",
                "\n",
                "axes[1].scatter(X_blobs[:, 0], X_blobs[:, 1], c=y_agg, cmap='viridis',\n",
                "                s=30, alpha=0.7, edgecolors='white', linewidth=0.3)\n",
                "axes[1].set_title('Agglomerative Clustering (K=4)', fontsize=14, fontweight='bold')\n",
                "\n",
                "for ax in axes:\n",
                "    ax.set_xlabel('Feature 1')\n",
                "    ax.set_ylabel('Feature 2')\n",
                "\n",
                "plt.suptitle('K-Means vs Hierarchical Clustering', fontsize=15, fontweight='bold')\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 3. DBSCAN\n",
                "\n",
                "**DBSCAN (Density-Based Spatial Clustering of Applications with Noise)** finds arbitrarily shaped clusters based on density. Unlike K-Means, it:\n",
                "- Does not require specifying the number of clusters\n",
                "- Can identify noise points (outliers)\n",
                "- Handles non-spherical cluster shapes\n",
                "\n",
                "Key parameters:\n",
                "- **eps**: Maximum distance between two points to be considered neighbors\n",
                "- **min_samples**: Minimum number of points to form a dense region"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate non-spherical data (Moons)\n",
                "X_moons, y_moons = make_moons(n_samples=300, noise=0.1, random_state=42)\n",
                "\n",
                "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
                "\n",
                "# K-Means (fails on non-spherical shapes)\n",
                "km_moons = KMeans(n_clusters=2, random_state=42, n_init=10)\n",
                "y_km_moons = km_moons.fit_predict(X_moons)\n",
                "axes[0].scatter(X_moons[:, 0], X_moons[:, 1], c=y_km_moons, cmap='viridis',\n",
                "                s=30, edgecolors='white', linewidth=0.3)\n",
                "axes[0].set_title('K-Means (K=2)', fontsize=13, fontweight='bold')\n",
                "\n",
                "# DBSCAN (handles non-spherical shapes)\n",
                "dbscan = DBSCAN(eps=0.2, min_samples=5)\n",
                "y_db = dbscan.fit_predict(X_moons)\n",
                "axes[1].scatter(X_moons[:, 0], X_moons[:, 1], c=y_db, cmap='viridis',\n",
                "                s=30, edgecolors='white', linewidth=0.3)\n",
                "n_noise = (y_db == -1).sum()\n",
                "axes[1].set_title(f'DBSCAN (eps=0.2)\\nNoise points: {n_noise}', fontsize=13, fontweight='bold')\n",
                "\n",
                "# True labels\n",
                "axes[2].scatter(X_moons[:, 0], X_moons[:, 1], c=y_moons, cmap='viridis',\n",
                "                s=30, edgecolors='white', linewidth=0.3)\n",
                "axes[2].set_title('True Labels', fontsize=13, fontweight='bold')\n",
                "\n",
                "for ax in axes:\n",
                "    ax.set_xlabel('Feature 1')\n",
                "    ax.set_ylabel('Feature 2')\n",
                "\n",
                "plt.suptitle('DBSCAN Handles Non-Spherical Clusters', fontsize=15, fontweight='bold')\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(\"K-Means assumes spherical clusters and fails on the Moons data.\")\n",
                "print(\"DBSCAN correctly identifies the two interleaved half-circles.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 4. Dimensionality Reduction: PCA\n",
                "\n",
                "**Principal Component Analysis (PCA)** reduces the number of features by projecting data onto the directions of maximum variance. Uses:\n",
                "- Visualization of high-dimensional data\n",
                "- Noise reduction\n",
                "- Speeding up ML algorithms\n",
                "- Feature extraction"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# PCA on the Iris dataset\n",
                "iris = load_iris()\n",
                "X_iris = StandardScaler().fit_transform(iris.data)\n",
                "\n",
                "# Reduce from 4D to 2D\n",
                "pca = PCA(n_components=2)\n",
                "X_pca = pca.fit_transform(X_iris)\n",
                "\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "# PCA projection\n",
                "for i, name in enumerate(iris.target_names):\n",
                "    mask = iris.target == i\n",
                "    axes[0].scatter(X_pca[mask, 0], X_pca[mask, 1], label=name, s=40,\n",
                "                    edgecolors='white', linewidth=0.3, alpha=0.7)\n",
                "axes[0].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)', fontsize=12)\n",
                "axes[0].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)', fontsize=12)\n",
                "axes[0].set_title('Iris Dataset — PCA (4D to 2D)', fontsize=14, fontweight='bold')\n",
                "axes[0].legend(fontsize=10)\n",
                "\n",
                "# Explained variance\n",
                "pca_full = PCA().fit(X_iris)\n",
                "cumulative_var = np.cumsum(pca_full.explained_variance_ratio_)\n",
                "axes[1].bar(range(1, 5), pca_full.explained_variance_ratio_, color='#2196F3',\n",
                "            edgecolor='white', alpha=0.7, label='Individual')\n",
                "axes[1].plot(range(1, 5), cumulative_var, 'ro-', linewidth=2, markersize=8, label='Cumulative')\n",
                "axes[1].axhline(y=0.95, color='gray', linestyle='--', alpha=0.5, label='95% threshold')\n",
                "axes[1].set_xlabel('Principal Component', fontsize=12)\n",
                "axes[1].set_ylabel('Explained Variance Ratio', fontsize=12)\n",
                "axes[1].set_title('Explained Variance by Component', fontsize=14, fontweight='bold')\n",
                "axes[1].set_xticks(range(1, 5))\n",
                "axes[1].legend(fontsize=10)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(f\"Explained variance by component: {pca_full.explained_variance_ratio_.round(3)}\")\n",
                "print(f\"Cumulative: {cumulative_var.round(3)}\")\n",
                "print(f\"\\nThe first 2 components capture {cumulative_var[1]:.1%} of the total variance.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# PCA on a higher-dimensional dataset (Digits)\n",
                "digits = load_digits()\n",
                "X_digits = StandardScaler().fit_transform(digits.data)\n",
                "\n",
                "print(f\"Digits dataset: {digits.data.shape[0]} samples, {digits.data.shape[1]} features\")\n",
                "print(f\"Each sample is an 8x8 pixel image of a handwritten digit (0-9)\")\n",
                "\n",
                "# Reduce 64D to 2D for visualization\n",
                "pca_digits = PCA(n_components=2)\n",
                "X_digits_2d = pca_digits.fit_transform(X_digits)\n",
                "\n",
                "fig, ax = plt.subplots(figsize=(10, 8))\n",
                "scatter = ax.scatter(X_digits_2d[:, 0], X_digits_2d[:, 1], c=digits.target,\n",
                "                     cmap='tab10', s=10, alpha=0.6)\n",
                "plt.colorbar(scatter, ax=ax, label='Digit')\n",
                "ax.set_xlabel(f'PC1 ({pca_digits.explained_variance_ratio_[0]:.1%})', fontsize=13)\n",
                "ax.set_ylabel(f'PC2 ({pca_digits.explained_variance_ratio_[1]:.1%})', fontsize=13)\n",
                "ax.set_title('Digits Dataset — PCA (64D to 2D)', fontsize=14, fontweight='bold')\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 5. Visualization with t-SNE\n",
                "\n",
                "**t-SNE (t-distributed Stochastic Neighbor Embedding)** is a nonlinear dimensionality reduction technique designed specifically for **visualization**. It preserves local structure, making clusters more visually apparent.\n",
                "\n",
                "Key differences from PCA:\n",
                "- PCA is linear; t-SNE is nonlinear\n",
                "- PCA preserves global variance; t-SNE preserves local neighborhoods\n",
                "- t-SNE is used for visualization only (not for feature extraction)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# t-SNE on Digits dataset\n",
                "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
                "X_digits_tsne = tsne.fit_transform(X_digits)\n",
                "\n",
                "fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
                "\n",
                "# PCA\n",
                "scatter1 = axes[0].scatter(X_digits_2d[:, 0], X_digits_2d[:, 1], c=digits.target,\n",
                "                           cmap='tab10', s=10, alpha=0.6)\n",
                "axes[0].set_title('PCA (64D to 2D)', fontsize=14, fontweight='bold')\n",
                "\n",
                "# t-SNE\n",
                "scatter2 = axes[1].scatter(X_digits_tsne[:, 0], X_digits_tsne[:, 1], c=digits.target,\n",
                "                           cmap='tab10', s=10, alpha=0.6)\n",
                "axes[1].set_title('t-SNE (64D to 2D)', fontsize=14, fontweight='bold')\n",
                "\n",
                "for ax in axes:\n",
                "    ax.set_xlabel('Component 1')\n",
                "    ax.set_ylabel('Component 2')\n",
                "\n",
                "plt.colorbar(scatter2, ax=axes[1], label='Digit')\n",
                "plt.suptitle('PCA vs t-SNE on Handwritten Digits', fontsize=15, fontweight='bold')\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(\"t-SNE produces much better-separated clusters for visualization purposes.\")\n",
                "print(\"However, the axes have no interpretable meaning (unlike PCA components).\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 6. Exercises\n",
                "\n",
                "### Exercise 1: Customer Segmentation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Exercise 1: Segment synthetic customer data\n",
                "#\n",
                "# 1. Generate customer data with make_blobs (n_samples=500, centers=5)\n",
                "# 2. Use the Elbow Method to determine the optimal K\n",
                "# 3. Apply K-Means with the optimal K\n",
                "# 4. Compute and print the silhouette score\n",
                "# 5. Visualize the clusters with centroids\n",
                "\n",
                "# Your code here:\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Exercise 2: DBSCAN Parameter Sensitivity"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Exercise 2: Explore DBSCAN's sensitivity to eps parameter\n",
                "#\n",
                "# Using the make_moons data created earlier:\n",
                "# 1. Try eps values: [0.05, 0.1, 0.2, 0.3, 0.5]\n",
                "# 2. For each, run DBSCAN and record: number of clusters, number of noise points\n",
                "# 3. Create a 1x5 subplot showing the clustering result for each eps\n",
                "# 4. Which eps value gives the best result? Why?\n",
                "\n",
                "# Your code here:\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Exercise 3: PCA for Model Speedup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Exercise 3: Test whether PCA can speed up training without losing accuracy\n",
                "#\n",
                "# Using the Digits dataset:\n",
                "# 1. Train a KNN classifier (K=5) on the original 64D data and measure accuracy + time\n",
                "# 2. Use PCA to reduce to 20 dimensions, then train KNN and measure accuracy + time\n",
                "# 3. Try 10 dimensions as well\n",
                "# 4. Compare accuracy and training time\n",
                "# 5. What is the minimum number of PCA components that retains > 95% accuracy?\n",
                "#\n",
                "# Hint: use time.time() or %%timeit to measure execution time\n",
                "\n",
                "import time\n",
                "from sklearn.neighbors import KNeighborsClassifier\n",
                "\n",
                "# Your code here:\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 7. Summary and Further Reading\n",
                "\n",
                "### What We Covered\n",
                "\n",
                "| Algorithm | Type | Key Feature | Limitation |\n",
                "|-----------|------|-------------|------------|\n",
                "| K-Means | Clustering | Fast, simple | Must specify K, assumes spherical clusters |\n",
                "| Hierarchical | Clustering | Dendrogram visualization | Slow on large datasets |\n",
                "| DBSCAN | Clustering | Finds arbitrary shapes, detects noise | Sensitive to eps and min_samples |\n",
                "| PCA | Dim. Reduction | Linear, interpretable, fast | Only captures linear relationships |\n",
                "| t-SNE | Visualization | Excellent cluster separation in 2D | Slow, non-deterministic, visualization only |\n",
                "\n",
                "### Recommended Reading\n",
                "\n",
                "- [Scikit-learn Clustering Guide](https://scikit-learn.org/stable/modules/clustering.html)\n",
                "- [Scikit-learn Decomposition Guide](https://scikit-learn.org/stable/modules/decomposition.html)\n",
                "- Chapter 8 of Aurélien Géron, *Hands-On Machine Learning* (Dimensionality Reduction)\n",
                "- Chapter 9 of Aurélien Géron, *Hands-On Machine Learning* (Unsupervised Learning)\n",
                "\n",
                "### Next Module\n",
                "\n",
                "In **Module 8: Ensemble Methods**, we will learn how combining multiple models (bagging, boosting, stacking) can dramatically improve predictive performance.\n",
                "\n",
                "---"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.9.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}