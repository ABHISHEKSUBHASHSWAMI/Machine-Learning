{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Module 2: Mathematical Foundations for Machine Learning\n",
                "\n",
                "---\n",
                "\n",
                "Machine Learning algorithms are built on mathematical principles. This module covers the core areas of mathematics you need to understand: **linear algebra**, **statistics**, **probability**, and the intuition behind **calculus** (gradients). We will use Python and NumPy to make every concept concrete.\n",
                "\n",
                "You do not need to be a mathematician — the goal is to build intuition and working fluency.\n",
                "\n",
                "---\n",
                "\n",
                "## Table of Contents\n",
                "\n",
                "1. [Linear Algebra Essentials](#1.-Linear-Algebra-Essentials)\n",
                "2. [Statistics Refresher](#2.-Statistics-Refresher)\n",
                "3. [Probability Basics](#3.-Probability-Basics)\n",
                "4. [Calculus Intuition: Gradients and Optimization](#4.-Calculus-Intuition)\n",
                "5. [Exercises](#5.-Exercises)\n",
                "6. [Summary and Further Reading](#6.-Summary-and-Further-Reading)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Libraries used throughout this module\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from scipy import stats\n",
                "\n",
                "plt.style.use('seaborn-v0_8-whitegrid')\n",
                "np.random.seed(42)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 1. Linear Algebra Essentials\n",
                "\n",
                "Linear algebra is the backbone of most ML algorithms. Data is represented as matrices, and transformations on that data are expressed as matrix operations.\n",
                "\n",
                "### 1.1 Scalars, Vectors, and Matrices\n",
                "\n",
                "| Object | Description | Example |\n",
                "|--------|------------|--------|\n",
                "| **Scalar** | A single number | Temperature: 25.3 |\n",
                "| **Vector** | An ordered list of numbers (1D) | Feature vector: [5.1, 3.5, 1.4, 0.2] |\n",
                "| **Matrix** | A 2D array of numbers (rows x columns) | A dataset of 100 samples with 4 features |"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Scalars, Vectors, Matrices in NumPy\n",
                "\n",
                "# Scalar\n",
                "scalar = 5.0\n",
                "print(f\"Scalar: {scalar}\")\n",
                "print(f\"  Type: {type(scalar)}\")\n",
                "\n",
                "# Vector (1D array)\n",
                "vector = np.array([5.1, 3.5, 1.4, 0.2])\n",
                "print(f\"\\nVector: {vector}\")\n",
                "print(f\"  Shape: {vector.shape}\")\n",
                "print(f\"  Dimension: {vector.ndim}\")\n",
                "\n",
                "# Matrix (2D array)\n",
                "matrix = np.array([\n",
                "    [1, 2, 3],\n",
                "    [4, 5, 6],\n",
                "    [7, 8, 9]\n",
                "])\n",
                "print(f\"\\nMatrix:\\n{matrix}\")\n",
                "print(f\"  Shape: {matrix.shape}  (rows x columns)\")\n",
                "print(f\"  Dimension: {matrix.ndim}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 1.2 Vector Operations\n",
                "\n",
                "These operations appear constantly in ML — from computing distances between data points to calculating model predictions."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "a = np.array([1, 2, 3])\n",
                "b = np.array([4, 5, 6])\n",
                "\n",
                "print(\"Vector a:\", a)\n",
                "print(\"Vector b:\", b)\n",
                "print()\n",
                "\n",
                "# Element-wise operations\n",
                "print(\"Addition (a + b):       \", a + b)\n",
                "print(\"Subtraction (a - b):    \", a - b)\n",
                "print(\"Element-wise multiply:  \", a * b)\n",
                "\n",
                "# Dot product: sum of element-wise products\n",
                "dot_product = np.dot(a, b)  # 1*4 + 2*5 + 3*6 = 32\n",
                "print(f\"\\nDot product (a . b):    {dot_product}\")\n",
                "print(f\"  Manual check: 1*4 + 2*5 + 3*6 = {1*4 + 2*5 + 3*6}\")\n",
                "\n",
                "# Vector magnitude (L2 norm)\n",
                "magnitude_a = np.linalg.norm(a)\n",
                "print(f\"\\nMagnitude of a (||a||): {magnitude_a:.4f}\")\n",
                "print(f\"  Manual check: sqrt(1^2 + 2^2 + 3^2) = {np.sqrt(1+4+9):.4f}\")\n",
                "\n",
                "# Euclidean distance between two vectors\n",
                "distance = np.linalg.norm(a - b)\n",
                "print(f\"\\nEuclidean distance between a and b: {distance:.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualizing vectors in 2D\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "# Vector addition\n",
                "v1 = np.array([2, 1])\n",
                "v2 = np.array([1, 3])\n",
                "v_sum = v1 + v2\n",
                "\n",
                "ax = axes[0]\n",
                "ax.quiver(0, 0, v1[0], v1[1], angles='xy', scale_units='xy', scale=1,\n",
                "          color='#2196F3', linewidth=2, label=f'a = {list(v1)}')\n",
                "ax.quiver(0, 0, v2[0], v2[1], angles='xy', scale_units='xy', scale=1,\n",
                "          color='#FF5722', linewidth=2, label=f'b = {list(v2)}')\n",
                "ax.quiver(0, 0, v_sum[0], v_sum[1], angles='xy', scale_units='xy', scale=1,\n",
                "          color='#4CAF50', linewidth=2, label=f'a + b = {list(v_sum)}')\n",
                "# Parallelogram\n",
                "ax.plot([v1[0], v_sum[0]], [v1[1], v_sum[1]], 'k--', alpha=0.3)\n",
                "ax.plot([v2[0], v_sum[0]], [v2[1], v_sum[1]], 'k--', alpha=0.3)\n",
                "ax.set_xlim(-0.5, 5)\n",
                "ax.set_ylim(-0.5, 5)\n",
                "ax.set_aspect('equal')\n",
                "ax.grid(True, alpha=0.3)\n",
                "ax.legend(fontsize=11)\n",
                "ax.set_title('Vector Addition', fontsize=14, fontweight='bold')\n",
                "ax.set_xlabel('x')\n",
                "ax.set_ylabel('y')\n",
                "\n",
                "# Dot product and angle\n",
                "ax = axes[1]\n",
                "v3 = np.array([3, 1])\n",
                "v4 = np.array([1, 3])\n",
                "cos_angle = np.dot(v3, v4) / (np.linalg.norm(v3) * np.linalg.norm(v4))\n",
                "angle = np.arccos(cos_angle)\n",
                "\n",
                "ax.quiver(0, 0, v3[0], v3[1], angles='xy', scale_units='xy', scale=1,\n",
                "          color='#2196F3', linewidth=2, label=f'a = {list(v3)}')\n",
                "ax.quiver(0, 0, v4[0], v4[1], angles='xy', scale_units='xy', scale=1,\n",
                "          color='#FF5722', linewidth=2, label=f'b = {list(v4)}')\n",
                "\n",
                "theta = np.linspace(np.arctan2(v3[1], v3[0]), np.arctan2(v4[1], v4[0]), 30)\n",
                "ax.plot(0.8 * np.cos(theta), 0.8 * np.sin(theta), 'k-', linewidth=1.5)\n",
                "mid_angle = (np.arctan2(v3[1], v3[0]) + np.arctan2(v4[1], v4[0])) / 2\n",
                "ax.text(1.1 * np.cos(mid_angle), 1.1 * np.sin(mid_angle),\n",
                "        f'{np.degrees(angle):.1f} deg', fontsize=11, ha='center')\n",
                "\n",
                "ax.set_xlim(-0.5, 4)\n",
                "ax.set_ylim(-0.5, 4)\n",
                "ax.set_aspect('equal')\n",
                "ax.grid(True, alpha=0.3)\n",
                "ax.legend(fontsize=11)\n",
                "ax.set_title(f'Dot Product and Angle\\na . b = {np.dot(v3, v4)}', fontsize=14, fontweight='bold')\n",
                "ax.set_xlabel('x')\n",
                "ax.set_ylabel('y')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 1.3 Matrix Operations\n",
                "\n",
                "In ML, a dataset is typically stored as a matrix where rows represent samples and columns represent features."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "A = np.array([[1, 2], [3, 4], [5, 6]])\n",
                "B = np.array([[7, 8], [9, 10]])\n",
                "\n",
                "print(\"Matrix A (3x2):\")\n",
                "print(A)\n",
                "print(f\"\\nMatrix B (2x2):\")\n",
                "print(B)\n",
                "\n",
                "# Transpose: swap rows and columns\n",
                "print(f\"\\nTranspose of A (2x3):\")\n",
                "print(A.T)\n",
                "\n",
                "# Matrix multiplication: A (3x2) @ B (2x2) = result (3x2)\n",
                "C = A @ B  # or np.dot(A, B)\n",
                "print(f\"\\nA @ B (matrix multiplication):\")\n",
                "print(C)\n",
                "print(f\"  Shape: {C.shape}\")\n",
                "\n",
                "# Identity matrix\n",
                "I = np.eye(3)\n",
                "print(f\"\\nIdentity matrix (3x3):\")\n",
                "print(I)\n",
                "print(\"Any matrix times the identity matrix returns itself (like multiplying by 1).\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Matrix inverse and solving linear systems\n",
                "# This is directly relevant to Linear Regression (Module 4)\n",
                "\n",
                "M = np.array([[2, 1], [5, 3]])\n",
                "print(\"Matrix M:\")\n",
                "print(M)\n",
                "\n",
                "# Inverse\n",
                "M_inv = np.linalg.inv(M)\n",
                "print(f\"\\nInverse of M:\")\n",
                "print(M_inv)\n",
                "\n",
                "# Verify: M @ M_inv should equal the identity matrix\n",
                "print(f\"\\nM @ M_inv (should be identity):\")\n",
                "print(np.round(M @ M_inv, decimals=10))\n",
                "\n",
                "# Determinant\n",
                "det = np.linalg.det(M)\n",
                "print(f\"\\nDeterminant of M: {det:.4f}\")\n",
                "print(\"A matrix is invertible only if its determinant is non-zero.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 1.4 Why Linear Algebra Matters in ML\n",
                "\n",
                "- **Data representation**: A dataset with n samples and p features is an n x p matrix.\n",
                "- **Linear Regression**: The closed-form solution involves matrix operations: w = (X^T X)^{-1} X^T y\n",
                "- **PCA (dimensionality reduction)**: Uses eigenvalue decomposition of the covariance matrix.\n",
                "- **Neural Networks**: Every layer computes a matrix multiplication followed by a nonlinear activation."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 2. Statistics Refresher\n",
                "\n",
                "Statistics helps us understand the properties of our data and make informed decisions about models.\n",
                "\n",
                "### 2.1 Measures of Central Tendency and Spread"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate a sample dataset\n",
                "np.random.seed(42)\n",
                "data = np.random.normal(loc=50, scale=15, size=1000)  # mean=50, std=15\n",
                "\n",
                "print(\"=\" * 50)\n",
                "print(\"DESCRIPTIVE STATISTICS\")\n",
                "print(\"=\" * 50)\n",
                "print(f\"Mean:               {np.mean(data):.2f}\")\n",
                "print(f\"Median:             {np.median(data):.2f}\")\n",
                "print(f\"Mode (approx):      {float(stats.mode(np.round(data), keepdims=True).mode):.2f}\")\n",
                "print(f\"Variance:           {np.var(data):.2f}\")\n",
                "print(f\"Standard Deviation: {np.std(data):.2f}\")\n",
                "print(f\"Minimum:            {np.min(data):.2f}\")\n",
                "print(f\"Maximum:            {np.max(data):.2f}\")\n",
                "print(f\"Range:              {np.ptp(data):.2f}\")\n",
                "print(f\"25th percentile:    {np.percentile(data, 25):.2f}\")\n",
                "print(f\"75th percentile:    {np.percentile(data, 75):.2f}\")\n",
                "print(f\"IQR:                {np.percentile(data, 75) - np.percentile(data, 25):.2f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize the distribution\n",
                "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
                "\n",
                "# Histogram\n",
                "ax = axes[0]\n",
                "ax.hist(data, bins=40, color='#2196F3', edgecolor='white', alpha=0.7, density=True)\n",
                "ax.axvline(np.mean(data), color='red', linestyle='--', linewidth=2, label=f'Mean: {np.mean(data):.1f}')\n",
                "ax.axvline(np.median(data), color='green', linestyle='--', linewidth=2, label=f'Median: {np.median(data):.1f}')\n",
                "ax.set_title('Histogram with Mean and Median', fontsize=13, fontweight='bold')\n",
                "ax.set_xlabel('Value')\n",
                "ax.set_ylabel('Density')\n",
                "ax.legend(fontsize=10)\n",
                "\n",
                "# Box plot\n",
                "ax = axes[1]\n",
                "bp = ax.boxplot(data, vert=True, patch_artist=True,\n",
                "                boxprops=dict(facecolor='#2196F3', alpha=0.6),\n",
                "                medianprops=dict(color='red', linewidth=2))\n",
                "ax.set_title('Box Plot', fontsize=13, fontweight='bold')\n",
                "ax.set_ylabel('Value')\n",
                "\n",
                "# QQ plot (check if data follows normal distribution)\n",
                "ax = axes[2]\n",
                "stats.probplot(data, dist='norm', plot=ax)\n",
                "ax.set_title('Q-Q Plot (Normality Check)', fontsize=13, fontweight='bold')\n",
                "ax.get_lines()[0].set_markerfacecolor('#2196F3')\n",
                "ax.get_lines()[0].set_alpha(0.5)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2.2 Common Distributions\n",
                "\n",
                "Understanding distributions is essential for choosing the right models and interpreting results."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
                "\n",
                "# Normal (Gaussian) Distribution\n",
                "ax = axes[0, 0]\n",
                "x = np.linspace(-4, 4, 200)\n",
                "for mu, sigma in [(0, 1), (0, 0.5), (1, 1.5)]:\n",
                "    y = stats.norm.pdf(x, mu, sigma)\n",
                "    ax.plot(x, y, linewidth=2, label=f'mu={mu}, sigma={sigma}')\n",
                "ax.set_title('Normal (Gaussian) Distribution', fontsize=13, fontweight='bold')\n",
                "ax.legend(fontsize=9)\n",
                "ax.set_xlabel('x')\n",
                "ax.set_ylabel('Probability Density')\n",
                "\n",
                "# Uniform Distribution\n",
                "ax = axes[0, 1]\n",
                "x_uni = np.linspace(-0.5, 1.5, 200)\n",
                "ax.plot(x_uni, stats.uniform.pdf(x_uni), linewidth=2, color='#FF5722')\n",
                "ax.fill_between(x_uni, stats.uniform.pdf(x_uni), alpha=0.3, color='#FF5722')\n",
                "ax.set_title('Uniform Distribution (0, 1)', fontsize=13, fontweight='bold')\n",
                "ax.set_xlabel('x')\n",
                "ax.set_ylabel('Probability Density')\n",
                "\n",
                "# Binomial Distribution\n",
                "ax = axes[1, 0]\n",
                "n_trials = 20\n",
                "for p in [0.3, 0.5, 0.7]:\n",
                "    x_binom = np.arange(0, n_trials + 1)\n",
                "    y_binom = stats.binom.pmf(x_binom, n_trials, p)\n",
                "    ax.bar(x_binom + (p - 0.5) * 0.25, y_binom, width=0.25, alpha=0.7, label=f'p={p}')\n",
                "ax.set_title('Binomial Distribution (n=20)', fontsize=13, fontweight='bold')\n",
                "ax.legend(fontsize=10)\n",
                "ax.set_xlabel('Number of Successes')\n",
                "ax.set_ylabel('Probability')\n",
                "\n",
                "# Poisson Distribution\n",
                "ax = axes[1, 1]\n",
                "for lam in [2, 5, 10]:\n",
                "    x_pois = np.arange(0, 25)\n",
                "    y_pois = stats.poisson.pmf(x_pois, lam)\n",
                "    ax.plot(x_pois, y_pois, 'o-', linewidth=1.5, markersize=5, label=f'lambda={lam}')\n",
                "ax.set_title('Poisson Distribution', fontsize=13, fontweight='bold')\n",
                "ax.legend(fontsize=10)\n",
                "ax.set_xlabel('k')\n",
                "ax.set_ylabel('Probability')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2.3 Correlation\n",
                "\n",
                "Correlation measures the linear relationship between two variables. It ranges from -1 (perfect negative correlation) to +1 (perfect positive correlation)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate correlated data\n",
                "np.random.seed(42)\n",
                "n = 200\n",
                "\n",
                "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
                "\n",
                "# Positive correlation\n",
                "x1 = np.random.randn(n)\n",
                "y1 = 0.8 * x1 + 0.3 * np.random.randn(n)\n",
                "r1 = np.corrcoef(x1, y1)[0, 1]\n",
                "axes[0].scatter(x1, y1, alpha=0.5, color='#2196F3', s=30, edgecolors='white', linewidth=0.3)\n",
                "axes[0].set_title(f'Positive Correlation\\nr = {r1:.2f}', fontsize=13, fontweight='bold')\n",
                "z = np.polyfit(x1, y1, 1)\n",
                "axes[0].plot(np.sort(x1), np.polyval(z, np.sort(x1)), 'r-', linewidth=2)\n",
                "\n",
                "# No correlation\n",
                "x2 = np.random.randn(n)\n",
                "y2 = np.random.randn(n)\n",
                "r2 = np.corrcoef(x2, y2)[0, 1]\n",
                "axes[1].scatter(x2, y2, alpha=0.5, color='#FF9800', s=30, edgecolors='white', linewidth=0.3)\n",
                "axes[1].set_title(f'No Correlation\\nr = {r2:.2f}', fontsize=13, fontweight='bold')\n",
                "\n",
                "# Negative correlation\n",
                "x3 = np.random.randn(n)\n",
                "y3 = -0.9 * x3 + 0.2 * np.random.randn(n)\n",
                "r3 = np.corrcoef(x3, y3)[0, 1]\n",
                "axes[2].scatter(x3, y3, alpha=0.5, color='#4CAF50', s=30, edgecolors='white', linewidth=0.3)\n",
                "axes[2].set_title(f'Negative Correlation\\nr = {r3:.2f}', fontsize=13, fontweight='bold')\n",
                "z3 = np.polyfit(x3, y3, 1)\n",
                "axes[2].plot(np.sort(x3), np.polyval(z3, np.sort(x3)), 'r-', linewidth=2)\n",
                "\n",
                "for ax in axes:\n",
                "    ax.set_xlabel('x')\n",
                "    ax.set_ylabel('y')\n",
                "\n",
                "plt.suptitle('Types of Correlation', fontsize=15, fontweight='bold')\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 3. Probability Basics\n",
                "\n",
                "Probability provides the mathematical framework for reasoning about uncertainty — which is central to making predictions with imperfect data.\n",
                "\n",
                "### 3.1 Core Concepts\n",
                "\n",
                "| Concept | Formula | Meaning |\n",
                "|---------|---------|--------|\n",
                "| Probability of event A | P(A) = favorable outcomes / total outcomes | How likely A is to occur |\n",
                "| Joint Probability | P(A and B) | Probability that both A and B occur |\n",
                "| Conditional Probability | P(A \\| B) = P(A and B) / P(B) | Probability of A given that B has occurred |\n",
                "| Bayes' Theorem | P(A \\| B) = P(B \\| A) * P(A) / P(B) | Update beliefs with new evidence |"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Example: Medical test scenario using Bayes' Theorem\n",
                "#\n",
                "# A disease affects 1% of the population.\n",
                "# A test has:\n",
                "#   - 99% sensitivity (true positive rate): P(Test+ | Disease) = 0.99\n",
                "#   - 95% specificity (true negative rate): P(Test- | No Disease) = 0.95\n",
                "#\n",
                "# Question: If someone tests positive, what is the probability they actually have the disease?\n",
                "\n",
                "P_disease = 0.01\n",
                "P_no_disease = 1 - P_disease\n",
                "P_pos_given_disease = 0.99     # sensitivity\n",
                "P_pos_given_no_disease = 0.05  # false positive rate (1 - specificity)\n",
                "\n",
                "# Total probability of testing positive\n",
                "P_pos = P_pos_given_disease * P_disease + P_pos_given_no_disease * P_no_disease\n",
                "\n",
                "# Bayes' Theorem\n",
                "P_disease_given_pos = (P_pos_given_disease * P_disease) / P_pos\n",
                "\n",
                "print(\"BAYES' THEOREM — MEDICAL TEST EXAMPLE\")\n",
                "print(\"=\" * 50)\n",
                "print(f\"Prior probability of disease:   P(D)          = {P_disease:.2%}\")\n",
                "print(f\"Sensitivity (true positive):    P(+|D)        = {P_pos_given_disease:.2%}\")\n",
                "print(f\"False positive rate:            P(+|no D)     = {P_pos_given_no_disease:.2%}\")\n",
                "print(f\"Probability of positive test:   P(+)          = {P_pos:.4f}\")\n",
                "print(f\"\\nPosterior probability:          P(D|+)        = {P_disease_given_pos:.2%}\")\n",
                "print(f\"\\nInterpretation: Even with a positive test result, there is only a {P_disease_given_pos:.1%}\")\n",
                "print(\"chance of actually having the disease. This counterintuitive result arises\")\n",
                "print(\"because the disease is rare — most positive tests are false positives.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize Bayes' Theorem with varying prior probabilities\n",
                "priors = np.linspace(0.001, 0.5, 200)\n",
                "posteriors = (P_pos_given_disease * priors) / \\\n",
                "             (P_pos_given_disease * priors + P_pos_given_no_disease * (1 - priors))\n",
                "\n",
                "fig, ax = plt.subplots(figsize=(10, 6))\n",
                "ax.plot(priors * 100, posteriors * 100, linewidth=2.5, color='#2196F3')\n",
                "ax.axhline(y=50, color='gray', linestyle='--', alpha=0.5, label='50% threshold')\n",
                "ax.axvline(x=1, color='red', linestyle='--', alpha=0.5, label='Our scenario (1% prior)')\n",
                "ax.scatter([1], [P_disease_given_pos * 100], color='red', s=100, zorder=5)\n",
                "ax.annotate(f'{P_disease_given_pos:.1%}', xy=(1, P_disease_given_pos * 100),\n",
                "            xytext=(5, P_disease_given_pos * 100 + 10), fontsize=12,\n",
                "            arrowprops=dict(arrowstyle='->', color='red'))\n",
                "ax.set_xlabel('Prior Probability of Disease (%)', fontsize=13)\n",
                "ax.set_ylabel('Posterior Probability Given Positive Test (%)', fontsize=13)\n",
                "ax.set_title(\"Bayes' Theorem: How Prior Probability Affects the Posterior\",\n",
                "             fontsize=14, fontweight='bold')\n",
                "ax.legend(fontsize=11)\n",
                "ax.set_xlim(0, 50)\n",
                "ax.set_ylim(0, 100)\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 4. Calculus Intuition: Gradients and Optimization\n",
                "\n",
                "Calculus is used in ML primarily for **optimization** — finding the parameters that minimize a cost function. You do not need to compute derivatives by hand, but you need to understand the intuition.\n",
                "\n",
                "### Key Idea: Gradient Descent\n",
                "\n",
                "Most ML models learn by minimizing a **loss function** (also called cost function or error). **Gradient descent** is the algorithm that does this:\n",
                "\n",
                "1. Start with random parameters.\n",
                "2. Compute the gradient (slope) of the loss function at the current point.\n",
                "3. Move the parameters in the direction that reduces the loss (opposite to the gradient).\n",
                "4. Repeat until convergence."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize gradient descent on a simple 1D function\n",
                "# Loss function: f(x) = x^2 (minimum at x=0)\n",
                "\n",
                "def f(x):\n",
                "    return x ** 2\n",
                "\n",
                "def df(x):\n",
                "    \"\"\"Derivative: f'(x) = 2x\"\"\"\n",
                "    return 2 * x\n",
                "\n",
                "# Gradient descent\n",
                "learning_rate = 0.2\n",
                "x_current = 4.0  # starting point\n",
                "history = [x_current]\n",
                "\n",
                "for i in range(15):\n",
                "    gradient = df(x_current)\n",
                "    x_current = x_current - learning_rate * gradient\n",
                "    history.append(x_current)\n",
                "\n",
                "# Plot\n",
                "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
                "\n",
                "# Left: Function and gradient descent path\n",
                "x_range = np.linspace(-5, 5, 200)\n",
                "ax = axes[0]\n",
                "ax.plot(x_range, f(x_range), 'b-', linewidth=2, label='f(x) = x²')\n",
                "ax.plot(history, [f(x) for x in history], 'ro-', markersize=8, linewidth=1.5, label='Gradient descent path')\n",
                "ax.annotate('Start', xy=(history[0], f(history[0])), xytext=(history[0] + 0.5, f(history[0]) + 2),\n",
                "            fontsize=11, arrowprops=dict(arrowstyle='->'))\n",
                "ax.annotate('Minimum', xy=(history[-1], f(history[-1])), xytext=(history[-1] + 1, f(history[-1]) + 3),\n",
                "            fontsize=11, arrowprops=dict(arrowstyle='->'))\n",
                "ax.set_xlabel('x (parameter)', fontsize=13)\n",
                "ax.set_ylabel('f(x) (loss)', fontsize=13)\n",
                "ax.set_title('Gradient Descent on f(x) = x²', fontsize=14, fontweight='bold')\n",
                "ax.legend(fontsize=11)\n",
                "\n",
                "# Right: Convergence plot\n",
                "ax = axes[1]\n",
                "ax.plot(range(len(history)), [f(x) for x in history], 'o-', color='#FF5722', linewidth=2, markersize=6)\n",
                "ax.set_xlabel('Iteration', fontsize=13)\n",
                "ax.set_ylabel('Loss f(x)', fontsize=13)\n",
                "ax.set_title('Convergence of Gradient Descent', fontsize=14, fontweight='bold')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(\"\\nGradient Descent Trace:\")\n",
                "print(f\"{'Step':>4}  {'x':>10}  {'f(x)':>10}  {'gradient':>10}\")\n",
                "print(\"-\" * 40)\n",
                "for i in range(min(8, len(history))):\n",
                "    print(f\"{i:>4}  {history[i]:>10.4f}  {f(history[i]):>10.4f}  {df(history[i]):>10.4f}\")\n",
                "print(f\"  ...\")\n",
                "print(f\"{len(history)-1:>4}  {history[-1]:>10.6f}  {f(history[-1]):>10.6f}  {df(history[-1]):>10.6f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Effect of learning rate on gradient descent\n",
                "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
                "learning_rates = [0.05, 0.3, 0.95]\n",
                "titles = ['Too Small (lr=0.05)\\nSlow convergence',\n",
                "          'Good (lr=0.3)\\nSmooth convergence',\n",
                "          'Too Large (lr=0.95)\\nOscillation / Divergence']\n",
                "\n",
                "for idx, (lr, title) in enumerate(zip(learning_rates, titles)):\n",
                "    ax = axes[idx]\n",
                "    x_range = np.linspace(-5, 5, 200)\n",
                "    ax.plot(x_range, f(x_range), 'b-', linewidth=2, alpha=0.5)\n",
                "\n",
                "    x_curr = 4.0\n",
                "    hist = [x_curr]\n",
                "    for _ in range(20):\n",
                "        x_curr = x_curr - lr * df(x_curr)\n",
                "        hist.append(x_curr)\n",
                "        if abs(x_curr) > 10:\n",
                "            break\n",
                "\n",
                "    hist_clipped = [x for x in hist if abs(x) <= 5.5]\n",
                "    ax.plot(hist_clipped, [f(x) for x in hist_clipped], 'ro-', markersize=6, linewidth=1.5)\n",
                "    ax.set_title(title, fontsize=12, fontweight='bold')\n",
                "    ax.set_xlabel('x')\n",
                "    ax.set_ylabel('f(x)')\n",
                "    ax.set_xlim(-5, 5)\n",
                "    ax.set_ylim(-1, 25)\n",
                "\n",
                "plt.suptitle('Effect of Learning Rate on Gradient Descent', fontsize=15, fontweight='bold')\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(\"\\nKey takeaways:\")\n",
                "print(\"  - Learning rate too small: converges, but very slowly.\")\n",
                "print(\"  - Learning rate just right: smooth and efficient convergence.\")\n",
                "print(\"  - Learning rate too large: oscillates or diverges — never reaches the minimum.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4.1 Partial Derivatives and Gradients in Higher Dimensions\n",
                "\n",
                "In practice, loss functions have many parameters (not just one). The **gradient** is a vector of partial derivatives — one for each parameter. Gradient descent follows the direction of steepest descent in the parameter space."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2D gradient descent visualization\n",
                "# Loss function: f(x, y) = x^2 + 2*y^2 (minimum at origin)\n",
                "\n",
                "def f_2d(x, y):\n",
                "    return x**2 + 2*y**2\n",
                "\n",
                "def grad_2d(x, y):\n",
                "    return np.array([2*x, 4*y])\n",
                "\n",
                "# Gradient descent in 2D\n",
                "lr = 0.15\n",
                "pos = np.array([4.0, 3.0])\n",
                "path = [pos.copy()]\n",
                "\n",
                "for _ in range(30):\n",
                "    g = grad_2d(pos[0], pos[1])\n",
                "    pos = pos - lr * g\n",
                "    path.append(pos.copy())\n",
                "\n",
                "path = np.array(path)\n",
                "\n",
                "# Contour plot\n",
                "fig, ax = plt.subplots(figsize=(10, 8))\n",
                "X_grid = np.linspace(-5, 5, 200)\n",
                "Y_grid = np.linspace(-4, 4, 200)\n",
                "X_mesh, Y_mesh = np.meshgrid(X_grid, Y_grid)\n",
                "Z = f_2d(X_mesh, Y_mesh)\n",
                "\n",
                "contours = ax.contour(X_mesh, Y_mesh, Z, levels=20, cmap='viridis', alpha=0.7)\n",
                "ax.clabel(contours, inline=True, fontsize=8)\n",
                "ax.contourf(X_mesh, Y_mesh, Z, levels=20, cmap='viridis', alpha=0.3)\n",
                "\n",
                "ax.plot(path[:, 0], path[:, 1], 'ro-', markersize=5, linewidth=1.5, label='Gradient descent path')\n",
                "ax.plot(path[0, 0], path[0, 1], 'rs', markersize=12, label='Start')\n",
                "ax.plot(path[-1, 0], path[-1, 1], 'r*', markersize=15, label='End (near minimum)')\n",
                "\n",
                "ax.set_xlabel('x', fontsize=13)\n",
                "ax.set_ylabel('y', fontsize=13)\n",
                "ax.set_title('2D Gradient Descent on f(x,y) = x² + 2y²', fontsize=14, fontweight='bold')\n",
                "ax.legend(fontsize=11)\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 5. Exercises\n",
                "\n",
                "### Exercise 1: Linear Algebra Practice"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Exercise 1: Complete the following tasks\n",
                "\n",
                "# 1a. Create two vectors: v1 = [3, 4, 5] and v2 = [1, 0, -1]\n",
                "#     Compute their dot product, norms, and the Euclidean distance between them.\n",
                "\n",
                "# Your code here:\n",
                "\n",
                "\n",
                "# 1b. Create a 3x3 matrix M = [[1, 2, 3], [0, 1, 4], [5, 6, 0]]\n",
                "#     Compute its transpose, determinant, and inverse.\n",
                "\n",
                "# Your code here:\n",
                "\n",
                "\n",
                "# 1c. Verify that M @ M_inv equals the identity matrix.\n",
                "\n",
                "# Your code here:\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Exercise 2: Statistics on Real Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Exercise 2: Use the Iris dataset to practice statistics\n",
                "\n",
                "from sklearn.datasets import load_iris\n",
                "\n",
                "iris = load_iris()\n",
                "iris_df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
                "\n",
                "# TODO: Compute mean, median, std, and variance for EACH feature\n",
                "# Hint: use iris_df.describe() or compute each individually with np functions\n",
                "\n",
                "# TODO: Create a correlation matrix and visualize it as a heatmap\n",
                "\n",
                "# TODO: Which two features are most correlated? Which are least correlated?\n",
                "\n",
                "# TODO: Plot histograms for all 4 features in a 2x2 subplot\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Exercise 3: Bayes' Theorem Application"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Exercise 3: Spam Filter using Bayes' Theorem\n",
                "#\n",
                "# Given:\n",
                "#   - 30% of all emails are spam:                    P(spam) = 0.30\n",
                "#   - The word \"free\" appears in 80% of spam emails: P(\"free\" | spam) = 0.80\n",
                "#   - The word \"free\" appears in 10% of non-spam:    P(\"free\" | not spam) = 0.10\n",
                "#\n",
                "# Question: If an email contains the word \"free\", what is the probability it is spam?\n",
                "#\n",
                "# TODO: Compute P(spam | \"free\") using Bayes' Theorem\n",
                "\n",
                "# Your code here:\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<details>\n",
                "<summary><b>Click here for the solution</b></summary>\n",
                "\n",
                "```python\n",
                "P_spam = 0.30\n",
                "P_not_spam = 0.70\n",
                "P_free_given_spam = 0.80\n",
                "P_free_given_not_spam = 0.10\n",
                "\n",
                "P_free = P_free_given_spam * P_spam + P_free_given_not_spam * P_not_spam\n",
                "P_spam_given_free = (P_free_given_spam * P_spam) / P_free\n",
                "\n",
                "print(f\"P(spam | 'free') = {P_spam_given_free:.2%}\")\n",
                "# Answer: approximately 77.42%\n",
                "```\n",
                "\n",
                "</details>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Exercise 4: Implement Gradient Descent"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Exercise 4: Implement gradient descent for f(x) = (x - 3)^2 + 5\n",
                "# The minimum should be at x = 3, f(3) = 5\n",
                "\n",
                "# TODO: Define the function and its derivative\n",
                "# TODO: Implement gradient descent starting from x = -2\n",
                "# TODO: Use learning_rate = 0.1 and run for 50 iterations\n",
                "# TODO: Plot the function and the gradient descent path\n",
                "# TODO: Print the final x value and f(x) — they should be close to 3 and 5\n",
                "\n",
                "# Your code here:\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 6. Summary and Further Reading\n",
                "\n",
                "### What We Covered\n",
                "\n",
                "- **Linear Algebra**: Vectors, matrices, dot products, matrix multiplication, transpose, inverse — the building blocks of data representation in ML.\n",
                "- **Statistics**: Measures of central tendency and spread, distributions (Normal, Uniform, Binomial, Poisson), correlation.\n",
                "- **Probability**: Conditional probability, Bayes' Theorem, and its real-world applications.\n",
                "- **Calculus**: The concept of gradients and how gradient descent is used to minimize loss functions.\n",
                "\n",
                "### Recommended Reading\n",
                "\n",
                "- Gilbert Strang, *Introduction to Linear Algebra* (or his free MIT OpenCourseWare lectures)\n",
                "- 3Blue1Brown, *Essence of Linear Algebra* video series (YouTube)\n",
                "- Khan Academy — Statistics and Probability (free online)\n",
                "- 3Blue1Brown, *Essence of Calculus* video series (YouTube)\n",
                "\n",
                "### Next Module\n",
                "\n",
                "In **Module 3: Data Preprocessing and Feature Engineering**, we will learn how to clean messy data, handle missing values, encode categorical variables, and scale features — the critical step between raw data and model training.\n",
                "\n",
                "---"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.9.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}