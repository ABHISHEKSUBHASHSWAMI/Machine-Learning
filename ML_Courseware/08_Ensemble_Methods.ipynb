{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Module 8: Ensemble Methods\n",
                "\n",
                "---\n",
                "\n",
                "Ensemble methods combine multiple models to produce a stronger, more robust predictor than any individual model. This module covers the three main ensemble strategies: **bagging**, **boosting**, and **stacking**.\n",
                "\n",
                "---\n",
                "\n",
                "## Table of Contents\n",
                "\n",
                "1. [Why Ensembles Work](#1.-Why-Ensembles-Work)\n",
                "2. [Bagging and Random Forests](#2.-Bagging-and-Random-Forests)\n",
                "3. [Boosting: AdaBoost and Gradient Boosting](#3.-Boosting)\n",
                "4. [XGBoost](#4.-XGBoost)\n",
                "5. [Stacking](#5.-Stacking)\n",
                "6. [Comprehensive Comparison](#6.-Comprehensive-Comparison)\n",
                "7. [Exercises](#7.-Exercises)\n",
                "8. [Summary and Further Reading](#8.-Summary-and-Further-Reading)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "from sklearn.datasets import load_breast_cancer, make_classification\n",
                "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "from sklearn.metrics import accuracy_score, classification_report\n",
                "\n",
                "from sklearn.tree import DecisionTreeClassifier\n",
                "from sklearn.ensemble import (BaggingClassifier, RandomForestClassifier,\n",
                "                               AdaBoostClassifier, GradientBoostingClassifier,\n",
                "                               StackingClassifier, VotingClassifier)\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.svm import SVC\n",
                "from sklearn.neighbors import KNeighborsClassifier\n",
                "\n",
                "plt.style.use('seaborn-v0_8-whitegrid')\n",
                "np.random.seed(42)\n",
                "\n",
                "# Prepare data\n",
                "cancer = load_breast_cancer()\n",
                "X, y = cancer.data, cancer.target\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
                "scaler = StandardScaler()\n",
                "X_train_s = scaler.fit_transform(X_train)\n",
                "X_test_s = scaler.transform(X_test)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 1. Why Ensembles Work\n",
                "\n",
                "The core idea behind ensemble methods is the **wisdom of crowds**: combining many imperfect models often produces a result that is better than any single model.\n",
                "\n",
                "| Strategy | Description | Key Idea |\n",
                "|----------|------------|----------|\n",
                "| **Bagging** | Train many models on bootstrap samples, average/vote | Reduces variance |\n",
                "| **Boosting** | Train models sequentially, each fixing previous errors | Reduces bias |\n",
                "| **Stacking** | Use a meta-model to combine predictions of base models | Leverages diversity |"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Demonstrate that a single weak classifier has high variance\n",
                "# while an ensemble of weak classifiers is more stable\n",
                "\n",
                "n_trials = 20\n",
                "single_accuracies = []\n",
                "ensemble_accuracies = []\n",
                "\n",
                "for i in range(n_trials):\n",
                "    # Random subsample\n",
                "    idx = np.random.choice(len(X_train_s), size=int(0.7 * len(X_train_s)), replace=True)\n",
                "    \n",
                "    # Single tree\n",
                "    dt = DecisionTreeClassifier(max_depth=3, random_state=i)\n",
                "    dt.fit(X_train_s[idx], y_train[idx])\n",
                "    single_accuracies.append(dt.score(X_test_s, y_test))\n",
                "    \n",
                "    # Bagging ensemble (50 trees)\n",
                "    bag = BaggingClassifier(estimator=DecisionTreeClassifier(max_depth=3),\n",
                "                            n_estimators=50, random_state=i)\n",
                "    bag.fit(X_train_s[idx], y_train[idx])\n",
                "    ensemble_accuracies.append(bag.score(X_test_s, y_test))\n",
                "\n",
                "fig, ax = plt.subplots(figsize=(10, 5))\n",
                "ax.plot(range(n_trials), single_accuracies, 'o-', label=f'Single Tree (std={np.std(single_accuracies):.3f})',\n",
                "        color='#FF5722', linewidth=1.5)\n",
                "ax.plot(range(n_trials), ensemble_accuracies, 's-', label=f'Ensemble of 50 Trees (std={np.std(ensemble_accuracies):.3f})',\n",
                "        color='#2196F3', linewidth=1.5)\n",
                "ax.set_xlabel('Trial', fontsize=13)\n",
                "ax.set_ylabel('Test Accuracy', fontsize=13)\n",
                "ax.set_title('Variance Reduction Through Ensembling', fontsize=14, fontweight='bold')\n",
                "ax.legend(fontsize=11)\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(f\"Single tree — Mean: {np.mean(single_accuracies):.4f}, Std: {np.std(single_accuracies):.4f}\")\n",
                "print(f\"Ensemble    — Mean: {np.mean(ensemble_accuracies):.4f}, Std: {np.std(ensemble_accuracies):.4f}\")\n",
                "print(\"\\nThe ensemble is both more accurate and more stable across trials.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 2. Bagging and Random Forests\n",
                "\n",
                "**Bagging (Bootstrap Aggregating)** creates multiple models trained on random subsets of the training data (with replacement). Final prediction is made by majority vote (classification) or averaging (regression).\n",
                "\n",
                "**Random Forest** is bagging applied to decision trees, with an additional twist: at each split, only a random subset of features is considered. This further decorrelates the trees."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Random Forest with different numbers of trees\n",
                "n_trees_range = [1, 5, 10, 25, 50, 100, 200, 300]\n",
                "rf_accuracies = []\n",
                "\n",
                "for n_trees in n_trees_range:\n",
                "    rf = RandomForestClassifier(n_estimators=n_trees, random_state=42, n_jobs=-1)\n",
                "    rf.fit(X_train_s, y_train)\n",
                "    rf_accuracies.append(rf.score(X_test_s, y_test))\n",
                "\n",
                "fig, ax = plt.subplots(figsize=(10, 5))\n",
                "ax.plot(n_trees_range, rf_accuracies, 'o-', linewidth=2, color='#4CAF50', markersize=8)\n",
                "ax.set_xlabel('Number of Trees', fontsize=13)\n",
                "ax.set_ylabel('Test Accuracy', fontsize=13)\n",
                "ax.set_title('Random Forest — Accuracy vs Number of Trees', fontsize=14, fontweight='bold')\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(\"Performance improves rapidly at first, then plateaus.\")\n",
                "print(\"More trees rarely hurt accuracy but increase computation time.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Feature importance from Random Forest\n",
                "rf_final = RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1)\n",
                "rf_final.fit(X_train_s, y_train)\n",
                "\n",
                "importance = pd.DataFrame({\n",
                "    'Feature': cancer.feature_names,\n",
                "    'Importance': rf_final.feature_importances_\n",
                "}).sort_values('Importance', ascending=False).head(15)\n",
                "\n",
                "fig, ax = plt.subplots(figsize=(10, 7))\n",
                "ax.barh(importance['Feature'], importance['Importance'], color='#4CAF50', edgecolor='white')\n",
                "ax.set_xlabel('Feature Importance (Mean Decrease in Impurity)', fontsize=13)\n",
                "ax.set_title('Top 15 Features — Random Forest', fontsize=14, fontweight='bold')\n",
                "ax.invert_yaxis()\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(f\"\\nRandom Forest Test Accuracy: {rf_final.score(X_test_s, y_test):.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 3. Boosting: AdaBoost and Gradient Boosting\n",
                "\n",
                "**Boosting** trains models sequentially, where each new model focuses on the mistakes of the previous models.\n",
                "\n",
                "- **AdaBoost**: Adjusts sample weights — misclassified samples get higher weight.\n",
                "- **Gradient Boosting**: Fits new models to the residual errors of the ensemble."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# AdaBoost\n",
                "ada = AdaBoostClassifier(\n",
                "    estimator=DecisionTreeClassifier(max_depth=1),  # weak learners (stumps)\n",
                "    n_estimators=200,\n",
                "    learning_rate=0.5,\n",
                "    random_state=42\n",
                ")\n",
                "ada.fit(X_train_s, y_train)\n",
                "\n",
                "# Gradient Boosting\n",
                "gb = GradientBoostingClassifier(\n",
                "    n_estimators=200,\n",
                "    learning_rate=0.1,\n",
                "    max_depth=3,\n",
                "    random_state=42\n",
                ")\n",
                "gb.fit(X_train_s, y_train)\n",
                "\n",
                "print(\"Boosting Results — Breast Cancer Dataset\")\n",
                "print(\"=\" * 50)\n",
                "print(f\"AdaBoost:          Train={ada.score(X_train_s, y_train):.4f}  Test={ada.score(X_test_s, y_test):.4f}\")\n",
                "print(f\"Gradient Boosting: Train={gb.score(X_train_s, y_train):.4f}  Test={gb.score(X_test_s, y_test):.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Staged prediction — how accuracy improves as trees are added\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "# AdaBoost staged\n",
                "ada_staged_train = list(ada.staged_score(X_train_s, y_train))\n",
                "ada_staged_test = list(ada.staged_score(X_test_s, y_test))\n",
                "axes[0].plot(range(1, len(ada_staged_train)+1), ada_staged_train, label='Train', color='#2196F3', linewidth=2)\n",
                "axes[0].plot(range(1, len(ada_staged_test)+1), ada_staged_test, label='Test', color='#FF5722', linewidth=2)\n",
                "axes[0].set_xlabel('Number of Estimators', fontsize=12)\n",
                "axes[0].set_ylabel('Accuracy', fontsize=12)\n",
                "axes[0].set_title('AdaBoost — Staged Performance', fontsize=13, fontweight='bold')\n",
                "axes[0].legend(fontsize=11)\n",
                "\n",
                "# Gradient Boosting staged\n",
                "gb_staged_train = list(gb.staged_score(X_train_s, y_train))\n",
                "gb_staged_test = list(gb.staged_score(X_test_s, y_test))\n",
                "axes[1].plot(range(1, len(gb_staged_train)+1), gb_staged_train, label='Train', color='#2196F3', linewidth=2)\n",
                "axes[1].plot(range(1, len(gb_staged_test)+1), gb_staged_test, label='Test', color='#FF5722', linewidth=2)\n",
                "axes[1].set_xlabel('Number of Estimators', fontsize=12)\n",
                "axes[1].set_ylabel('Accuracy', fontsize=12)\n",
                "axes[1].set_title('Gradient Boosting — Staged Performance', fontsize=13, fontweight='bold')\n",
                "axes[1].legend(fontsize=11)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 4. XGBoost\n",
                "\n",
                "**XGBoost (Extreme Gradient Boosting)** is an optimized gradient boosting library widely used in machine learning competitions and industry. It adds regularization, handles missing values, and is highly efficient.\n",
                "\n",
                "Note: XGBoost is an external library. If not installed, run `pip install xgboost`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "try:\n",
                "    from xgboost import XGBClassifier\n",
                "    \n",
                "    xgb = XGBClassifier(\n",
                "        n_estimators=200,\n",
                "        learning_rate=0.1,\n",
                "        max_depth=3,\n",
                "        random_state=42,\n",
                "        use_label_encoder=False,\n",
                "        eval_metric='logloss'\n",
                "    )\n",
                "    xgb.fit(X_train_s, y_train)\n",
                "    \n",
                "    print(f\"XGBoost Results:\")\n",
                "    print(f\"  Train accuracy: {xgb.score(X_train_s, y_train):.4f}\")\n",
                "    print(f\"  Test accuracy:  {xgb.score(X_test_s, y_test):.4f}\")\n",
                "    \n",
                "    # Feature importance\n",
                "    xgb_importance = pd.DataFrame({\n",
                "        'Feature': cancer.feature_names,\n",
                "        'Importance': xgb.feature_importances_\n",
                "    }).sort_values('Importance', ascending=False).head(10)\n",
                "    \n",
                "    fig, ax = plt.subplots(figsize=(10, 5))\n",
                "    ax.barh(xgb_importance['Feature'], xgb_importance['Importance'], color='#9C27B0', edgecolor='white')\n",
                "    ax.set_xlabel('Feature Importance', fontsize=13)\n",
                "    ax.set_title('Top 10 Features — XGBoost', fontsize=14, fontweight='bold')\n",
                "    ax.invert_yaxis()\n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "    \n",
                "except ImportError:\n",
                "    print(\"XGBoost is not installed. To install, run: pip install xgboost\")\n",
                "    print(\"Skipping this section.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 5. Stacking\n",
                "\n",
                "**Stacking** trains a meta-model on the predictions of several base models. The idea is that different models capture different patterns, and a meta-learner can combine them optimally."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Stacking ensemble\n",
                "base_models = [\n",
                "    ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
                "    ('knn', KNeighborsClassifier(n_neighbors=5)),\n",
                "    ('svm', SVC(kernel='rbf', probability=True, random_state=42)),\n",
                "]\n",
                "\n",
                "stacking = StackingClassifier(\n",
                "    estimators=base_models,\n",
                "    final_estimator=LogisticRegression(max_iter=5000),\n",
                "    cv=5\n",
                ")\n",
                "\n",
                "stacking.fit(X_train_s, y_train)\n",
                "print(f\"Stacking Ensemble:\")\n",
                "print(f\"  Train accuracy: {stacking.score(X_train_s, y_train):.4f}\")\n",
                "print(f\"  Test accuracy:  {stacking.score(X_test_s, y_test):.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 6. Comprehensive Comparison"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Compare all ensemble methods\n",
                "all_models = {\n",
                "    'Single Decision Tree': DecisionTreeClassifier(max_depth=5, random_state=42),\n",
                "    'Bagging (50 Trees)': BaggingClassifier(n_estimators=50, random_state=42),\n",
                "    'Random Forest (200)': RandomForestClassifier(n_estimators=200, random_state=42),\n",
                "    'AdaBoost (200)': AdaBoostClassifier(n_estimators=200, random_state=42),\n",
                "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=200, random_state=42),\n",
                "}\n",
                "\n",
                "results = []\n",
                "cv_fold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
                "\n",
                "for name, model in all_models.items():\n",
                "    scores = cross_val_score(model, X_train_s, y_train, cv=cv_fold, scoring='accuracy')\n",
                "    model.fit(X_train_s, y_train)\n",
                "    test_acc = model.score(X_test_s, y_test)\n",
                "    results.append({\n",
                "        'Model': name,\n",
                "        'CV Mean': scores.mean(),\n",
                "        'CV Std': scores.std(),\n",
                "        'Test Acc': test_acc\n",
                "    })\n",
                "\n",
                "results_df = pd.DataFrame(results).sort_values('Test Acc', ascending=False)\n",
                "print(\"Ensemble Methods Comparison — Breast Cancer Dataset\")\n",
                "print(\"=\" * 70)\n",
                "print(results_df.to_string(index=False))\n",
                "\n",
                "# Bar chart\n",
                "fig, ax = plt.subplots(figsize=(12, 6))\n",
                "x = np.arange(len(results_df))\n",
                "ax.bar(x, results_df['Test Acc'], color=['#FF5722', '#2196F3', '#4CAF50', '#FF9800', '#9C27B0'],\n",
                "       edgecolor='white')\n",
                "ax.set_xticks(x)\n",
                "ax.set_xticklabels(results_df['Model'], rotation=20, ha='right')\n",
                "ax.set_ylabel('Test Accuracy', fontsize=13)\n",
                "ax.set_title('Ensemble Methods — Test Accuracy Comparison', fontsize=14, fontweight='bold')\n",
                "ax.set_ylim(0.92, 1.0)\n",
                "for i, v in enumerate(results_df['Test Acc']):\n",
                "    ax.text(i, v + 0.002, f'{v:.3f}', ha='center', fontsize=11)\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 7. Exercises\n",
                "\n",
                "### Exercise 1: Random Forest Tuning"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Exercise 1: Tune a Random Forest using GridSearchCV\n",
                "# Search over:\n",
                "#   n_estimators: [50, 100, 200]\n",
                "#   max_depth: [3, 5, 10, None]\n",
                "#   min_samples_split: [2, 5, 10]\n",
                "#\n",
                "# Report the best parameters and best CV score.\n",
                "# Compare the tuned model's test accuracy with a default Random Forest.\n",
                "\n",
                "from sklearn.model_selection import GridSearchCV\n",
                "\n",
                "# Your code here:\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Exercise 2: Boosting Learning Rate Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Exercise 2: Analyze the effect of learning_rate on Gradient Boosting\n",
                "# 1. Try learning rates: [0.01, 0.05, 0.1, 0.3, 0.5, 1.0]\n",
                "# 2. Use n_estimators=200, max_depth=3\n",
                "# 3. Record train and test accuracy for each\n",
                "# 4. Plot learning_rate vs accuracy\n",
                "# 5. What is the relationship between learning_rate and overfitting?\n",
                "\n",
                "# Your code here:\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Exercise 3: Voting Classifier"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Exercise 3: Build a VotingClassifier\n",
                "# 1. Combine: LogisticRegression, RandomForest, SVM\n",
                "# 2. Try both 'hard' voting (majority) and 'soft' voting (probability-based)\n",
                "# 3. Compare their test accuracies\n",
                "# 4. Is the voting classifier better than any individual model?\n",
                "\n",
                "# Your code here:\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 8. Summary and Further Reading\n",
                "\n",
                "### What We Covered\n",
                "\n",
                "| Method | Strategy | Key Benefit | When to Use |\n",
                "|--------|----------|-------------|-------------|\n",
                "| Bagging | Parallel, bootstrap | Reduces variance | High-variance models (deep trees) |\n",
                "| Random Forest | Bagging + random features | Robust, feature importance | General-purpose, strong baseline |\n",
                "| AdaBoost | Sequential, reweight samples | Focuses on hard examples | Weak learners |\n",
                "| Gradient Boosting | Sequential, fit residuals | Often best accuracy | Tabular data competitions |\n",
                "| XGBoost | Optimized gradient boosting | Speed, regularization | Industry standard for tabular data |\n",
                "| Stacking | Meta-learner | Leverages model diversity | When you have diverse base models |\n",
                "\n",
                "### Recommended Reading\n",
                "\n",
                "- [Scikit-learn Ensemble Methods](https://scikit-learn.org/stable/modules/ensemble.html)\n",
                "- [XGBoost Documentation](https://xgboost.readthedocs.io/)\n",
                "- Chapter 7 of Aurélien Géron, *Hands-On Machine Learning* (Ensemble Learning and Random Forests)\n",
                "\n",
                "### Next Module\n",
                "\n",
                "In **Module 9: Neural Networks and Deep Learning**, we will explore the fundamentals of neural networks, from perceptrons to convolutional networks, and build models using Keras/TensorFlow.\n",
                "\n",
                "---"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.9.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}