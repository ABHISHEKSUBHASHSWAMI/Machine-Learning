{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Module 4: Supervised Learning — Regression\n",
                "\n",
                "---\n",
                "\n",
                "Regression is the branch of supervised learning where we predict a **continuous numerical value**. This module covers the most important regression algorithms, from simple linear regression to regularized methods, along with evaluation metrics and practical code.\n",
                "\n",
                "**What you will learn:**\n",
                "- Simple and Multiple Linear Regression\n",
                "- Polynomial Regression\n",
                "- Regularization: Ridge, Lasso, and ElasticNet\n",
                "- Gradient Descent from scratch\n",
                "- Regression evaluation metrics\n",
                "\n",
                "---\n",
                "\n",
                "## Table of Contents\n",
                "\n",
                "1. [Simple Linear Regression](#1.-Simple-Linear-Regression)\n",
                "2. [Multiple Linear Regression](#2.-Multiple-Linear-Regression)\n",
                "3. [Polynomial Regression](#3.-Polynomial-Regression)\n",
                "4. [Regularization: Ridge, Lasso, ElasticNet](#4.-Regularization)\n",
                "5. [Gradient Descent from Scratch](#5.-Gradient-Descent-from-Scratch)\n",
                "6. [Regression Metrics](#6.-Regression-Metrics)\n",
                "7. [Exercises](#7.-Exercises)\n",
                "8. [Summary and Further Reading](#8.-Summary-and-Further-Reading)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "\n",
                "plt.style.use('seaborn-v0_8-whitegrid')\n",
                "np.random.seed(42)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 1. Simple Linear Regression\n",
                "\n",
                "Simple linear regression models the relationship between a single feature (x) and a target (y) as a straight line:\n",
                "\n",
                "$$y = w_0 + w_1 \\cdot x$$\n",
                "\n",
                "Where:\n",
                "- $w_0$ is the **intercept** (bias)\n",
                "- $w_1$ is the **slope** (weight/coefficient)\n",
                "\n",
                "The model finds the line that minimizes the sum of squared differences between predicted and actual values."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate synthetic data: study hours vs exam score\n",
                "np.random.seed(42)\n",
                "hours = np.random.uniform(1, 10, 50)\n",
                "scores = 10 + 8 * hours + np.random.normal(0, 5, 50)\n",
                "\n",
                "# Visualize\n",
                "fig, ax = plt.subplots(figsize=(10, 6))\n",
                "ax.scatter(hours, scores, color='#2196F3', s=60, edgecolors='white', linewidth=0.5)\n",
                "ax.set_xlabel('Study Hours', fontsize=13)\n",
                "ax.set_ylabel('Exam Score', fontsize=13)\n",
                "ax.set_title('Study Hours vs Exam Score', fontsize=14, fontweight='bold')\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(\"There appears to be a positive linear relationship between study hours and exam scores.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.linear_model import LinearRegression\n",
                "\n",
                "# Reshape X for sklearn (needs 2D array)\n",
                "X = hours.reshape(-1, 1)\n",
                "y = scores\n",
                "\n",
                "# Split\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
                "\n",
                "# Train\n",
                "lr = LinearRegression()\n",
                "lr.fit(X_train, y_train)\n",
                "\n",
                "print(\"Simple Linear Regression Results:\")\n",
                "print(f\"  Intercept (w0):  {lr.intercept_:.4f}\")\n",
                "print(f\"  Slope (w1):      {lr.coef_[0]:.4f}\")\n",
                "print(f\"  Equation:        y = {lr.intercept_:.2f} + {lr.coef_[0]:.2f} * x\")\n",
                "print(f\"  Training R2:     {lr.score(X_train, y_train):.4f}\")\n",
                "print(f\"  Test R2:         {lr.score(X_test, y_test):.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot the regression line with predictions\n",
                "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
                "\n",
                "# Left: Regression line\n",
                "ax = axes[0]\n",
                "ax.scatter(X_train, y_train, color='#2196F3', s=60, edgecolors='white',\n",
                "           linewidth=0.5, label='Training data')\n",
                "ax.scatter(X_test, y_test, color='#FF5722', s=60, edgecolors='white',\n",
                "           linewidth=0.5, marker='s', label='Test data')\n",
                "x_line = np.linspace(0, 11, 100).reshape(-1, 1)\n",
                "ax.plot(x_line, lr.predict(x_line), 'k-', linewidth=2, label='Regression line')\n",
                "ax.set_xlabel('Study Hours', fontsize=13)\n",
                "ax.set_ylabel('Exam Score', fontsize=13)\n",
                "ax.set_title('Linear Regression Fit', fontsize=14, fontweight='bold')\n",
                "ax.legend(fontsize=10)\n",
                "\n",
                "# Right: Residual plot\n",
                "ax = axes[1]\n",
                "y_pred_train = lr.predict(X_train)\n",
                "residuals = y_train - y_pred_train\n",
                "ax.scatter(y_pred_train, residuals, color='#2196F3', s=60, edgecolors='white', linewidth=0.5)\n",
                "ax.axhline(y=0, color='red', linestyle='--', linewidth=2)\n",
                "ax.set_xlabel('Predicted Values', fontsize=13)\n",
                "ax.set_ylabel('Residuals', fontsize=13)\n",
                "ax.set_title('Residual Plot', fontsize=14, fontweight='bold')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(\"A good residual plot shows randomly scattered points around zero,\")\n",
                "print(\"with no obvious pattern. Patterns would suggest a non-linear relationship.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 2. Multiple Linear Regression\n",
                "\n",
                "When we have multiple features, the model becomes:\n",
                "\n",
                "$$y = w_0 + w_1 x_1 + w_2 x_2 + \\ldots + w_p x_p$$\n",
                "\n",
                "This is the same concept extended to higher dimensions. We will use a real-world dataset for this section."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.datasets import fetch_california_housing\n",
                "\n",
                "# Load the California Housing dataset\n",
                "housing = fetch_california_housing()\n",
                "X_housing = pd.DataFrame(housing.data, columns=housing.feature_names)\n",
                "y_housing = housing.target  # Median house value in $100K\n",
                "\n",
                "print(f\"Dataset shape: {X_housing.shape}\")\n",
                "print(f\"Target: Median house value (in $100,000)\")\n",
                "print(f\"\\nFeature descriptions:\")\n",
                "for name in housing.feature_names:\n",
                "    print(f\"  - {name}\")\n",
                "print(f\"\\n{X_housing.describe().round(2)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Split and scale\n",
                "X_train_h, X_test_h, y_train_h, y_test_h = train_test_split(\n",
                "    X_housing, y_housing, test_size=0.2, random_state=42\n",
                ")\n",
                "\n",
                "scaler = StandardScaler()\n",
                "X_train_scaled = scaler.fit_transform(X_train_h)\n",
                "X_test_scaled = scaler.transform(X_test_h)\n",
                "\n",
                "# Train multiple linear regression\n",
                "mlr = LinearRegression()\n",
                "mlr.fit(X_train_scaled, y_train_h)\n",
                "\n",
                "print(\"Multiple Linear Regression on California Housing:\")\n",
                "print(f\"  Training R2: {mlr.score(X_train_scaled, y_train_h):.4f}\")\n",
                "print(f\"  Test R2:     {mlr.score(X_test_scaled, y_test_h):.4f}\")\n",
                "print(f\"\\nFeature coefficients:\")\n",
                "coef_df = pd.DataFrame({\n",
                "    'Feature': housing.feature_names,\n",
                "    'Coefficient': mlr.coef_\n",
                "}).sort_values('Coefficient', key=abs, ascending=False)\n",
                "print(coef_df.to_string(index=False))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize feature importance (coefficients)\n",
                "fig, ax = plt.subplots(figsize=(10, 6))\n",
                "colors = ['#2196F3' if c >= 0 else '#FF5722' for c in coef_df['Coefficient']]\n",
                "ax.barh(coef_df['Feature'], coef_df['Coefficient'], color=colors, edgecolor='white')\n",
                "ax.set_xlabel('Coefficient Value', fontsize=13)\n",
                "ax.set_title('Feature Coefficients (Multiple Linear Regression)', fontsize=14, fontweight='bold')\n",
                "ax.axvline(x=0, color='black', linewidth=0.5)\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(\"Positive coefficients increase the predicted price; negative ones decrease it.\")\n",
                "print(\"Note: coefficients are comparable because we scaled the features first.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 3. Polynomial Regression\n",
                "\n",
                "When the relationship between features and target is non-linear, we can add polynomial terms (x^2, x^3, etc.) to capture curved patterns.\n",
                "\n",
                "$$y = w_0 + w_1 x + w_2 x^2 + w_3 x^3 + \\ldots$$\n",
                "\n",
                "Despite the non-linear features, this is still \"linear\" regression — it is linear in the coefficients."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.preprocessing import PolynomialFeatures\n",
                "from sklearn.metrics import mean_squared_error\n",
                "\n",
                "# Generate non-linear data\n",
                "np.random.seed(42)\n",
                "X_nl = np.sort(np.random.uniform(-3, 3, 60)).reshape(-1, 1)\n",
                "y_nl = 0.5 * X_nl[:, 0]**3 - 2 * X_nl[:, 0]**2 + X_nl[:, 0] + 3 + np.random.normal(0, 3, 60)\n",
                "\n",
                "X_train_nl, X_test_nl, y_train_nl, y_test_nl = train_test_split(\n",
                "    X_nl, y_nl, test_size=0.2, random_state=42\n",
                ")\n",
                "\n",
                "# Compare different polynomial degrees\n",
                "fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
                "degrees = [1, 2, 3, 10]\n",
                "X_plot = np.linspace(-3.5, 3.5, 200).reshape(-1, 1)\n",
                "\n",
                "for idx, degree in enumerate(degrees):\n",
                "    ax = axes[idx]\n",
                "    \n",
                "    # Create polynomial features and fit\n",
                "    poly = PolynomialFeatures(degree=degree)\n",
                "    X_train_poly = poly.fit_transform(X_train_nl)\n",
                "    X_test_poly = poly.transform(X_test_nl)\n",
                "    X_plot_poly = poly.transform(X_plot)\n",
                "    \n",
                "    model = LinearRegression()\n",
                "    model.fit(X_train_poly, y_train_nl)\n",
                "    \n",
                "    train_r2 = model.score(X_train_poly, y_train_nl)\n",
                "    test_r2 = model.score(X_test_poly, y_test_nl)\n",
                "    \n",
                "    ax.scatter(X_train_nl, y_train_nl, color='#2196F3', s=40, edgecolors='white',\n",
                "               linewidth=0.3, label='Train')\n",
                "    ax.scatter(X_test_nl, y_test_nl, color='#FF5722', s=40, edgecolors='white',\n",
                "               linewidth=0.3, marker='s', label='Test')\n",
                "    y_plot = model.predict(X_plot_poly)\n",
                "    y_plot = np.clip(y_plot, -50, 50)  # clip for visibility\n",
                "    ax.plot(X_plot, y_plot, 'k-', linewidth=2)\n",
                "    ax.set_title(f'Degree {degree}\\nTrain R2={train_r2:.3f}, Test R2={test_r2:.3f}',\n",
                "                 fontsize=11, fontweight='bold')\n",
                "    ax.set_ylim(-30, 30)\n",
                "    ax.legend(fontsize=8)\n",
                "\n",
                "plt.suptitle('Polynomial Regression — Effect of Degree', fontsize=15, fontweight='bold')\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(\"Observations:\")\n",
                "print(\"  - Degree 1 (linear): underfits — cannot capture the curve.\")\n",
                "print(\"  - Degree 3: good fit — matches the true underlying relationship.\")\n",
                "print(\"  - Degree 10: overfits — fits noise in training data, performs poorly on test data.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 4. Regularization: Ridge, Lasso, and ElasticNet\n",
                "\n",
                "Regularization adds a penalty term to the loss function to prevent overfitting by discouraging large coefficients.\n",
                "\n",
                "| Method | Penalty | Effect |\n",
                "|--------|---------|--------|\n",
                "| **Ridge (L2)** | Sum of squared coefficients | Shrinks coefficients toward zero |\n",
                "| **Lasso (L1)** | Sum of absolute coefficients | Can set coefficients exactly to zero (feature selection) |\n",
                "| **ElasticNet** | Combination of L1 and L2 | Balances both approaches |"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
                "\n",
                "# Use the California Housing dataset\n",
                "models = {\n",
                "    'Linear Regression': LinearRegression(),\n",
                "    'Ridge (alpha=1.0)': Ridge(alpha=1.0),\n",
                "    'Lasso (alpha=0.1)': Lasso(alpha=0.1),\n",
                "    'ElasticNet (alpha=0.1)': ElasticNet(alpha=0.1, l1_ratio=0.5),\n",
                "}\n",
                "\n",
                "results = []\n",
                "coefficients = {}\n",
                "\n",
                "for name, model in models.items():\n",
                "    model.fit(X_train_scaled, y_train_h)\n",
                "    train_r2 = model.score(X_train_scaled, y_train_h)\n",
                "    test_r2 = model.score(X_test_scaled, y_test_h)\n",
                "    y_pred = model.predict(X_test_scaled)\n",
                "    rmse = np.sqrt(mean_squared_error(y_test_h, y_pred))\n",
                "    results.append({'Model': name, 'Train R2': train_r2, 'Test R2': test_r2, 'RMSE': rmse})\n",
                "    coefficients[name] = model.coef_\n",
                "\n",
                "results_df = pd.DataFrame(results)\n",
                "print(\"Model Comparison:\")\n",
                "print(results_df.to_string(index=False))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize how regularization affects coefficients\n",
                "fig, ax = plt.subplots(figsize=(14, 6))\n",
                "x_pos = np.arange(len(housing.feature_names))\n",
                "width = 0.2\n",
                "colors = ['#2196F3', '#4CAF50', '#FF9800', '#9C27B0']\n",
                "\n",
                "for i, (name, coefs) in enumerate(coefficients.items()):\n",
                "    ax.bar(x_pos + i * width, coefs, width, label=name, color=colors[i], edgecolor='white')\n",
                "\n",
                "ax.set_xticks(x_pos + 1.5 * width)\n",
                "ax.set_xticklabels(housing.feature_names, rotation=45, ha='right')\n",
                "ax.set_ylabel('Coefficient Value', fontsize=13)\n",
                "ax.set_title('Coefficient Comparison Across Regularization Methods', fontsize=14, fontweight='bold')\n",
                "ax.legend(fontsize=10)\n",
                "ax.axhline(y=0, color='black', linewidth=0.5)\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(\"Observations:\")\n",
                "print(\"  - Lasso drives some coefficients to exactly zero (feature selection).\")\n",
                "print(\"  - Ridge shrinks all coefficients but keeps them all non-zero.\")\n",
                "print(\"  - ElasticNet is a mixture of both behaviors.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Effect of alpha (regularization strength) on Ridge coefficients\n",
                "alphas = np.logspace(-2, 4, 50)\n",
                "ridge_coefs = []\n",
                "\n",
                "for alpha in alphas:\n",
                "    ridge = Ridge(alpha=alpha)\n",
                "    ridge.fit(X_train_scaled, y_train_h)\n",
                "    ridge_coefs.append(ridge.coef_)\n",
                "\n",
                "ridge_coefs = np.array(ridge_coefs)\n",
                "\n",
                "fig, ax = plt.subplots(figsize=(12, 6))\n",
                "for i, name in enumerate(housing.feature_names):\n",
                "    ax.plot(alphas, ridge_coefs[:, i], linewidth=2, label=name)\n",
                "\n",
                "ax.set_xscale('log')\n",
                "ax.set_xlabel('Alpha (regularization strength)', fontsize=13)\n",
                "ax.set_ylabel('Coefficient Value', fontsize=13)\n",
                "ax.set_title('Ridge Coefficients as a Function of Alpha', fontsize=14, fontweight='bold')\n",
                "ax.legend(fontsize=9, bbox_to_anchor=(1.05, 1), loc='upper left')\n",
                "ax.axhline(y=0, color='black', linewidth=0.5, linestyle='--')\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(\"As alpha increases, all coefficients shrink toward zero.\")\n",
                "print(\"Small alpha: behaves like ordinary linear regression.\")\n",
                "print(\"Large alpha: heavily penalizes large coefficients, leading to underfitting.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 5. Gradient Descent from Scratch\n",
                "\n",
                "Let us implement linear regression using gradient descent to understand how the optimization actually works."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Simple gradient descent for linear regression: y = w*x + b\n",
                "\n",
                "# Use the study hours data from Section 1\n",
                "X_gd = hours.reshape(-1, 1)\n",
                "y_gd = scores\n",
                "\n",
                "# Normalize X\n",
                "X_mean, X_std = X_gd.mean(), X_gd.std()\n",
                "X_norm = (X_gd - X_mean) / X_std\n",
                "\n",
                "# Initialize parameters\n",
                "w = 0.0  # weight\n",
                "b = 0.0  # bias\n",
                "learning_rate = 0.1\n",
                "n_iterations = 100\n",
                "n_samples = len(X_norm)\n",
                "\n",
                "# Track loss history\n",
                "loss_history = []\n",
                "w_history = []\n",
                "b_history = []\n",
                "\n",
                "for i in range(n_iterations):\n",
                "    # Forward pass: predictions\n",
                "    y_pred = w * X_norm[:, 0] + b\n",
                "    \n",
                "    # Compute loss (Mean Squared Error)\n",
                "    loss = np.mean((y_gd - y_pred) ** 2)\n",
                "    loss_history.append(loss)\n",
                "    w_history.append(w)\n",
                "    b_history.append(b)\n",
                "    \n",
                "    # Compute gradients\n",
                "    dw = -2/n_samples * np.sum(X_norm[:, 0] * (y_gd - y_pred))\n",
                "    db = -2/n_samples * np.sum(y_gd - y_pred)\n",
                "    \n",
                "    # Update parameters\n",
                "    w = w - learning_rate * dw\n",
                "    b = b - learning_rate * db\n",
                "\n",
                "print(\"Gradient Descent Results:\")\n",
                "print(f\"  Final weight (w): {w:.4f}\")\n",
                "print(f\"  Final bias (b):   {b:.4f}\")\n",
                "print(f\"  Final MSE loss:   {loss_history[-1]:.4f}\")\n",
                "print(f\"\\nCompare with sklearn LinearRegression:\")\n",
                "lr_check = LinearRegression()\n",
                "lr_check.fit(X_norm, y_gd)\n",
                "print(f\"  sklearn weight:   {lr_check.coef_[0]:.4f}\")\n",
                "print(f\"  sklearn bias:     {lr_check.intercept_:.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize gradient descent convergence\n",
                "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
                "\n",
                "# Loss over iterations\n",
                "axes[0].plot(loss_history, linewidth=2, color='#FF5722')\n",
                "axes[0].set_xlabel('Iteration', fontsize=13)\n",
                "axes[0].set_ylabel('MSE Loss', fontsize=13)\n",
                "axes[0].set_title('Loss Convergence', fontsize=14, fontweight='bold')\n",
                "\n",
                "# Weight trajectory\n",
                "axes[1].plot(w_history, linewidth=2, color='#2196F3')\n",
                "axes[1].axhline(y=lr_check.coef_[0], color='red', linestyle='--', label='Optimal w')\n",
                "axes[1].set_xlabel('Iteration', fontsize=13)\n",
                "axes[1].set_ylabel('Weight (w)', fontsize=13)\n",
                "axes[1].set_title('Weight Convergence', fontsize=14, fontweight='bold')\n",
                "axes[1].legend(fontsize=10)\n",
                "\n",
                "# Final fit\n",
                "axes[2].scatter(X_norm, y_gd, color='#2196F3', s=40, edgecolors='white', linewidth=0.3)\n",
                "x_plot = np.linspace(X_norm.min(), X_norm.max(), 100)\n",
                "axes[2].plot(x_plot, w * x_plot + b, 'r-', linewidth=2, label='Gradient descent fit')\n",
                "axes[2].set_xlabel('Normalized Study Hours', fontsize=13)\n",
                "axes[2].set_ylabel('Exam Score', fontsize=13)\n",
                "axes[2].set_title('Final Gradient Descent Fit', fontsize=14, fontweight='bold')\n",
                "axes[2].legend(fontsize=10)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 6. Regression Metrics\n",
                "\n",
                "Understanding how to measure model performance is just as important as building the model.\n",
                "\n",
                "| Metric | Formula | Interpretation |\n",
                "|--------|---------|---------------|\n",
                "| **MAE** | Mean of absolute errors | Average magnitude of errors (in original units) |\n",
                "| **MSE** | Mean of squared errors | Penalizes large errors more heavily |\n",
                "| **RMSE** | Square root of MSE | Same units as target variable |\n",
                "| **R2 Score** | 1 - (SS_res / SS_tot) | Proportion of variance explained (1.0 = perfect) |"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
                "\n",
                "# Use the California Housing multiple regression model\n",
                "y_pred_housing = mlr.predict(X_test_scaled)\n",
                "\n",
                "mae = mean_absolute_error(y_test_h, y_pred_housing)\n",
                "mse = mean_squared_error(y_test_h, y_pred_housing)\n",
                "rmse = np.sqrt(mse)\n",
                "r2 = r2_score(y_test_h, y_pred_housing)\n",
                "\n",
                "print(\"Regression Metrics (California Housing — Linear Regression):\")\n",
                "print(f\"  MAE:  {mae:.4f}  (average error: ${mae * 100000:,.0f})\")\n",
                "print(f\"  MSE:  {mse:.4f}\")\n",
                "print(f\"  RMSE: {rmse:.4f}  (typical error: ${rmse * 100000:,.0f})\")\n",
                "print(f\"  R2:   {r2:.4f}  (explains {r2:.1%} of variance)\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Actual vs Predicted plot\n",
                "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
                "\n",
                "# Actual vs Predicted\n",
                "ax = axes[0]\n",
                "ax.scatter(y_test_h, y_pred_housing, alpha=0.4, color='#2196F3', s=20, edgecolors='white', linewidth=0.2)\n",
                "ax.plot([0, 5.5], [0, 5.5], 'r--', linewidth=2, label='Perfect prediction')\n",
                "ax.set_xlabel('Actual Values', fontsize=13)\n",
                "ax.set_ylabel('Predicted Values', fontsize=13)\n",
                "ax.set_title('Actual vs Predicted (California Housing)', fontsize=14, fontweight='bold')\n",
                "ax.legend(fontsize=11)\n",
                "\n",
                "# Residual distribution\n",
                "ax = axes[1]\n",
                "residuals_h = y_test_h - y_pred_housing\n",
                "ax.hist(residuals_h, bins=40, color='#2196F3', edgecolor='white', alpha=0.7)\n",
                "ax.axvline(x=0, color='red', linestyle='--', linewidth=2)\n",
                "ax.set_xlabel('Residual (Actual - Predicted)', fontsize=13)\n",
                "ax.set_ylabel('Frequency', fontsize=13)\n",
                "ax.set_title('Residual Distribution', fontsize=14, fontweight='bold')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(\"A well-performing model shows points clustered tightly around the diagonal line,\")\n",
                "print(\"and residuals centered around zero with a roughly normal distribution.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 7. Exercises\n",
                "\n",
                "### Exercise 1: Boston-Style Housing Regression"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Exercise 1: Using the California Housing dataset:\n",
                "# 1. Split the data (80/20) and scale the features\n",
                "# 2. Train Linear Regression, Ridge (alpha=10), and Lasso (alpha=0.01)\n",
                "# 3. Compute MAE, RMSE, and R2 for each model on the test set\n",
                "# 4. Create a bar chart comparing RMSE across models\n",
                "# 5. Which model performs best? Why?\n",
                "\n",
                "# Your code here:\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Exercise 2: Polynomial Degree Selection"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Exercise 2: For the non-linear data below:\n",
                "# 1. Try polynomial degrees from 1 to 15\n",
                "# 2. Record training and test R2 for each degree\n",
                "# 3. Plot training R2 and test R2 vs degree on the same chart\n",
                "# 4. Identify the optimal degree (highest test R2)\n",
                "# 5. Comment on the overfitting behavior at high degrees\n",
                "\n",
                "np.random.seed(42)\n",
                "X_ex = np.sort(np.random.uniform(-2, 2, 80)).reshape(-1, 1)\n",
                "y_ex = np.sin(2 * X_ex[:, 0]) + 0.3 * np.random.randn(80)\n",
                "\n",
                "# Your code here:\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Exercise 3: Gradient Descent Variations"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Exercise 3: Modify the gradient descent implementation from Section 5:\n",
                "# 1. Try learning rates: 0.001, 0.01, 0.1, 0.5\n",
                "# 2. Run each for 200 iterations\n",
                "# 3. Plot the loss curves for all four learning rates on the same chart\n",
                "# 4. Which learning rate converges the fastest?\n",
                "# 5. Does any learning rate diverge?\n",
                "\n",
                "# Your code here:\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 8. Summary and Further Reading\n",
                "\n",
                "### What We Covered\n",
                "\n",
                "- **Simple Linear Regression**: Fitting a line to data with one feature.\n",
                "- **Multiple Linear Regression**: Extending to multiple features using the California Housing dataset.\n",
                "- **Polynomial Regression**: Capturing non-linear relationships by adding polynomial terms.\n",
                "- **Regularization**: Ridge (L2), Lasso (L1), and ElasticNet to prevent overfitting and perform feature selection.\n",
                "- **Gradient Descent**: How optimization works under the hood.\n",
                "- **Metrics**: MAE, MSE, RMSE, and R2 for evaluating regression models.\n",
                "\n",
                "### Recommended Reading\n",
                "\n",
                "- [Scikit-learn Linear Models Documentation](https://scikit-learn.org/stable/modules/linear_model.html)\n",
                "- Chapter 4 of Aurélien Géron, *Hands-On Machine Learning* (Training Models)\n",
                "- Chapter 3 and 6 of *Introduction to Statistical Learning (ISLR)* — Linear Regression and Regularization\n",
                "\n",
                "### Next Module\n",
                "\n",
                "In **Module 5: Supervised Learning — Classification**, we will cover the algorithms used when the target is a category rather than a number: Logistic Regression, KNN, SVM, Decision Trees, and Naive Bayes.\n",
                "\n",
                "---"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.9.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}