{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Module 1: Introduction to Machine Learning\n",
                "\n",
                "---\n",
                "\n",
                "Welcome to the Machine Learning Courseware. This is the first module in a 10-part series that will take you from the fundamentals of Machine Learning to advanced topics like Neural Networks and NLP. By the end of this course, you will be able to build, evaluate, and deploy ML models with confidence.\n",
                "\n",
                "**Prerequisites:** Basic Python, NumPy, and Pandas knowledge.\n",
                "\n",
                "---\n",
                "\n",
                "## Table of Contents\n",
                "\n",
                "1. [What is Machine Learning?](#1.-What-is-Machine-Learning?)\n",
                "2. [Types of Machine Learning](#2.-Types-of-Machine-Learning)\n",
                "3. [The Machine Learning Workflow](#3.-The-Machine-Learning-Workflow)\n",
                "4. [Key Terminology](#4.-Key-Terminology)\n",
                "5. [Hands-On: Your First ML Model](#5.-Hands-On:-Your-First-ML-Model)\n",
                "6. [Exercises](#6.-Exercises)\n",
                "7. [Summary and Further Reading](#7.-Summary-and-Further-Reading)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 1. What is Machine Learning?\n",
                "\n",
                "### Definition\n",
                "\n",
                "> *\"Machine Learning is the field of study that gives computers the ability to learn without being explicitly programmed.\"* — **Arthur Samuel, 1959**\n",
                "\n",
                "> *\"A computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E.\"* — **Tom Mitchell, 1997**\n",
                "\n",
                "In simpler terms, Machine Learning is about teaching computers to find patterns in data and make decisions or predictions based on those patterns.\n",
                "\n",
                "### Traditional Programming vs Machine Learning\n",
                "\n",
                "| Traditional Programming | Machine Learning |\n",
                "|---|---|\n",
                "| Input: **Data + Rules** | Input: **Data + Expected Outputs** |\n",
                "| Output: **Answers** | Output: **Rules (Model)** |\n",
                "| Human writes the logic | Machine learns the logic |\n",
                "| Example: Spam filter with hand-coded rules | Example: Spam filter that learns from labeled emails |\n",
                "\n",
                "### Real-World Applications\n",
                "\n",
                "Machine Learning is used across a wide range of industries:\n",
                "\n",
                "- **Email**: Spam filtering, smart replies\n",
                "- **Search engines**: Google ranking, autocomplete\n",
                "- **E-commerce**: Product recommendations (Amazon, Netflix)\n",
                "- **Healthcare**: Disease diagnosis, drug discovery\n",
                "- **Autonomous vehicles**: Self-driving cars (Tesla, Waymo)\n",
                "- **Virtual assistants**: Siri, Alexa, Google Assistant\n",
                "- **Finance**: Fraud detection, credit scoring\n",
                "- **Social media**: Face recognition, content moderation"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 2. Types of Machine Learning\n",
                "\n",
                "Machine Learning can be broadly categorized into **three types**:\n",
                "\n",
                "### 2.1 Supervised Learning\n",
                "\n",
                "The model learns from **labeled data** — each input has a corresponding correct output.\n",
                "\n",
                "- **Goal**: Learn a mapping from inputs to outputs\n",
                "- **Types**:\n",
                "  - **Regression**: Predict a continuous number (e.g., house price)\n",
                "  - **Classification**: Predict a category or class (e.g., spam or not spam)\n",
                "- **Common algorithms**: Linear Regression, Logistic Regression, Decision Trees, SVM, KNN\n",
                "\n",
                "### 2.2 Unsupervised Learning\n",
                "\n",
                "The model works with **unlabeled data** — it must discover hidden patterns on its own.\n",
                "\n",
                "- **Goal**: Find structure in the data\n",
                "- **Types**:\n",
                "  - **Clustering**: Group similar data points (e.g., customer segments)\n",
                "  - **Dimensionality Reduction**: Reduce number of features while preserving information (e.g., PCA)\n",
                "  - **Association**: Find rules in transactions (e.g., market basket analysis)\n",
                "- **Common algorithms**: K-Means, DBSCAN, PCA, t-SNE\n",
                "\n",
                "### 2.3 Reinforcement Learning\n",
                "\n",
                "The model learns by **interacting with an environment** and receiving rewards or penalties.\n",
                "\n",
                "- **Goal**: Learn a strategy (policy) that maximizes cumulative reward\n",
                "- **Key concepts**: Agent, Environment, State, Action, Reward\n",
                "- **Applications**: Game playing (AlphaGo), robotics, recommendation systems"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualizing the three types of Machine Learning\n",
                "\n",
                "import matplotlib.pyplot as plt\n",
                "import matplotlib.patches as mpatches\n",
                "import numpy as np\n",
                "\n",
                "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
                "\n",
                "# --- Supervised Learning (Classification Example) ---\n",
                "np.random.seed(42)\n",
                "x_class0 = np.random.randn(30, 2) + np.array([2, 2])\n",
                "x_class1 = np.random.randn(30, 2) + np.array([-2, -2])\n",
                "axes[0].scatter(x_class0[:, 0], x_class0[:, 1], c='#2196F3', label='Class A',\n",
                "                s=60, edgecolors='white', linewidth=0.5)\n",
                "axes[0].scatter(x_class1[:, 0], x_class1[:, 1], c='#FF5722', label='Class B',\n",
                "                s=60, edgecolors='white', linewidth=0.5)\n",
                "x_line = np.linspace(-5, 5, 100)\n",
                "axes[0].plot(x_line, -x_line, 'k--', alpha=0.5, linewidth=2)\n",
                "axes[0].set_title('Supervised Learning\\n(Classification)', fontsize=14, fontweight='bold')\n",
                "axes[0].legend(fontsize=11)\n",
                "axes[0].set_xlabel('Feature 1')\n",
                "axes[0].set_ylabel('Feature 2')\n",
                "axes[0].set_xlim(-5, 5)\n",
                "axes[0].set_ylim(-5, 5)\n",
                "\n",
                "# --- Unsupervised Learning (Clustering Example) ---\n",
                "from sklearn.datasets import make_blobs\n",
                "X_blobs, labels = make_blobs(n_samples=90, centers=3, cluster_std=0.8, random_state=42)\n",
                "colors = ['#4CAF50', '#FF9800', '#9C27B0']\n",
                "for i in range(3):\n",
                "    mask = (labels == i)\n",
                "    axes[1].scatter(X_blobs[mask, 0], X_blobs[mask, 1], c=colors[i],\n",
                "                    s=60, edgecolors='white', linewidth=0.5)\n",
                "axes[1].set_title('Unsupervised Learning\\n(Clustering)', fontsize=14, fontweight='bold')\n",
                "axes[1].set_xlabel('Feature 1')\n",
                "axes[1].set_ylabel('Feature 2')\n",
                "axes[1].text(0.05, 0.95, 'No labels provided —\\nmodel discovers groups',\n",
                "             transform=axes[1].transAxes, fontsize=11, verticalalignment='top',\n",
                "             style='italic', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
                "\n",
                "# --- Reinforcement Learning (Grid World Illustration) ---\n",
                "grid = np.zeros((5, 5))\n",
                "axes[2].imshow(grid, cmap='Pastel1', extent=[0, 5, 0, 5])\n",
                "for i in range(6):\n",
                "    axes[2].axhline(i, color='gray', linewidth=0.5)\n",
                "    axes[2].axvline(i, color='gray', linewidth=0.5)\n",
                "axes[2].text(0.5, 0.5, 'Agent', fontsize=11, ha='center', va='center',\n",
                "             bbox=dict(boxstyle='round', facecolor='#2196F3', alpha=0.7), color='white')\n",
                "axes[2].text(4.5, 4.5, 'Goal', fontsize=11, ha='center', va='center',\n",
                "             bbox=dict(boxstyle='round', facecolor='#4CAF50', alpha=0.7), color='white')\n",
                "axes[2].add_patch(plt.Rectangle((2, 1), 1, 1, color='#F44336', alpha=0.5))\n",
                "axes[2].add_patch(plt.Rectangle((1, 3), 1, 1, color='#F44336', alpha=0.5))\n",
                "axes[2].add_patch(plt.Rectangle((3, 2), 1, 1, color='#F44336', alpha=0.5))\n",
                "axes[2].annotate('', xy=(1.5, 0.5), xytext=(0.5, 0.5),\n",
                "                arrowprops=dict(arrowstyle='->', color='#2196F3', lw=2))\n",
                "axes[2].annotate('', xy=(1.5, 1.5), xytext=(1.5, 0.5),\n",
                "                arrowprops=dict(arrowstyle='->', color='#2196F3', lw=2))\n",
                "axes[2].set_title('Reinforcement Learning\\n(Agent navigates to goal)', fontsize=14, fontweight='bold')\n",
                "axes[2].set_xticks([])\n",
                "axes[2].set_yticks([])\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.suptitle('Three Types of Machine Learning', fontsize=16, fontweight='bold', y=1.02)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 3. The Machine Learning Workflow\n",
                "\n",
                "Every ML project follows a similar pipeline:\n",
                "\n",
                "```\n",
                "Define Problem --> Collect Data --> Prepare Data --> Choose Model --> Train Model --> Evaluate --> Deploy\n",
                "```\n",
                "\n",
                "| Step | Description | Tools / Libraries |\n",
                "|------|-------------|-------------------|\n",
                "| 1. Define Problem | What do you want to predict or discover? | Domain knowledge |\n",
                "| 2. Collect Data | Gather relevant data | APIs, CSVs, web scraping, databases |\n",
                "| 3. Prepare Data | Clean, transform, engineer features | Pandas, NumPy, Scikit-learn |\n",
                "| 4. Choose Model | Pick an appropriate algorithm | Scikit-learn, XGBoost, TensorFlow |\n",
                "| 5. Train Model | Fit the model to training data | `model.fit()` |\n",
                "| 6. Evaluate Model | Measure performance on unseen data | Accuracy, F1, RMSE, ROC |\n",
                "| 7. Deploy / Iterate | Put in production or improve | Flask, Docker, cloud services |\n",
                "\n",
                "In practice, steps 3 through 6 are often repeated multiple times as you refine your approach."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 4. Key Terminology\n",
                "\n",
                "Before we dive into code, let us define the essential terms you will encounter throughout this course:\n",
                "\n",
                "| Term | Definition | Example |\n",
                "|------|-----------|--------|\n",
                "| **Feature** | An input variable used to make predictions | House size, number of rooms |\n",
                "| **Label / Target** | The output variable we want to predict | House price |\n",
                "| **Sample / Instance** | A single data point (one row) | One house record |\n",
                "| **Dataset** | Collection of samples | Table of 1000 houses |\n",
                "| **Training Set** | Data used to train the model | 80% of the dataset |\n",
                "| **Test Set** | Data used to evaluate the model | 20% of the dataset |\n",
                "| **Model** | The mathematical function learned from data | Linear equation, decision tree |\n",
                "| **Training** | Process of learning patterns from data | Calling `model.fit(X, y)` |\n",
                "| **Inference** | Using a trained model on new data | Calling `model.predict(X_new)` |\n",
                "| **Overfitting** | Model memorizes training data, performs poorly on new data | High train accuracy, low test accuracy |\n",
                "| **Underfitting** | Model is too simple to capture underlying patterns | Low accuracy on both train and test |\n",
                "| **Hyperparameter** | A setting configured before training begins | Learning rate, tree depth |"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 5. Hands-On: Your First ML Model\n",
                "\n",
                "Let us build a complete ML pipeline using the well-known **Iris dataset**. This dataset contains measurements of 150 iris flowers from 3 species.\n",
                "\n",
                "We will follow the ML workflow step by step."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Step 1 and 2: Define the Problem and Collect Data\n",
                "\n",
                "**Problem**: Given measurements of an iris flower (sepal length, sepal width, petal length, petal width), predict which species it belongs to.\n",
                "\n",
                "**Data**: We will use `sklearn.datasets.load_iris()`, a classic dataset that comes built into scikit-learn."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import essential libraries\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from sklearn.datasets import load_iris\n",
                "\n",
                "# Set a clean visual style\n",
                "plt.style.use('seaborn-v0_8-whitegrid')\n",
                "sns.set_palette('husl')\n",
                "\n",
                "# Load the Iris dataset\n",
                "iris = load_iris()\n",
                "\n",
                "# Create a DataFrame for easier exploration\n",
                "df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
                "df['species'] = pd.Categorical.from_codes(iris.target, iris.target_names)\n",
                "\n",
                "print(f\"Dataset shape: {df.shape}\")\n",
                "print(f\"\\nFeature names: {iris.feature_names}\")\n",
                "print(f\"Target classes: {list(iris.target_names)}\")\n",
                "print(f\"\\nSamples per class:\")\n",
                "print(df['species'].value_counts())\n",
                "print(\"\\n--- First 10 rows ---\")\n",
                "df.head(10)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Step 3: Explore and Prepare the Data\n",
                "\n",
                "Before training a model, we need to understand the structure of our data. Visualization is a critical part of this step."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Basic statistics\n",
                "print(\"=\" * 60)\n",
                "print(\"DESCRIPTIVE STATISTICS\")\n",
                "print(\"=\" * 60)\n",
                "df.describe().round(2)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check for missing values\n",
                "print(\"Missing values per column:\")\n",
                "print(df.isnull().sum())\n",
                "print(f\"\\nTotal missing values: {df.isnull().sum().sum()}\")\n",
                "print(\"\\nNo missing values found in this dataset.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Pairplot — visualize relationships between all features, colored by species\n",
                "g = sns.pairplot(df, hue='species', height=2.5,\n",
                "                 plot_kws={'alpha': 0.7, 's': 50, 'edgecolor': 'white', 'linewidth': 0.5},\n",
                "                 diag_kws={'alpha': 0.6})\n",
                "g.figure.suptitle('Iris Dataset — Feature Relationships by Species', y=1.02,\n",
                "                  fontsize=16, fontweight='bold')\n",
                "plt.show()\n",
                "\n",
                "print(\"\\nObservations:\")\n",
                "print(\"  - Setosa is clearly separable from the other two species.\")\n",
                "print(\"  - Versicolor and Virginica overlap in several feature spaces.\")\n",
                "print(\"  - Petal length and petal width appear to be the most discriminative features.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Feature distributions by species\n",
                "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
                "colors = ['#2196F3', '#FF9800', '#4CAF50']\n",
                "\n",
                "for idx, feature in enumerate(iris.feature_names):\n",
                "    ax = axes[idx // 2, idx % 2]\n",
                "    for i, species in enumerate(iris.target_names):\n",
                "        data = df[df['species'] == species][feature]\n",
                "        ax.hist(data, bins=15, alpha=0.6, label=species, color=colors[i], edgecolor='white')\n",
                "    ax.set_title(f'{feature}', fontsize=13, fontweight='bold')\n",
                "    ax.set_xlabel('Value')\n",
                "    ax.set_ylabel('Frequency')\n",
                "    ax.legend(fontsize=10)\n",
                "\n",
                "plt.suptitle('Feature Distributions by Species', fontsize=16, fontweight='bold')\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Correlation heatmap\n",
                "fig, ax = plt.subplots(figsize=(8, 6))\n",
                "correlation = df.drop('species', axis=1).corr()\n",
                "sns.heatmap(correlation, annot=True, cmap='RdYlBu_r', center=0, ax=ax,\n",
                "            square=True, linewidths=1, fmt='.2f',\n",
                "            cbar_kws={'label': 'Correlation Coefficient'})\n",
                "ax.set_title('Feature Correlation Heatmap', fontsize=14, fontweight='bold')\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(\"\\nObservations:\")\n",
                "print(\"  - Petal length and petal width are highly correlated (0.96).\")\n",
                "print(\"  - Sepal width has low correlation with the other features.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Step 4: Split the Data\n",
                "\n",
                "We split our data into **training** and **test** sets. The model learns from the training set and is evaluated on the held-out test set, which simulates unseen real-world data."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.model_selection import train_test_split\n",
                "\n",
                "# Features (X) and Target (y)\n",
                "X = iris.data    # shape: (150, 4)\n",
                "y = iris.target  # shape: (150,)\n",
                "\n",
                "# Split: 80% training, 20% testing\n",
                "X_train, X_test, y_train, y_test = train_test_split(\n",
                "    X, y,\n",
                "    test_size=0.2,     # 20% reserved for testing\n",
                "    random_state=42,   # ensures reproducibility\n",
                "    stratify=y         # maintains class distribution in both sets\n",
                ")\n",
                "\n",
                "print(f\"Training set: {X_train.shape[0]} samples\")\n",
                "print(f\"Test set:     {X_test.shape[0]} samples\")\n",
                "print(f\"\\nClass distribution in training set: {np.bincount(y_train)}\")\n",
                "print(f\"Class distribution in test set:     {np.bincount(y_test)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Step 5: Choose and Train a Model\n",
                "\n",
                "We will use **K-Nearest Neighbors (KNN)** — one of the simplest and most intuitive ML algorithms.\n",
                "\n",
                "**How KNN works:**\n",
                "1. Given a new data point, find the **K closest points** in the training set (using distance).\n",
                "2. Assign the **majority class** among those K neighbors to the new point.\n",
                "\n",
                "It is a non-parametric, instance-based algorithm — it does not learn an explicit function; instead, it memorizes all training data and compares at prediction time."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.neighbors import KNeighborsClassifier\n",
                "\n",
                "# Create a KNN classifier with K=5 (consider 5 nearest neighbors)\n",
                "knn = KNeighborsClassifier(n_neighbors=5)\n",
                "\n",
                "# Train the model on the training data\n",
                "knn.fit(X_train, y_train)\n",
                "\n",
                "print(\"Model trained successfully.\")\n",
                "print(f\"\\nModel: {knn}\")\n",
                "print(f\"Number of training samples: {X_train.shape[0]}\")\n",
                "print(f\"Number of features: {X_train.shape[1]}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Step 6: Evaluate the Model\n",
                "\n",
                "Now let us measure how well our model performs on the **test set** — data it has never seen during training."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
                "\n",
                "# Make predictions on the test set\n",
                "y_pred = knn.predict(X_test)\n",
                "\n",
                "# Overall accuracy\n",
                "accuracy = accuracy_score(y_test, y_pred)\n",
                "print(f\"Test Accuracy: {accuracy:.2%}\")\n",
                "\n",
                "# Detailed classification report\n",
                "print(\"\\n\" + \"=\" * 60)\n",
                "print(\"CLASSIFICATION REPORT\")\n",
                "print(\"=\" * 60)\n",
                "print(classification_report(y_test, y_pred, target_names=iris.target_names))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Confusion matrix visualization\n",
                "fig, ax = plt.subplots(figsize=(8, 6))\n",
                "cm = confusion_matrix(y_test, y_pred)\n",
                "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
                "            xticklabels=iris.target_names,\n",
                "            yticklabels=iris.target_names,\n",
                "            square=True, linewidths=1, ax=ax,\n",
                "            annot_kws={'size': 16})\n",
                "ax.set_xlabel('Predicted Label', fontsize=13)\n",
                "ax.set_ylabel('True Label', fontsize=13)\n",
                "ax.set_title('Confusion Matrix — KNN Classifier', fontsize=14, fontweight='bold')\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(\"\\nHow to read the confusion matrix:\")\n",
                "print(\"  - Diagonal entries (top-left to bottom-right) represent correct predictions.\")\n",
                "print(\"  - Off-diagonal entries represent misclassifications.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Making Predictions on New Data\n",
                "\n",
                "Let us simulate predicting the species of a flower whose measurements we have not seen before."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Predict the species of a new flower\n",
                "new_flower = np.array([[5.1, 3.5, 1.4, 0.2]])  # sepal_l, sepal_w, petal_l, petal_w\n",
                "\n",
                "prediction = knn.predict(new_flower)\n",
                "probabilities = knn.predict_proba(new_flower)\n",
                "\n",
                "print(\"New Flower Measurements:\")\n",
                "print(f\"   Sepal Length: {new_flower[0][0]} cm\")\n",
                "print(f\"   Sepal Width:  {new_flower[0][1]} cm\")\n",
                "print(f\"   Petal Length: {new_flower[0][2]} cm\")\n",
                "print(f\"   Petal Width:  {new_flower[0][3]} cm\")\n",
                "print(f\"\\nPredicted Species: {iris.target_names[prediction[0]]}\")\n",
                "print(f\"\\nPrediction Probabilities:\")\n",
                "for name, prob in zip(iris.target_names, probabilities[0]):\n",
                "    bar = '#' * int(prob * 30)\n",
                "    print(f\"   {name:>12s}: {prob:.2%} {bar}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Effect of K on Model Performance\n",
                "\n",
                "The choice of **K** (number of neighbors) is a hyperparameter that significantly affects the model. Let us examine how different values of K influence accuracy."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test different values of K\n",
                "k_values = range(1, 26)\n",
                "train_accuracies = []\n",
                "test_accuracies = []\n",
                "\n",
                "for k in k_values:\n",
                "    knn_k = KNeighborsClassifier(n_neighbors=k)\n",
                "    knn_k.fit(X_train, y_train)\n",
                "    train_accuracies.append(knn_k.score(X_train, y_train))\n",
                "    test_accuracies.append(knn_k.score(X_test, y_test))\n",
                "\n",
                "# Plot training vs test accuracy for each K\n",
                "fig, ax = plt.subplots(figsize=(12, 6))\n",
                "ax.plot(k_values, train_accuracies, 'o-', label='Training Accuracy',\n",
                "        color='#2196F3', linewidth=2, markersize=6)\n",
                "ax.plot(k_values, test_accuracies, 's-', label='Test Accuracy',\n",
                "        color='#FF5722', linewidth=2, markersize=6)\n",
                "ax.fill_between(k_values, test_accuracies, alpha=0.1, color='#FF5722')\n",
                "\n",
                "best_k = k_values[np.argmax(test_accuracies)]\n",
                "ax.axvline(x=best_k, color='green', linestyle='--', alpha=0.7, label=f'Best K={best_k}')\n",
                "\n",
                "ax.set_xlabel('K (Number of Neighbors)', fontsize=13)\n",
                "ax.set_ylabel('Accuracy', fontsize=13)\n",
                "ax.set_title('KNN: Effect of K on Model Accuracy', fontsize=14, fontweight='bold')\n",
                "ax.legend(fontsize=12)\n",
                "ax.set_xticks(k_values)\n",
                "ax.set_ylim(0.85, 1.02)\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(f\"\\nBest K: {best_k} (Test Accuracy: {max(test_accuracies):.2%})\")\n",
                "print(\"\\nKey Insight:\")\n",
                "print(\"  - Small K: the model is highly sensitive to noise (risk of overfitting).\")\n",
                "print(\"  - Large K: the model becomes overly general (risk of underfitting).\")\n",
                "print(\"  - The optimal K balances these two extremes.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 6. Exercises\n",
                "\n",
                "### Exercise 1: Classify the Scenario\n",
                "\n",
                "For each scenario below, determine whether it is **Supervised**, **Unsupervised**, or **Reinforcement Learning**, and whether the task is **Classification**, **Regression**, or **Clustering**."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Exercise 1: Replace the \"???\" with your answers\n",
                "\n",
                "scenarios = {\n",
                "    \"Predicting house prices based on features\": {\n",
                "        \"ml_type\": \"???\",      # Supervised / Unsupervised / Reinforcement\n",
                "        \"task_type\": \"???\",     # Regression / Classification / Clustering\n",
                "    },\n",
                "    \"Grouping customers by purchasing behavior\": {\n",
                "        \"ml_type\": \"???\",\n",
                "        \"task_type\": \"???\",\n",
                "    },\n",
                "    \"Email spam detection\": {\n",
                "        \"ml_type\": \"???\",\n",
                "        \"task_type\": \"???\",\n",
                "    },\n",
                "    \"A robot learning to walk\": {\n",
                "        \"ml_type\": \"???\",\n",
                "        \"task_type\": \"???\",\n",
                "    },\n",
                "    \"Predicting tomorrow's temperature\": {\n",
                "        \"ml_type\": \"???\",\n",
                "        \"task_type\": \"???\",\n",
                "    },\n",
                "}\n",
                "\n",
                "for scenario, answers in scenarios.items():\n",
                "    print(f\"Scenario: {scenario}\")\n",
                "    print(f\"   ML Type:   {answers['ml_type']}\")\n",
                "    print(f\"   Task Type: {answers['task_type']}\")\n",
                "    print()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<details>\n",
                "<summary><b>Click here to see the answers</b></summary>\n",
                "\n",
                "| Scenario | ML Type | Task Type |\n",
                "|---|---|---|\n",
                "| Predicting house prices | Supervised | Regression |\n",
                "| Grouping customers | Unsupervised | Clustering |\n",
                "| Email spam detection | Supervised | Classification |\n",
                "| A robot learning to walk | Reinforcement | Policy learning |\n",
                "| Predicting temperature | Supervised | Regression |\n",
                "\n",
                "</details>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Exercise 2: Explore a New Dataset\n",
                "\n",
                "Load the **Wine dataset** from scikit-learn, explore it, and build a KNN classifier. Follow the same steps we used for the Iris dataset above."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Exercise 2: Complete the code below\n",
                "from sklearn.datasets import load_wine\n",
                "\n",
                "# Step 1: Load the dataset\n",
                "wine = load_wine()\n",
                "\n",
                "# TODO: Create a DataFrame from the wine dataset\n",
                "# wine_df = pd.DataFrame(???)\n",
                "\n",
                "# TODO: Print the shape, feature names, and target names\n",
                "\n",
                "# TODO: Display the first 5 rows\n",
                "\n",
                "# TODO: Check for missing values\n",
                "\n",
                "# TODO: Split the data into train and test sets (80/20)\n",
                "\n",
                "# TODO: Train a KNN classifier (try K=3)\n",
                "\n",
                "# TODO: Print the test accuracy\n",
                "\n",
                "# TODO: Try different K values and plot the results\n",
                "\n",
                "print(\"Hint: Follow the exact same steps we used for the Iris dataset above.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<details>\n",
                "<summary><b>Click here for the solution</b></summary>\n",
                "\n",
                "```python\n",
                "from sklearn.datasets import load_wine\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.neighbors import KNeighborsClassifier\n",
                "from sklearn.metrics import accuracy_score\n",
                "\n",
                "wine = load_wine()\n",
                "wine_df = pd.DataFrame(data=wine.data, columns=wine.feature_names)\n",
                "wine_df['target'] = wine.target\n",
                "\n",
                "print(f\"Shape: {wine_df.shape}\")\n",
                "print(f\"Features: {wine.feature_names}\")\n",
                "print(f\"Classes: {wine.target_names}\")\n",
                "print(wine_df.head())\n",
                "print(f\"Missing values: {wine_df.isnull().sum().sum()}\")\n",
                "\n",
                "X_train, X_test, y_train, y_test = train_test_split(\n",
                "    wine.data, wine.target, test_size=0.2, random_state=42, stratify=wine.target\n",
                ")\n",
                "\n",
                "knn = KNeighborsClassifier(n_neighbors=3)\n",
                "knn.fit(X_train, y_train)\n",
                "print(f\"Test Accuracy: {knn.score(X_test, y_test):.2%}\")\n",
                "```\n",
                "\n",
                "</details>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Exercise 3: Conceptual Questions\n",
                "\n",
                "Answer the following in your own words (add a markdown cell below each question):\n",
                "\n",
                "1. What is the difference between a feature and a label?\n",
                "2. Why is it important to split data into training and test sets?\n",
                "3. What happens if K=1 in KNN? What about K=n (total number of training samples)?\n",
                "4. Can you think of a real-world problem in your domain where Machine Learning could be useful?"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 7. Summary and Further Reading\n",
                "\n",
                "### What We Covered\n",
                "\n",
                "- Machine Learning enables computers to learn patterns from data rather than following explicit rules.\n",
                "- There are three main categories: Supervised, Unsupervised, and Reinforcement Learning.\n",
                "- The ML workflow consists of: problem definition, data collection, preprocessing, model selection, training, evaluation, and deployment.\n",
                "- We built our first classifier using KNN on the Iris dataset and achieved strong accuracy.\n",
                "- The hyperparameter K controls the trade-off between overfitting and underfitting.\n",
                "\n",
                "### Recommended Reading\n",
                "\n",
                "- [Scikit-learn User Guide](https://scikit-learn.org/stable/user_guide.html)\n",
                "- [Google Machine Learning Crash Course](https://developers.google.com/machine-learning/crash-course)\n",
                "- Aurélien Géron, *Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow*, O'Reilly, 3rd Edition\n",
                "- Gareth James et al., *An Introduction to Statistical Learning (ISLR)* — freely available at https://www.statlearning.com\n",
                "\n",
                "### Next Module\n",
                "\n",
                "In **Module 2: Mathematical Foundations**, we will cover the essential mathematics behind ML algorithms — linear algebra, statistics, probability, and calculus — all through Python code and visualizations.\n",
                "\n",
                "---"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".ml_env",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
