{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Module 5: Supervised Learning — Classification\n",
                "\n",
                "---\n",
                "\n",
                "Classification is the supervised learning task where we predict a **discrete category** rather than a continuous number. This module covers the most widely used classification algorithms, from logistic regression to decision trees, along with decision boundary visualizations.\n",
                "\n",
                "**What you will learn:**\n",
                "- Logistic Regression\n",
                "- K-Nearest Neighbors (KNN)\n",
                "- Support Vector Machines (SVM)\n",
                "- Decision Trees\n",
                "- Naive Bayes\n",
                "- Comparing classifiers on the same dataset\n",
                "\n",
                "---\n",
                "\n",
                "## Table of Contents\n",
                "\n",
                "1. [Logistic Regression](#1.-Logistic-Regression)\n",
                "2. [K-Nearest Neighbors](#2.-K-Nearest-Neighbors)\n",
                "3. [Support Vector Machines](#3.-Support-Vector-Machines)\n",
                "4. [Decision Trees](#4.-Decision-Trees)\n",
                "5. [Naive Bayes](#5.-Naive-Bayes)\n",
                "6. [Classifier Comparison](#6.-Classifier-Comparison)\n",
                "7. [Exercises](#7.-Exercises)\n",
                "8. [Summary and Further Reading](#8.-Summary-and-Further-Reading)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from matplotlib.colors import ListedColormap\n",
                "\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
                "from sklearn.datasets import load_breast_cancer, make_moons, make_classification\n",
                "\n",
                "plt.style.use('seaborn-v0_8-whitegrid')\n",
                "np.random.seed(42)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Utility function: plot decision boundaries for any 2D classifier\n",
                "def plot_decision_boundary(model, X, y, ax, title='', xlabel='Feature 1', ylabel='Feature 2'):\n",
                "    \"\"\"Plot the decision boundary of a fitted classifier on 2D data.\"\"\"\n",
                "    h = 0.02\n",
                "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
                "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
                "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
                "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
                "    Z = Z.reshape(xx.shape)\n",
                "    \n",
                "    cmap_bg = ListedColormap(['#BBDEFB', '#FFCCBC'])\n",
                "    cmap_pts = ListedColormap(['#1565C0', '#E64A19'])\n",
                "    \n",
                "    ax.contourf(xx, yy, Z, alpha=0.4, cmap=cmap_bg)\n",
                "    ax.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_pts, s=30, edgecolors='white', linewidth=0.5)\n",
                "    ax.set_xlabel(xlabel, fontsize=11)\n",
                "    ax.set_ylabel(ylabel, fontsize=11)\n",
                "    ax.set_title(title, fontsize=13, fontweight='bold')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Prepare a 2D dataset for decision boundary visualization (Moons dataset)\n",
                "X_moons, y_moons = make_moons(n_samples=300, noise=0.25, random_state=42)\n",
                "\n",
                "# Also prepare the Breast Cancer dataset for full-feature experiments\n",
                "cancer = load_breast_cancer()\n",
                "X_cancer = cancer.data\n",
                "y_cancer = cancer.target\n",
                "\n",
                "X_train_c, X_test_c, y_train_c, y_test_c = train_test_split(\n",
                "    X_cancer, y_cancer, test_size=0.2, random_state=42, stratify=y_cancer\n",
                ")\n",
                "scaler = StandardScaler()\n",
                "X_train_cs = scaler.fit_transform(X_train_c)\n",
                "X_test_cs = scaler.transform(X_test_c)\n",
                "\n",
                "print(f\"Moons dataset: {X_moons.shape[0]} samples, {X_moons.shape[1]} features\")\n",
                "print(f\"Breast Cancer dataset: {X_cancer.shape[0]} samples, {X_cancer.shape[1]} features\")\n",
                "print(f\"  Target classes: {list(cancer.target_names)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 1. Logistic Regression\n",
                "\n",
                "Despite its name, Logistic Regression is a **classification** algorithm. It models the probability that an input belongs to a particular class using the **sigmoid function**:\n",
                "\n",
                "$$\\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n",
                "\n",
                "where $z = w_0 + w_1 x_1 + w_2 x_2 + \\ldots$\n",
                "\n",
                "The sigmoid squashes any real number into the range (0, 1), which we interpret as a probability."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize the sigmoid function\n",
                "z = np.linspace(-8, 8, 200)\n",
                "sigmoid = 1 / (1 + np.exp(-z))\n",
                "\n",
                "fig, ax = plt.subplots(figsize=(10, 5))\n",
                "ax.plot(z, sigmoid, linewidth=2.5, color='#2196F3')\n",
                "ax.axhline(y=0.5, color='red', linestyle='--', alpha=0.7, label='Decision threshold (0.5)')\n",
                "ax.axvline(x=0, color='gray', linestyle='--', alpha=0.3)\n",
                "ax.fill_between(z, sigmoid, 0.5, where=(sigmoid >= 0.5), alpha=0.1, color='green', label='Predict class 1')\n",
                "ax.fill_between(z, sigmoid, 0.5, where=(sigmoid < 0.5), alpha=0.1, color='red', label='Predict class 0')\n",
                "ax.set_xlabel('z = w*x + b', fontsize=13)\n",
                "ax.set_ylabel('Sigmoid output (probability)', fontsize=13)\n",
                "ax.set_title('The Sigmoid Function', fontsize=14, fontweight='bold')\n",
                "ax.legend(fontsize=11)\n",
                "ax.set_ylim(-0.05, 1.05)\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.linear_model import LogisticRegression\n",
                "\n",
                "# Train on the Moons dataset (2D for visualization)\n",
                "X_train_m, X_test_m, y_train_m, y_test_m = train_test_split(\n",
                "    X_moons, y_moons, test_size=0.2, random_state=42\n",
                ")\n",
                "\n",
                "logreg = LogisticRegression(random_state=42)\n",
                "logreg.fit(X_train_m, y_train_m)\n",
                "\n",
                "print(f\"Logistic Regression — Moons Dataset\")\n",
                "print(f\"  Training accuracy: {logreg.score(X_train_m, y_train_m):.4f}\")\n",
                "print(f\"  Test accuracy:     {logreg.score(X_test_m, y_test_m):.4f}\")\n",
                "\n",
                "# Train on Breast Cancer dataset\n",
                "logreg_bc = LogisticRegression(max_iter=5000, random_state=42)\n",
                "logreg_bc.fit(X_train_cs, y_train_c)\n",
                "print(f\"\\nLogistic Regression — Breast Cancer Dataset\")\n",
                "print(f\"  Training accuracy: {logreg_bc.score(X_train_cs, y_train_c):.4f}\")\n",
                "print(f\"  Test accuracy:     {logreg_bc.score(X_test_cs, y_test_c):.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "fig, ax = plt.subplots(figsize=(8, 6))\n",
                "plot_decision_boundary(logreg, X_moons, y_moons, ax,\n",
                "                       title='Logistic Regression — Decision Boundary')\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(\"Logistic Regression produces a linear decision boundary.\")\n",
                "print(\"It struggles with non-linearly separable data like the Moons dataset.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 2. K-Nearest Neighbors (KNN)\n",
                "\n",
                "KNN is a non-parametric algorithm that classifies a data point based on the majority class of its K nearest neighbors. It makes no assumptions about the underlying data distribution."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.neighbors import KNeighborsClassifier\n",
                "\n",
                "# Compare different values of K\n",
                "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
                "k_values = [1, 5, 15]\n",
                "\n",
                "for idx, k in enumerate(k_values):\n",
                "    knn = KNeighborsClassifier(n_neighbors=k)\n",
                "    knn.fit(X_train_m, y_train_m)\n",
                "    acc = knn.score(X_test_m, y_test_m)\n",
                "    plot_decision_boundary(knn, X_moons, y_moons, axes[idx],\n",
                "                           title=f'KNN (K={k}) — Test Acc: {acc:.3f}')\n",
                "\n",
                "plt.suptitle('KNN Decision Boundaries for Different K Values', fontsize=15, fontweight='bold')\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(\"Observations:\")\n",
                "print(\"  - K=1: Very complex boundary, fits noise (overfitting risk).\")\n",
                "print(\"  - K=5: Good balance between flexibility and smoothness.\")\n",
                "print(\"  - K=15: Overly smooth boundary (underfitting risk).\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# KNN on Breast Cancer dataset\n",
                "knn_bc = KNeighborsClassifier(n_neighbors=5)\n",
                "knn_bc.fit(X_train_cs, y_train_c)\n",
                "\n",
                "y_pred_knn = knn_bc.predict(X_test_cs)\n",
                "print(\"KNN (K=5) — Breast Cancer Dataset\")\n",
                "print(f\"  Test accuracy: {accuracy_score(y_test_c, y_pred_knn):.4f}\")\n",
                "print(f\"\\n{classification_report(y_test_c, y_pred_knn, target_names=cancer.target_names)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 3. Support Vector Machines (SVM)\n",
                "\n",
                "SVM finds the hyperplane that maximizes the **margin** between classes. It can handle non-linear boundaries using the **kernel trick**.\n",
                "\n",
                "Key concepts:\n",
                "- **Margin**: Distance between the decision boundary and the nearest data points (support vectors)\n",
                "- **Kernel**: A function that maps data to a higher-dimensional space (linear, polynomial, RBF)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.svm import SVC\n",
                "\n",
                "# Compare different kernels\n",
                "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
                "kernels = ['linear', 'poly', 'rbf']\n",
                "kernel_names = ['Linear Kernel', 'Polynomial Kernel', 'RBF (Gaussian) Kernel']\n",
                "\n",
                "for idx, (kernel, name) in enumerate(zip(kernels, kernel_names)):\n",
                "    svm = SVC(kernel=kernel, random_state=42)\n",
                "    svm.fit(X_train_m, y_train_m)\n",
                "    acc = svm.score(X_test_m, y_test_m)\n",
                "    plot_decision_boundary(svm, X_moons, y_moons, axes[idx],\n",
                "                           title=f'{name}\\nTest Acc: {acc:.3f}')\n",
                "\n",
                "plt.suptitle('SVM Decision Boundaries — Different Kernels', fontsize=15, fontweight='bold')\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(\"Observations:\")\n",
                "print(\"  - Linear: similar to logistic regression — straight boundary.\")\n",
                "print(\"  - Polynomial: can capture some curvature.\")\n",
                "print(\"  - RBF: highly flexible, adapts well to complex shapes.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# SVM (RBF) on Breast Cancer dataset\n",
                "svm_bc = SVC(kernel='rbf', random_state=42)\n",
                "svm_bc.fit(X_train_cs, y_train_c)\n",
                "\n",
                "y_pred_svm = svm_bc.predict(X_test_cs)\n",
                "print(\"SVM (RBF Kernel) — Breast Cancer Dataset\")\n",
                "print(f\"  Test accuracy: {accuracy_score(y_test_c, y_pred_svm):.4f}\")\n",
                "print(f\"\\n{classification_report(y_test_c, y_pred_svm, target_names=cancer.target_names)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 4. Decision Trees\n",
                "\n",
                "A decision tree splits the data into subsets based on feature values, creating a tree-like structure of if-then rules. It is one of the most interpretable models.\n",
                "\n",
                "Key concepts:\n",
                "- **Splitting criteria**: Gini impurity or Entropy (information gain)\n",
                "- **Depth**: Number of levels in the tree (controls complexity)\n",
                "- **Pruning**: Limiting tree depth to prevent overfitting"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
                "\n",
                "# Compare different depths\n",
                "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
                "depths = [2, 5, None]  # None means unlimited depth\n",
                "depth_labels = ['Depth=2', 'Depth=5', 'No limit']\n",
                "\n",
                "for idx, (depth, label) in enumerate(zip(depths, depth_labels)):\n",
                "    dt = DecisionTreeClassifier(max_depth=depth, random_state=42)\n",
                "    dt.fit(X_train_m, y_train_m)\n",
                "    acc = dt.score(X_test_m, y_test_m)\n",
                "    plot_decision_boundary(dt, X_moons, y_moons, axes[idx],\n",
                "                           title=f'Decision Tree ({label})\\nTest Acc: {acc:.3f}')\n",
                "\n",
                "plt.suptitle('Decision Tree — Effect of Maximum Depth', fontsize=15, fontweight='bold')\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(\"Observations:\")\n",
                "print(\"  - Shallow trees (depth=2): simple, potentially underfitting.\")\n",
                "print(\"  - Deep trees (no limit): complex boundaries, risk of overfitting.\")\n",
                "print(\"  - Decision boundaries are always axis-aligned (rectangular).\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize the tree structure\n",
                "dt_small = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
                "dt_small.fit(X_train_m, y_train_m)\n",
                "\n",
                "fig, ax = plt.subplots(figsize=(16, 8))\n",
                "plot_tree(dt_small, filled=True, rounded=True,\n",
                "          feature_names=['Feature 1', 'Feature 2'],\n",
                "          class_names=['Class 0', 'Class 1'],\n",
                "          ax=ax, fontsize=10)\n",
                "ax.set_title('Decision Tree Structure (max_depth=3)', fontsize=14, fontweight='bold')\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(\"Gini impurity measures the probability of misclassifying a randomly chosen sample.\")\n",
                "print(\"A Gini of 0.0 means the node is pure (all samples belong to one class).\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Decision Tree on Breast Cancer dataset — with feature importance\n",
                "dt_bc = DecisionTreeClassifier(max_depth=5, random_state=42)\n",
                "dt_bc.fit(X_train_cs, y_train_c)\n",
                "\n",
                "y_pred_dt = dt_bc.predict(X_test_cs)\n",
                "print(\"Decision Tree (depth=5) — Breast Cancer Dataset\")\n",
                "print(f\"  Test accuracy: {accuracy_score(y_test_c, y_pred_dt):.4f}\")\n",
                "\n",
                "# Feature importance\n",
                "importance = pd.DataFrame({\n",
                "    'Feature': cancer.feature_names,\n",
                "    'Importance': dt_bc.feature_importances_\n",
                "}).sort_values('Importance', ascending=False).head(10)\n",
                "\n",
                "fig, ax = plt.subplots(figsize=(10, 6))\n",
                "ax.barh(importance['Feature'], importance['Importance'], color='#2196F3', edgecolor='white')\n",
                "ax.set_xlabel('Feature Importance', fontsize=13)\n",
                "ax.set_title('Top 10 Features — Decision Tree', fontsize=14, fontweight='bold')\n",
                "ax.invert_yaxis()\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 5. Naive Bayes\n",
                "\n",
                "Naive Bayes applies Bayes' theorem with the \"naive\" assumption that features are conditionally independent given the class label. Despite this simplification, it often performs surprisingly well, especially on text classification and high-dimensional data."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.naive_bayes import GaussianNB\n",
                "\n",
                "# Moons dataset\n",
                "gnb = GaussianNB()\n",
                "gnb.fit(X_train_m, y_train_m)\n",
                "\n",
                "fig, ax = plt.subplots(figsize=(8, 6))\n",
                "acc = gnb.score(X_test_m, y_test_m)\n",
                "plot_decision_boundary(gnb, X_moons, y_moons, ax,\n",
                "                       title=f'Gaussian Naive Bayes — Test Acc: {acc:.3f}')\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "# Breast Cancer dataset\n",
                "gnb_bc = GaussianNB()\n",
                "gnb_bc.fit(X_train_cs, y_train_c)\n",
                "y_pred_nb = gnb_bc.predict(X_test_cs)\n",
                "print(f\"Gaussian Naive Bayes — Breast Cancer Dataset\")\n",
                "print(f\"  Test accuracy: {accuracy_score(y_test_c, y_pred_nb):.4f}\")\n",
                "print(f\"\\n{classification_report(y_test_c, y_pred_nb, target_names=cancer.target_names)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 6. Classifier Comparison\n",
                "\n",
                "Let us compare all five classifiers side by side — both visually (decision boundaries) and numerically (accuracy on the Breast Cancer dataset)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Decision boundary comparison on Moons dataset\n",
                "classifiers = {\n",
                "    'Logistic Regression': LogisticRegression(random_state=42),\n",
                "    'KNN (K=5)': KNeighborsClassifier(n_neighbors=5),\n",
                "    'SVM (RBF)': SVC(kernel='rbf', random_state=42),\n",
                "    'Decision Tree (d=5)': DecisionTreeClassifier(max_depth=5, random_state=42),\n",
                "    'Naive Bayes': GaussianNB()\n",
                "}\n",
                "\n",
                "fig, axes = plt.subplots(2, 3, figsize=(18, 11))\n",
                "axes = axes.flatten()\n",
                "\n",
                "for idx, (name, clf) in enumerate(classifiers.items()):\n",
                "    clf.fit(X_train_m, y_train_m)\n",
                "    acc = clf.score(X_test_m, y_test_m)\n",
                "    plot_decision_boundary(clf, X_moons, y_moons, axes[idx],\n",
                "                           title=f'{name}\\nTest Acc: {acc:.3f}')\n",
                "\n",
                "axes[5].axis('off')  # hide the empty subplot\n",
                "plt.suptitle('Classifier Comparison — Moons Dataset', fontsize=16, fontweight='bold')\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Performance comparison on Breast Cancer dataset\n",
                "classifiers_bc = {\n",
                "    'Logistic Regression': LogisticRegression(max_iter=5000, random_state=42),\n",
                "    'KNN (K=5)': KNeighborsClassifier(n_neighbors=5),\n",
                "    'SVM (RBF)': SVC(kernel='rbf', random_state=42),\n",
                "    'Decision Tree': DecisionTreeClassifier(max_depth=5, random_state=42),\n",
                "    'Naive Bayes': GaussianNB()\n",
                "}\n",
                "\n",
                "comparison_results = []\n",
                "for name, clf in classifiers_bc.items():\n",
                "    clf.fit(X_train_cs, y_train_c)\n",
                "    train_acc = clf.score(X_train_cs, y_train_c)\n",
                "    test_acc = clf.score(X_test_cs, y_test_c)\n",
                "    comparison_results.append({\n",
                "        'Classifier': name, 'Train Accuracy': train_acc, 'Test Accuracy': test_acc\n",
                "    })\n",
                "\n",
                "comp_df = pd.DataFrame(comparison_results).sort_values('Test Accuracy', ascending=False)\n",
                "print(\"Classifier Comparison — Breast Cancer Dataset\")\n",
                "print(\"=\" * 60)\n",
                "print(comp_df.to_string(index=False))\n",
                "\n",
                "# Bar chart\n",
                "fig, ax = plt.subplots(figsize=(12, 6))\n",
                "x = np.arange(len(comp_df))\n",
                "width = 0.35\n",
                "ax.bar(x - width/2, comp_df['Train Accuracy'], width, label='Train', color='#2196F3', edgecolor='white')\n",
                "ax.bar(x + width/2, comp_df['Test Accuracy'], width, label='Test', color='#FF5722', edgecolor='white')\n",
                "ax.set_xticks(x)\n",
                "ax.set_xticklabels(comp_df['Classifier'], rotation=20, ha='right')\n",
                "ax.set_ylabel('Accuracy', fontsize=13)\n",
                "ax.set_title('Classifier Comparison — Breast Cancer Dataset', fontsize=14, fontweight='bold')\n",
                "ax.legend(fontsize=11)\n",
                "ax.set_ylim(0.9, 1.01)\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 7. Exercises\n",
                "\n",
                "### Exercise 1: Iris Classification"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Exercise 1: Using the Iris dataset (3 classes):\n",
                "# 1. Load the data and split 80/20 with stratification\n",
                "# 2. Scale the features using StandardScaler\n",
                "# 3. Train all five classifiers from this module\n",
                "# 4. Print a comparison table of test accuracies\n",
                "# 5. Generate a confusion matrix for the best-performing classifier\n",
                "\n",
                "from sklearn.datasets import load_iris\n",
                "\n",
                "# Your code here:\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Exercise 2: Hyperparameter Tuning"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Exercise 2: For the SVM classifier on the Breast Cancer dataset:\n",
                "# 1. Try C values: [0.01, 0.1, 1, 10, 100] with RBF kernel\n",
                "# 2. Record train and test accuracy for each C\n",
                "# 3. Plot C vs accuracy (both train and test)\n",
                "# 4. What value of C gives the best test accuracy?\n",
                "# 5. What does C control? (Hint: it controls the regularization strength)\n",
                "\n",
                "# Your code here:\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Exercise 3: Decision Tree Depth Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Exercise 3: Using the Breast Cancer dataset:\n",
                "# 1. Train Decision Trees with max_depth from 1 to 20\n",
                "# 2. Record training and test accuracy for each depth\n",
                "# 3. Plot depth vs accuracy\n",
                "# 4. Identify when overfitting begins (train accuracy >> test accuracy)\n",
                "# 5. What is the optimal depth?\n",
                "\n",
                "# Your code here:\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 8. Summary and Further Reading\n",
                "\n",
                "### What We Covered\n",
                "\n",
                "| Algorithm | Type | Strengths | Weaknesses |\n",
                "|-----------|------|-----------|------------|\n",
                "| Logistic Regression | Linear | Fast, interpretable, good baseline | Only linear boundaries |\n",
                "| KNN | Non-parametric | Simple, no training phase | Slow at prediction time, sensitive to scale |\n",
                "| SVM | Kernel-based | Handles high dimensions well, flexible kernels | Slow on large datasets, needs scaling |\n",
                "| Decision Tree | Rule-based | Interpretable, handles mixed types | Prone to overfitting |\n",
                "| Naive Bayes | Probabilistic | Very fast, works well with high dimensions | Assumes feature independence |\n",
                "\n",
                "### Recommended Reading\n",
                "\n",
                "- [Scikit-learn Classification Guide](https://scikit-learn.org/stable/supervised_learning.html)\n",
                "- Chapter 3 (Classification) and Chapter 5 (SVM) of Aurélien Géron, *Hands-On Machine Learning*\n",
                "- Chapter 4 (Classification) of *ISLR*\n",
                "\n",
                "### Next Module\n",
                "\n",
                "In **Module 6: Model Evaluation and Validation**, we will learn how to rigorously evaluate classifier performance using confusion matrices, ROC curves, cross-validation, and hyperparameter tuning strategies.\n",
                "\n",
                "---"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.9.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}