{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Module 10: Advanced Topics and Best Practices\n",
                "\n",
                "---\n",
                "\n",
                "This final module covers practical topics that bridge the gap between academic exercises and real-world machine learning work: handling imbalanced data, feature engineering strategies, model persistence, pipeline design, and a complete end-to-end project.\n",
                "\n",
                "---\n",
                "\n",
                "## Table of Contents\n",
                "\n",
                "1. [Handling Imbalanced Data](#1.-Handling-Imbalanced-Data)\n",
                "2. [Advanced Feature Engineering](#2.-Advanced-Feature-Engineering)\n",
                "3. [Model Persistence (Saving and Loading)](#3.-Model-Persistence)\n",
                "4. [End-to-End ML Pipeline](#4.-End-to-End-ML-Pipeline)\n",
                "5. [Common Pitfalls and Best Practices](#5.-Common-Pitfalls-and-Best-Practices)\n",
                "6. [Capstone Exercise](#6.-Capstone-Exercise)\n",
                "7. [Course Summary and Next Steps](#7.-Course-Summary-and-Next-Steps)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
                "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
                "from sklearn.metrics import (accuracy_score, classification_report, confusion_matrix,\n",
                "                              f1_score, roc_auc_score, roc_curve)\n",
                "from sklearn.datasets import make_classification\n",
                "\n",
                "plt.style.use('seaborn-v0_8-whitegrid')\n",
                "np.random.seed(42)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 1. Handling Imbalanced Data\n",
                "\n",
                "In many real-world problems — fraud detection, medical diagnosis, rare event prediction — one class vastly outnumbers the other. Standard algorithms optimize for overall accuracy, which can be misleading when 95% of samples belong to one class."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create an imbalanced dataset (95% class 0, 5% class 1)\n",
                "X_imb, y_imb = make_classification(\n",
                "    n_samples=2000, n_features=10, n_informative=5,\n",
                "    weights=[0.95, 0.05], random_state=42, flip_y=0.01\n",
                ")\n",
                "\n",
                "print(f\"Class distribution:\")\n",
                "unique, counts = np.unique(y_imb, return_counts=True)\n",
                "for cls, cnt in zip(unique, counts):\n",
                "    print(f\"  Class {cls}: {cnt} samples ({cnt/len(y_imb):.1%})\")\n",
                "\n",
                "X_train_i, X_test_i, y_train_i, y_test_i = train_test_split(\n",
                "    X_imb, y_imb, test_size=0.2, random_state=42, stratify=y_imb\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.ensemble import RandomForestClassifier\n",
                "\n",
                "# Baseline: train without handling imbalance\n",
                "lr_baseline = LogisticRegression(max_iter=1000, random_state=42)\n",
                "lr_baseline.fit(X_train_i, y_train_i)\n",
                "y_pred_bl = lr_baseline.predict(X_test_i)\n",
                "\n",
                "print(\"BASELINE (no imbalance handling)\")\n",
                "print(f\"  Accuracy: {accuracy_score(y_test_i, y_pred_bl):.4f}\")\n",
                "print(f\"  F1 (minority class): {f1_score(y_test_i, y_pred_bl):.4f}\")\n",
                "print(f\"\\n{classification_report(y_test_i, y_pred_bl)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Strategy 1: Class weight adjustment\n",
                "lr_weighted = LogisticRegression(class_weight='balanced', max_iter=1000, random_state=42)\n",
                "lr_weighted.fit(X_train_i, y_train_i)\n",
                "y_pred_w = lr_weighted.predict(X_test_i)\n",
                "\n",
                "# Strategy 2: SMOTE (Synthetic Minority Over-sampling Technique)\n",
                "# Note: requires imbalanced-learn library\n",
                "try:\n",
                "    from imblearn.over_sampling import SMOTE\n",
                "    smote = SMOTE(random_state=42)\n",
                "    X_smote, y_smote = smote.fit_resample(X_train_i, y_train_i)\n",
                "    \n",
                "    lr_smote = LogisticRegression(max_iter=1000, random_state=42)\n",
                "    lr_smote.fit(X_smote, y_smote)\n",
                "    y_pred_s = lr_smote.predict(X_test_i)\n",
                "    smote_available = True\n",
                "    print(f\"After SMOTE: {np.bincount(y_smote)} (balanced)\")\n",
                "except ImportError:\n",
                "    smote_available = False\n",
                "    print(\"imbalanced-learn not installed. Run: pip install imbalanced-learn\")\n",
                "\n",
                "# Strategy 3: Threshold adjustment\n",
                "y_proba_bl = lr_baseline.predict_proba(X_test_i)[:, 1]\n",
                "y_pred_thresh = (y_proba_bl >= 0.3).astype(int)  # lower threshold\n",
                "\n",
                "# Compare results\n",
                "print(\"\\nCOMPARISON\")\n",
                "print(\"=\" * 60)\n",
                "strategies = [\n",
                "    ('Baseline', y_pred_bl),\n",
                "    ('Class Weights', y_pred_w),\n",
                "    ('Threshold=0.3', y_pred_thresh),\n",
                "]\n",
                "if smote_available:\n",
                "    strategies.append(('SMOTE', y_pred_s))\n",
                "\n",
                "for name, preds in strategies:\n",
                "    print(f\"  {name:>20s}: Accuracy={accuracy_score(y_test_i, preds):.4f}  F1(minority)={f1_score(y_test_i, preds):.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize confusion matrices side by side\n",
                "fig, axes = plt.subplots(1, 3, figsize=(18, 4.5))\n",
                "matrices = [('Baseline', y_pred_bl), ('Class Weights', y_pred_w), ('Threshold=0.3', y_pred_thresh)]\n",
                "\n",
                "for idx, (name, preds) in enumerate(matrices):\n",
                "    cm = confusion_matrix(y_test_i, preds)\n",
                "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx],\n",
                "                square=True, linewidths=1, annot_kws={'size': 14})\n",
                "    axes[idx].set_title(name, fontsize=13, fontweight='bold')\n",
                "    axes[idx].set_xlabel('Predicted')\n",
                "    axes[idx].set_ylabel('Actual')\n",
                "\n",
                "plt.suptitle('Confusion Matrices — Imbalanced Data Strategies', fontsize=15, fontweight='bold')\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 2. Advanced Feature Engineering\n",
                "\n",
                "Feature engineering — creating new features from existing data — often has a greater impact on model performance than algorithm selection."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create a synthetic dataset with datetime-like features\n",
                "np.random.seed(42)\n",
                "n = 500\n",
                "\n",
                "data = pd.DataFrame({\n",
                "    'age': np.random.randint(18, 70, n),\n",
                "    'income': np.random.lognormal(10.5, 0.5, n).astype(int),\n",
                "    'years_employed': np.random.randint(0, 40, n),\n",
                "    'num_products': np.random.randint(1, 6, n),\n",
                "    'credit_score': np.random.randint(300, 850, n),\n",
                "    'balance': np.random.lognormal(8, 1.5, n).astype(int),\n",
                "})\n",
                "\n",
                "# Target: whether the customer churns\n",
                "data['churned'] = ((data['credit_score'] < 500) |\n",
                "                   (data['num_products'] >= 4) |\n",
                "                   (data['balance'] < 1000) &\n",
                "                   (np.random.random(n) > 0.7)).astype(int)\n",
                "\n",
                "print(\"Original features:\")\n",
                "print(data.head())\n",
                "print(f\"\\nShape: {data.shape}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Feature engineering techniques\n",
                "data_fe = data.copy()\n",
                "\n",
                "# 1. Ratio features\n",
                "data_fe['income_per_year'] = data_fe['income'] / (data_fe['years_employed'] + 1)\n",
                "data_fe['balance_to_income'] = data_fe['balance'] / (data_fe['income'] + 1)\n",
                "\n",
                "# 2. Binning\n",
                "data_fe['age_group'] = pd.cut(data_fe['age'], bins=[0, 25, 35, 50, 100],\n",
                "                               labels=['Young', 'Mid', 'Senior', 'Elder'])\n",
                "data_fe['credit_tier'] = pd.cut(data_fe['credit_score'], bins=[0, 500, 650, 750, 900],\n",
                "                                 labels=['Poor', 'Fair', 'Good', 'Excellent'])\n",
                "\n",
                "# 3. Polynomial interactions\n",
                "data_fe['age_x_income'] = data_fe['age'] * data_fe['income']\n",
                "\n",
                "# 4. Log transformations (for skewed distributions)\n",
                "data_fe['log_income'] = np.log1p(data_fe['income'])\n",
                "data_fe['log_balance'] = np.log1p(data_fe['balance'])\n",
                "\n",
                "print(\"Feature-engineered dataset:\")\n",
                "print(data_fe[['age', 'income', 'income_per_year', 'balance_to_income',\n",
                "               'age_group', 'credit_tier', 'log_income']].head())\n",
                "print(f\"\\nOriginal features: {data.shape[1] - 1}\")\n",
                "print(f\"After engineering: {data_fe.shape[1] - 1}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Compare model performance before and after feature engineering\n",
                "from sklearn.ensemble import GradientBoostingClassifier\n",
                "from sklearn.preprocessing import OrdinalEncoder\n",
                "\n",
                "# Original features\n",
                "X_orig = data.drop('churned', axis=1)\n",
                "y_target = data['churned']\n",
                "\n",
                "scores_orig = cross_val_score(\n",
                "    GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
                "    X_orig, y_target, cv=5, scoring='f1'\n",
                ")\n",
                "\n",
                "# Engineered features\n",
                "X_eng = data_fe.drop('churned', axis=1)\n",
                "# Encode categorical columns\n",
                "cat_cols = X_eng.select_dtypes(include='category').columns\n",
                "for col in cat_cols:\n",
                "    X_eng[col] = X_eng[col].cat.codes\n",
                "\n",
                "scores_eng = cross_val_score(\n",
                "    GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
                "    X_eng, y_target, cv=5, scoring='f1'\n",
                ")\n",
                "\n",
                "print(f\"F1 Score Comparison (5-fold CV):\")\n",
                "print(f\"  Original features:    {scores_orig.mean():.4f} (+/- {scores_orig.std():.4f})\")\n",
                "print(f\"  Engineered features:  {scores_eng.mean():.4f} (+/- {scores_eng.std():.4f})\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 3. Model Persistence (Saving and Loading)\n",
                "\n",
                "Once you have trained a model, you need to save it for later use (deployment, sharing, reproducibility). The two standard approaches are `pickle` and `joblib`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import joblib\n",
                "import pickle\n",
                "import os\n",
                "\n",
                "# Train a model\n",
                "from sklearn.datasets import load_breast_cancer\n",
                "cancer = load_breast_cancer()\n",
                "X_train_p, X_test_p, y_train_p, y_test_p = train_test_split(\n",
                "    cancer.data, cancer.target, test_size=0.2, random_state=42\n",
                ")\n",
                "scaler_p = StandardScaler()\n",
                "X_train_ps = scaler_p.fit_transform(X_train_p)\n",
                "X_test_ps = scaler_p.transform(X_test_p)\n",
                "\n",
                "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
                "rf_model.fit(X_train_ps, y_train_p)\n",
                "print(f\"Original test accuracy: {rf_model.score(X_test_ps, y_test_p):.4f}\")\n",
                "\n",
                "# Method 1: joblib (recommended for large numpy arrays)\n",
                "joblib.dump(rf_model, 'model_rf.joblib')\n",
                "joblib.dump(scaler_p, 'scaler.joblib')\n",
                "print(f\"\\nSaved model with joblib: {os.path.getsize('model_rf.joblib') / 1024:.1f} KB\")\n",
                "\n",
                "# Load and verify\n",
                "rf_loaded = joblib.load('model_rf.joblib')\n",
                "scaler_loaded = joblib.load('scaler.joblib')\n",
                "X_test_verify = scaler_loaded.transform(X_test_p)\n",
                "print(f\"Loaded model accuracy: {rf_loaded.score(X_test_verify, y_test_p):.4f}\")\n",
                "\n",
                "# Method 2: pickle\n",
                "with open('model_rf.pkl', 'wb') as f:\n",
                "    pickle.dump(rf_model, f)\n",
                "print(f\"Saved model with pickle: {os.path.getsize('model_rf.pkl') / 1024:.1f} KB\")\n",
                "\n",
                "# Clean up\n",
                "for f in ['model_rf.joblib', 'scaler.joblib', 'model_rf.pkl']:\n",
                "    if os.path.exists(f):\n",
                "        os.remove(f)\n",
                "print(\"\\nCleaned up saved files.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 4. End-to-End ML Pipeline\n",
                "\n",
                "An end-to-end ML pipeline integrates all steps — data loading, preprocessing, feature engineering, model training, evaluation, and prediction — into a single reproducible workflow."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.pipeline import Pipeline\n",
                "from sklearn.compose import ColumnTransformer\n",
                "from sklearn.impute import SimpleImputer\n",
                "from sklearn.preprocessing import OneHotEncoder\n",
                "from sklearn.ensemble import GradientBoostingClassifier\n",
                "from sklearn.model_selection import GridSearchCV\n",
                "\n",
                "# Create a realistic dataset with mixed types and missing values\n",
                "np.random.seed(42)\n",
                "n = 800\n",
                "\n",
                "df = pd.DataFrame({\n",
                "    'age': np.random.randint(18, 80, n).astype(float),\n",
                "    'income': np.random.lognormal(10.5, 0.6, n),\n",
                "    'credit_score': np.random.randint(300, 850, n).astype(float),\n",
                "    'education': np.random.choice(['High School', 'Bachelor', 'Master', 'PhD'], n),\n",
                "    'employment': np.random.choice(['Employed', 'Self-Employed', 'Unemployed', 'Retired'], n),\n",
                "    'balance': np.random.lognormal(8, 1.5, n),\n",
                "})\n",
                "\n",
                "# Introduce missing values\n",
                "for col in ['age', 'income', 'credit_score']:\n",
                "    mask = np.random.random(n) < 0.05\n",
                "    df.loc[mask, col] = np.nan\n",
                "\n",
                "# Create target variable\n",
                "df['approved'] = ((df['credit_score'].fillna(500) > 550) &\n",
                "                  (df['income'] > 30000) |\n",
                "                  (np.random.random(n) > 0.6)).astype(int)\n",
                "\n",
                "print(f\"Dataset shape: {df.shape}\")\n",
                "print(f\"Missing values:\\n{df.isnull().sum()}\")\n",
                "print(f\"\\n{df.head()}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define the full pipeline\n",
                "X = df.drop('approved', axis=1)\n",
                "y = df['approved']\n",
                "\n",
                "# Identify column types\n",
                "numeric_features = ['age', 'income', 'credit_score', 'balance']\n",
                "categorical_features = ['education', 'employment']\n",
                "\n",
                "# Preprocessing pipeline\n",
                "numeric_transformer = Pipeline(steps=[\n",
                "    ('imputer', SimpleImputer(strategy='median')),\n",
                "    ('scaler', StandardScaler())\n",
                "])\n",
                "\n",
                "categorical_transformer = Pipeline(steps=[\n",
                "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
                "    ('encoder', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'))\n",
                "])\n",
                "\n",
                "preprocessor = ColumnTransformer(transformers=[\n",
                "    ('num', numeric_transformer, numeric_features),\n",
                "    ('cat', categorical_transformer, categorical_features)\n",
                "])\n",
                "\n",
                "# Full pipeline: preprocessing + model\n",
                "pipeline = Pipeline(steps=[\n",
                "    ('preprocessor', preprocessor),\n",
                "    ('classifier', GradientBoostingClassifier(random_state=42))\n",
                "])\n",
                "\n",
                "# Split data\n",
                "X_train_p, X_test_p, y_train_p, y_test_p = train_test_split(\n",
                "    X, y, test_size=0.2, random_state=42, stratify=y\n",
                ")\n",
                "\n",
                "# Fit the entire pipeline\n",
                "pipeline.fit(X_train_p, y_train_p)\n",
                "\n",
                "print(f\"Pipeline Test Accuracy: {pipeline.score(X_test_p, y_test_p):.4f}\")\n",
                "print(f\"\\n{classification_report(y_test_p, pipeline.predict(X_test_p))}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Hyperparameter tuning within the pipeline\n",
                "param_grid = {\n",
                "    'classifier__n_estimators': [100, 200],\n",
                "    'classifier__max_depth': [3, 5],\n",
                "    'classifier__learning_rate': [0.05, 0.1]\n",
                "}\n",
                "\n",
                "grid = GridSearchCV(\n",
                "    pipeline, param_grid,\n",
                "    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
                "    scoring='f1', n_jobs=-1, verbose=0\n",
                ")\n",
                "\n",
                "grid.fit(X_train_p, y_train_p)\n",
                "\n",
                "print(f\"Best Parameters: {grid.best_params_}\")\n",
                "print(f\"Best CV F1: {grid.best_score_:.4f}\")\n",
                "print(f\"Test Accuracy: {grid.score(X_test_p, y_test_p):.4f}\")\n",
                "\n",
                "print(\"\\nThe pipeline ensures that:\")\n",
                "print(\"  1. Preprocessing is applied consistently to train and test data\")\n",
                "print(\"  2. No data leakage occurs during cross-validation\")\n",
                "print(\"  3. The entire workflow is reproducible and deployable as a single object\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 5. Common Pitfalls and Best Practices\n",
                "\n",
                "### Pitfalls to Avoid\n",
                "\n",
                "| Pitfall | Problem | Solution |\n",
                "|---------|---------|----------|\n",
                "| Data leakage | Information from test set influences training | Use pipelines; scale/impute AFTER splitting |\n",
                "| Overfitting | Model memorizes training data | Cross-validation, regularization, early stopping |\n",
                "| Using accuracy on imbalanced data | Misleading metric (predict majority class = high accuracy) | Use F1, precision, recall, AUC |\n",
                "| Not scaling features | Algorithms sensitive to scale give poor results | Always scale for distance-based models (KNN, SVM) |\n",
                "| Cherry-picking results | Reporting best of many runs | Use cross-validation with fixed seeds |\n",
                "| Ignoring feature distributions | Outliers and skewness affect models | Visualize distributions, apply transformations |"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Demonstrate data leakage\n",
                "from sklearn.datasets import load_breast_cancer\n",
                "from sklearn.svm import SVC\n",
                "\n",
                "cancer = load_breast_cancer()\n",
                "X_leak = cancer.data\n",
                "y_leak = cancer.target\n",
                "\n",
                "# WRONG: Scale before splitting (data leakage)\n",
                "scaler_wrong = StandardScaler()\n",
                "X_leak_scaled = scaler_wrong.fit_transform(X_leak)  # fit on ALL data including test\n",
                "X_train_w, X_test_w, y_train_w, y_test_w = train_test_split(\n",
                "    X_leak_scaled, y_leak, test_size=0.2, random_state=42\n",
                ")\n",
                "svm_wrong = SVC(random_state=42).fit(X_train_w, y_train_w)\n",
                "\n",
                "# CORRECT: Split first, then scale\n",
                "X_train_r, X_test_r, y_train_r, y_test_r = train_test_split(\n",
                "    X_leak, y_leak, test_size=0.2, random_state=42\n",
                ")\n",
                "scaler_right = StandardScaler()\n",
                "X_train_rs = scaler_right.fit_transform(X_train_r)  # fit only on training data\n",
                "X_test_rs = scaler_right.transform(X_test_r)        # transform test with train stats\n",
                "svm_right = SVC(random_state=42).fit(X_train_rs, y_train_r)\n",
                "\n",
                "print(\"Data Leakage Demonstration\")\n",
                "print(\"=\" * 50)\n",
                "print(f\"  With leakage (scale before split): {svm_wrong.score(X_test_w, y_test_w):.4f}\")\n",
                "print(f\"  Without leakage (scale after split): {svm_right.score(X_test_rs, y_test_r):.4f}\")\n",
                "print(\"\\nThe difference may be small here, but in production systems it can be significant.\")\n",
                "print(\"Always use pipelines to prevent leakage automatically.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Best Practices Checklist\n",
                "\n",
                "1. **Always visualize your data first** before building any model.\n",
                "2. **Split data early**, before any preprocessing or feature engineering.\n",
                "3. **Use pipelines** to prevent data leakage and ensure reproducibility.\n",
                "4. **Start simple** — try logistic regression or a decision tree before deep learning.\n",
                "5. **Use cross-validation** instead of a single train/test split.\n",
                "6. **Choose metrics wisely** — accuracy is not always the right metric.\n",
                "7. **Tune hyperparameters systematically** using GridSearchCV or RandomizedSearchCV.\n",
                "8. **Document everything** — data sources, preprocessing steps, model choices, results.\n",
                "9. **Save your models** along with the preprocessing pipeline.\n",
                "10. **Monitor model performance** over time after deployment (model drift)."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 6. Capstone Exercise"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# CAPSTONE EXERCISE: End-to-End ML Project\n",
                "#\n",
                "# Build a complete ML pipeline on a classification problem:\n",
                "#\n",
                "# 1. DATA PREPARATION\n",
                "#    - Generate or load a dataset (e.g., make_classification with 1000 samples, 15 features)\n",
                "#    - Add some missing values randomly\n",
                "#    - Add 2-3 categorical features\n",
                "#    - Explore the data with visualizations\n",
                "#\n",
                "# 2. PREPROCESSING\n",
                "#    - Build a ColumnTransformer pipeline:\n",
                "#      * Impute missing values (median for numeric, mode for categorical)\n",
                "#      * Scale numeric features\n",
                "#      * One-hot encode categorical features\n",
                "#\n",
                "# 3. MODEL SELECTION\n",
                "#    - Try at least 3 different classifiers\n",
                "#    - Use 5-fold cross-validation to compare\n",
                "#    - Select the best model\n",
                "#\n",
                "# 4. HYPERPARAMETER TUNING\n",
                "#    - Use GridSearchCV or RandomizedSearchCV on the best model\n",
                "#    - Report the best parameters\n",
                "#\n",
                "# 5. FINAL EVALUATION\n",
                "#    - Train the final model on the full training set\n",
                "#    - Evaluate on the test set\n",
                "#    - Print classification report and confusion matrix\n",
                "#    - Plot the ROC curve\n",
                "#\n",
                "# 6. SAVE\n",
                "#    - Save the pipeline with joblib\n",
                "\n",
                "# Your code here:\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 7. Course Summary and Next Steps\n",
                "\n",
                "### Complete Course Overview\n",
                "\n",
                "| Module | Topic | Key Takeaway |\n",
                "|--------|-------|-------------|\n",
                "| 1 | Introduction to ML | ML is learning patterns from data; three types: supervised, unsupervised, reinforcement |\n",
                "| 2 | Mathematical Foundations | Linear algebra, statistics, probability, and calculus are the language of ML |\n",
                "| 3 | Data Preprocessing | Raw data must be cleaned, encoded, and scaled before modeling |\n",
                "| 4 | Regression | Predicting continuous values with linear models and regularization |\n",
                "| 5 | Classification | Predicting categories with logistic regression, KNN, SVM, trees, and Naive Bayes |\n",
                "| 6 | Model Evaluation | Confusion matrix, ROC/AUC, cross-validation, and hyperparameter tuning |\n",
                "| 7 | Unsupervised Learning | Clustering (K-Means, DBSCAN) and dimensionality reduction (PCA, t-SNE) |\n",
                "| 8 | Ensemble Methods | Combining models via bagging, boosting, and stacking for stronger predictions |\n",
                "| 9 | Neural Networks | From perceptron to deep learning with Keras/TensorFlow |\n",
                "| 10 | Advanced Topics | Imbalanced data, feature engineering, pipelines, and best practices |\n",
                "\n",
                "### Recommended Next Steps\n",
                "\n",
                "1. **Practice on real datasets**: Explore datasets on [Kaggle](https://www.kaggle.com/datasets) or the [UCI ML Repository](https://archive.ics.uci.edu/ml/index.php).\n",
                "2. **Participate in competitions**: Start with beginner competitions on Kaggle.\n",
                "3. **Study deep learning in depth**: Work through TensorFlow tutorials or the fast.ai course.\n",
                "4. **Learn MLOps**: Understand model deployment, monitoring, and maintenance.\n",
                "5. **Read foundational books**:\n",
                "   - Aurélien Géron, *Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow*\n",
                "   - James et al., *Introduction to Statistical Learning (ISLR)*\n",
                "   - Ian Goodfellow et al., *Deep Learning*\n",
                "\n",
                "---\n",
                "\n",
                "This concludes the Machine Learning Courseware. The skills you have developed — from data preprocessing to neural networks — provide a comprehensive foundation for applying machine learning to real-world problems."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.9.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}