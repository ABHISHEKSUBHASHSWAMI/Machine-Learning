{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Module 3: Data Preprocessing and Feature Engineering\n",
                "\n",
                "---\n",
                "\n",
                "Real-world data is rarely clean. Missing values, inconsistent formats, and varied scales can significantly degrade model performance. This module covers the essential techniques for preparing data before feeding it into a machine learning algorithm.\n",
                "\n",
                "**What you will learn:**\n",
                "- Loading and inspecting datasets\n",
                "- Handling missing values\n",
                "- Encoding categorical variables\n",
                "- Feature scaling and normalization\n",
                "- Feature selection\n",
                "- Train-test splitting strategies\n",
                "\n",
                "---\n",
                "\n",
                "## Table of Contents\n",
                "\n",
                "1. [Loading and Inspecting Data](#1.-Loading-and-Inspecting-Data)\n",
                "2. [Handling Missing Values](#2.-Handling-Missing-Values)\n",
                "3. [Encoding Categorical Variables](#3.-Encoding-Categorical-Variables)\n",
                "4. [Feature Scaling](#4.-Feature-Scaling)\n",
                "5. [Feature Selection](#5.-Feature-Selection)\n",
                "6. [Train-Test Splitting](#6.-Train-Test-Splitting)\n",
                "7. [End-to-End Preprocessing Pipeline](#7.-End-to-End-Preprocessing-Pipeline)\n",
                "8. [Exercises](#8.-Exercises)\n",
                "9. [Summary and Further Reading](#9.-Summary-and-Further-Reading)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "plt.style.use('seaborn-v0_8-whitegrid')\n",
                "np.random.seed(42)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 1. Loading and Inspecting Data\n",
                "\n",
                "The first step in any ML project is to load the data and understand its structure. We will create a realistic synthetic dataset that contains the types of issues you will encounter in practice."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create a synthetic dataset simulating housing data with realistic issues\n",
                "np.random.seed(42)\n",
                "n = 200\n",
                "\n",
                "data = {\n",
                "    'area_sqft': np.random.randint(500, 5000, n).astype(float),\n",
                "    'bedrooms': np.random.choice([1, 2, 3, 4, 5], n),\n",
                "    'age_years': np.random.randint(0, 50, n).astype(float),\n",
                "    'location': np.random.choice(['Urban', 'Suburban', 'Rural'], n),\n",
                "    'garage': np.random.choice(['Yes', 'No'], n),\n",
                "    'condition': np.random.choice(['Poor', 'Fair', 'Good', 'Excellent'], n),\n",
                "}\n",
                "\n",
                "# Generate price as a function of features plus noise\n",
                "price = (data['area_sqft'] * 100 +\n",
                "         data['bedrooms'] * 15000 +\n",
                "         (50 - data['age_years']) * 1000 +\n",
                "         np.random.normal(0, 20000, n))\n",
                "data['price'] = price\n",
                "\n",
                "df = pd.DataFrame(data)\n",
                "\n",
                "# Introduce missing values deliberately (to simulate real-world data)\n",
                "missing_indices_area = np.random.choice(n, 15, replace=False)\n",
                "missing_indices_age = np.random.choice(n, 10, replace=False)\n",
                "missing_indices_garage = np.random.choice(n, 20, replace=False)\n",
                "\n",
                "df.loc[missing_indices_area, 'area_sqft'] = np.nan\n",
                "df.loc[missing_indices_age, 'age_years'] = np.nan\n",
                "df.loc[missing_indices_garage, 'garage'] = np.nan\n",
                "\n",
                "print(f\"Dataset shape: {df.shape}\")\n",
                "print(f\"Columns: {list(df.columns)}\")\n",
                "print()\n",
                "df.head(10)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Essential inspection steps\n",
                "print(\"=\" * 60)\n",
                "print(\"DATA TYPES\")\n",
                "print(\"=\" * 60)\n",
                "print(df.dtypes)\n",
                "\n",
                "print(\"\\n\" + \"=\" * 60)\n",
                "print(\"BASIC STATISTICS (Numerical Columns)\")\n",
                "print(\"=\" * 60)\n",
                "print(df.describe().round(2))\n",
                "\n",
                "print(\"\\n\" + \"=\" * 60)\n",
                "print(\"MISSING VALUES\")\n",
                "print(\"=\" * 60)\n",
                "missing = df.isnull().sum()\n",
                "missing_pct = (df.isnull().sum() / len(df) * 100).round(1)\n",
                "missing_summary = pd.DataFrame({'Missing Count': missing, 'Percentage': missing_pct})\n",
                "print(missing_summary[missing_summary['Missing Count'] > 0])\n",
                "print(f\"\\nTotal missing values: {df.isnull().sum().sum()} out of {df.size} entries\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize missing values\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "# Bar chart of missing values\n",
                "missing_cols = df.isnull().sum()\n",
                "missing_cols = missing_cols[missing_cols > 0]\n",
                "axes[0].barh(missing_cols.index, missing_cols.values, color='#FF5722', edgecolor='white')\n",
                "axes[0].set_xlabel('Number of Missing Values')\n",
                "axes[0].set_title('Missing Values by Column', fontsize=14, fontweight='bold')\n",
                "for i, v in enumerate(missing_cols.values):\n",
                "    axes[0].text(v + 0.3, i, str(v), va='center', fontsize=11)\n",
                "\n",
                "# Heatmap of missing values\n",
                "sns.heatmap(df.isnull(), cbar=True, yticklabels=False, cmap='YlOrRd', ax=axes[1])\n",
                "axes[1].set_title('Missing Value Heatmap', fontsize=14, fontweight='bold')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 2. Handling Missing Values\n",
                "\n",
                "Missing data must be addressed before training. The main strategies are:\n",
                "\n",
                "| Strategy | When to Use | Method |\n",
                "|----------|------------|--------|\n",
                "| **Drop rows** | Very few missing values, large dataset | `df.dropna()` |\n",
                "| **Drop columns** | Column has > 50% missing values | `df.drop(columns=...)` |\n",
                "| **Mean/Median imputation** | Numerical features, roughly symmetric | `SimpleImputer(strategy='mean')` |\n",
                "| **Mode imputation** | Categorical features | `SimpleImputer(strategy='most_frequent')` |\n",
                "| **Forward/Backward fill** | Time series data | `df.fillna(method='ffill')` |"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.impute import SimpleImputer\n",
                "\n",
                "# Work on a copy\n",
                "df_clean = df.copy()\n",
                "\n",
                "# Strategy 1: Impute numerical columns with the median\n",
                "# (Median is preferred over mean when data may have outliers)\n",
                "num_imputer = SimpleImputer(strategy='median')\n",
                "numerical_cols = ['area_sqft', 'age_years']\n",
                "df_clean[numerical_cols] = num_imputer.fit_transform(df_clean[numerical_cols])\n",
                "\n",
                "# Strategy 2: Impute categorical columns with the mode (most frequent value)\n",
                "cat_imputer = SimpleImputer(strategy='most_frequent')\n",
                "df_clean[['garage']] = cat_imputer.fit_transform(df_clean[['garage']])\n",
                "\n",
                "# Verify no missing values remain\n",
                "print(\"Missing values after imputation:\")\n",
                "print(df_clean.isnull().sum())\n",
                "print(f\"\\nTotal missing: {df_clean.isnull().sum().sum()}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Compare distributions before and after imputation\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "for idx, col in enumerate(numerical_cols):\n",
                "    ax = axes[idx]\n",
                "    ax.hist(df[col].dropna(), bins=25, alpha=0.6, color='#2196F3',\n",
                "            label='Before (with NaN dropped)', density=True, edgecolor='white')\n",
                "    ax.hist(df_clean[col], bins=25, alpha=0.6, color='#FF5722',\n",
                "            label='After imputation', density=True, edgecolor='white')\n",
                "    ax.set_title(f'{col}: Before vs After Imputation', fontsize=13, fontweight='bold')\n",
                "    ax.legend(fontsize=10)\n",
                "    ax.set_xlabel(col)\n",
                "    ax.set_ylabel('Density')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "print(\"Median imputation preserves the overall distribution shape while filling in gaps.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 3. Encoding Categorical Variables\n",
                "\n",
                "ML algorithms work with numbers, not strings. We need to convert categorical variables into numerical representations.\n",
                "\n",
                "### Three Common Encoding Methods\n",
                "\n",
                "| Method | Use Case | Example |\n",
                "|--------|---------|--------|\n",
                "| **Label Encoding** | Ordinal categories (natural order) | Poor=0, Fair=1, Good=2, Excellent=3 |\n",
                "| **One-Hot Encoding** | Nominal categories (no order) | Urban=[1,0,0], Suburban=[0,1,0], Rural=[0,0,1] |\n",
                "| **Ordinal Encoding** | Similar to Label, but with explicit order mapping | Same as Label, but for sklearn pipelines |"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.preprocessing import LabelEncoder, OrdinalEncoder\n",
                "\n",
                "df_encoded = df_clean.copy()\n",
                "\n",
                "# --- Label Encoding for binary variables (Yes/No) ---\n",
                "le = LabelEncoder()\n",
                "df_encoded['garage_encoded'] = le.fit_transform(df_encoded['garage'])\n",
                "print(\"Label Encoding for 'garage':\")\n",
                "print(f\"  Classes: {list(le.classes_)}\")\n",
                "print(f\"  Mapping: {dict(zip(le.classes_, le.transform(le.classes_)))}\")\n",
                "print()\n",
                "\n",
                "# --- Ordinal Encoding for ordered categories ---\n",
                "condition_order = [['Poor', 'Fair', 'Good', 'Excellent']]\n",
                "oe = OrdinalEncoder(categories=condition_order)\n",
                "df_encoded['condition_encoded'] = oe.fit_transform(df_encoded[['condition']])\n",
                "print(\"Ordinal Encoding for 'condition':\")\n",
                "print(f\"  Order: Poor=0, Fair=1, Good=2, Excellent=3\")\n",
                "print()\n",
                "\n",
                "# --- One-Hot Encoding for nominal categories (no inherent order) ---\n",
                "location_dummies = pd.get_dummies(df_encoded['location'], prefix='location', dtype=int)\n",
                "df_encoded = pd.concat([df_encoded, location_dummies], axis=1)\n",
                "print(\"One-Hot Encoding for 'location':\")\n",
                "print(location_dummies.head())\n",
                "\n",
                "# Drop original categorical columns\n",
                "df_encoded = df_encoded.drop(columns=['garage', 'condition', 'location'])\n",
                "\n",
                "print(\"\\n--- Encoded DataFrame (first 5 rows) ---\")\n",
                "df_encoded.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 4. Feature Scaling\n",
                "\n",
                "Features with different scales can cause problems for many algorithms (e.g., KNN, SVM, gradient descent-based models). Scaling ensures all features contribute equally.\n",
                "\n",
                "### Common Scaling Methods\n",
                "\n",
                "| Method | Formula | Range | Best For |\n",
                "|--------|---------|-------|----------|\n",
                "| **StandardScaler** | (x - mean) / std | Centered at 0 | Most ML algorithms |\n",
                "| **MinMaxScaler** | (x - min) / (max - min) | [0, 1] | Neural networks, bounded algorithms |\n",
                "| **RobustScaler** | (x - median) / IQR | Varies | Data with outliers |"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
                "\n",
                "# Select numerical features for scaling\n",
                "features_to_scale = ['area_sqft', 'bedrooms', 'age_years']\n",
                "original_data = df_encoded[features_to_scale].copy()\n",
                "\n",
                "# Apply each scaler\n",
                "scalers = {\n",
                "    'Original (unscaled)': original_data.values,\n",
                "    'StandardScaler': StandardScaler().fit_transform(original_data),\n",
                "    'MinMaxScaler': MinMaxScaler().fit_transform(original_data),\n",
                "    'RobustScaler': RobustScaler().fit_transform(original_data),\n",
                "}\n",
                "\n",
                "# Compare statistics\n",
                "print(f\"{'':>20} {'area_sqft':>12} {'bedrooms':>12} {'age_years':>12}\")\n",
                "print(\"-\" * 58)\n",
                "for name, values in scalers.items():\n",
                "    means = values.mean(axis=0)\n",
                "    print(f\"{name:>20}: mean = [{means[0]:>8.2f}, {means[1]:>8.2f}, {means[2]:>8.2f}]\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize the effect of different scalers\n",
                "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
                "colors = ['#2196F3', '#FF5722', '#4CAF50']\n",
                "\n",
                "for idx, (name, values) in enumerate(scalers.items()):\n",
                "    ax = axes[idx // 2, idx % 2]\n",
                "    df_temp = pd.DataFrame(values, columns=features_to_scale)\n",
                "    for j, col in enumerate(features_to_scale):\n",
                "        ax.hist(df_temp[col], bins=25, alpha=0.6, color=colors[j],\n",
                "                label=col, edgecolor='white')\n",
                "    ax.set_title(name, fontsize=13, fontweight='bold')\n",
                "    ax.legend(fontsize=9)\n",
                "    ax.set_xlabel('Value')\n",
                "    ax.set_ylabel('Frequency')\n",
                "\n",
                "plt.suptitle('Effect of Different Scaling Methods', fontsize=15, fontweight='bold')\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(\"Observations:\")\n",
                "print(\"  - Original: features are on vastly different scales (area_sqft vs bedrooms).\")\n",
                "print(\"  - StandardScaler: centers each feature at mean=0, std=1.\")\n",
                "print(\"  - MinMaxScaler: compresses all features into the [0, 1] range.\")\n",
                "print(\"  - RobustScaler: uses median and IQR — less affected by outliers.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 5. Feature Selection\n",
                "\n",
                "Not all features are useful. Irrelevant or redundant features can hurt performance and slow training. Feature selection identifies the most informative features.\n",
                "\n",
                "### Common Approaches\n",
                "\n",
                "1. **Correlation analysis** — remove features that are highly correlated with each other\n",
                "2. **Variance threshold** — remove features with very low variance (near-constant)\n",
                "3. **Statistical tests** — select features based on their relationship with the target"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Correlation analysis\n",
                "fig, ax = plt.subplots(figsize=(10, 8))\n",
                "corr_matrix = df_encoded.corr()\n",
                "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))  # show only lower triangle\n",
                "sns.heatmap(corr_matrix, mask=mask, annot=True, fmt='.2f', cmap='RdYlBu_r',\n",
                "            center=0, square=True, linewidths=0.5, ax=ax,\n",
                "            cbar_kws={'label': 'Correlation'})\n",
                "ax.set_title('Feature Correlation Matrix', fontsize=14, fontweight='bold')\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "# Which features correlate most with the target (price)?\n",
                "print(\"\\nCorrelation with target variable (price):\")\n",
                "print(\"-\" * 40)\n",
                "target_corr = corr_matrix['price'].drop('price').abs().sort_values(ascending=False)\n",
                "for feat, corr in target_corr.items():\n",
                "    print(f\"  {feat:>25s}: {corr:.3f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Variance Threshold — remove near-constant features\n",
                "from sklearn.feature_selection import VarianceThreshold\n",
                "\n",
                "# Apply to numerical features only\n",
                "selector = VarianceThreshold(threshold=0.01)  # remove features with variance < 0.01\n",
                "numerical_features = df_encoded.select_dtypes(include=[np.number])\n",
                "\n",
                "print(\"Feature variances:\")\n",
                "for col in numerical_features.columns:\n",
                "    print(f\"  {col:>25s}: {numerical_features[col].var():.2f}\")\n",
                "\n",
                "print(\"\\nIn this dataset, all features have sufficient variance. In practice,\")\n",
                "print(\"you would remove features with near-zero variance as they carry no information.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 6. Train-Test Splitting\n",
                "\n",
                "Splitting the data correctly is critical for reliable model evaluation."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.model_selection import train_test_split\n",
                "\n",
                "# Prepare features and target\n",
                "X = df_encoded.drop(columns=['price'])\n",
                "y = df_encoded['price']\n",
                "\n",
                "# Standard 80/20 split\n",
                "X_train, X_test, y_train, y_test = train_test_split(\n",
                "    X, y, test_size=0.2, random_state=42\n",
                ")\n",
                "\n",
                "print(f\"Total samples:    {len(X)}\")\n",
                "print(f\"Training samples: {len(X_train)} ({len(X_train)/len(X):.0%})\")\n",
                "print(f\"Test samples:     {len(X_test)} ({len(X_test)/len(X):.0%})\")\n",
                "\n",
                "print(f\"\\nFeature columns ({X.shape[1]}): {list(X.columns)}\")\n",
                "print(f\"\\nTraining target statistics:\")\n",
                "print(f\"  Mean:  {y_train.mean():,.0f}\")\n",
                "print(f\"  Std:   {y_train.std():,.0f}\")\n",
                "print(f\"\\nTest target statistics:\")\n",
                "print(f\"  Mean:  {y_test.mean():,.0f}\")\n",
                "print(f\"  Std:   {y_test.std():,.0f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 7. End-to-End Preprocessing Pipeline\n",
                "\n",
                "Scikit-learn provides `Pipeline` and `ColumnTransformer` to chain preprocessing steps together. This is best practice because it:\n",
                "- Prevents data leakage (scaler is fitted only on training data)\n",
                "- Makes the workflow reproducible\n",
                "- Simplifies deployment"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.pipeline import Pipeline\n",
                "from sklearn.compose import ColumnTransformer\n",
                "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
                "from sklearn.impute import SimpleImputer\n",
                "\n",
                "# Start from the original (dirty) data\n",
                "X_raw = df.drop(columns=['price'])\n",
                "y_raw = df['price']\n",
                "\n",
                "# Define column groups\n",
                "numeric_features = ['area_sqft', 'bedrooms', 'age_years']\n",
                "categorical_features = ['location', 'garage', 'condition']\n",
                "\n",
                "# Numeric pipeline: impute missing values with median, then standardize\n",
                "numeric_pipeline = Pipeline(steps=[\n",
                "    ('imputer', SimpleImputer(strategy='median')),\n",
                "    ('scaler', StandardScaler())\n",
                "])\n",
                "\n",
                "# Categorical pipeline: impute missing with mode, then one-hot encode\n",
                "categorical_pipeline = Pipeline(steps=[\n",
                "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
                "    ('encoder', OneHotEncoder(drop='first', sparse_output=False))  # drop='first' to avoid multicollinearity\n",
                "])\n",
                "\n",
                "# Combine into a single ColumnTransformer\n",
                "preprocessor = ColumnTransformer(\n",
                "    transformers=[\n",
                "        ('num', numeric_pipeline, numeric_features),\n",
                "        ('cat', categorical_pipeline, categorical_features)\n",
                "    ]\n",
                ")\n",
                "\n",
                "# Split data FIRST (before fitting the preprocessor)\n",
                "X_train_raw, X_test_raw, y_train_raw, y_test_raw = train_test_split(\n",
                "    X_raw, y_raw, test_size=0.2, random_state=42\n",
                ")\n",
                "\n",
                "# Fit on training data, transform both training and test\n",
                "X_train_processed = preprocessor.fit_transform(X_train_raw)\n",
                "X_test_processed = preprocessor.transform(X_test_raw)  # only transform, no fit!\n",
                "\n",
                "# Get feature names after transformation\n",
                "cat_feature_names = preprocessor.named_transformers_['cat']['encoder'].get_feature_names_out(categorical_features)\n",
                "all_feature_names = list(numeric_features) + list(cat_feature_names)\n",
                "\n",
                "print(\"Pipeline Steps:\")\n",
                "print(\"  1. Numerical: Median Imputation -> Standard Scaling\")\n",
                "print(\"  2. Categorical: Mode Imputation -> One-Hot Encoding\")\n",
                "print(f\"\\nOriginal features: {X_raw.shape[1]}\")\n",
                "print(f\"Processed features: {X_train_processed.shape[1]}\")\n",
                "print(f\"Feature names: {all_feature_names}\")\n",
                "print(f\"\\nTraining set shape: {X_train_processed.shape}\")\n",
                "print(f\"Test set shape:     {X_test_processed.shape}\")\n",
                "\n",
                "# Quick sanity check: processed training data\n",
                "processed_df = pd.DataFrame(X_train_processed, columns=all_feature_names)\n",
                "print(\"\\n--- Processed Training Data (first 5 rows) ---\")\n",
                "processed_df.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Quick model to verify the pipeline works end-to-end\n",
                "from sklearn.linear_model import LinearRegression\n",
                "from sklearn.metrics import mean_squared_error, r2_score\n",
                "\n",
                "model = LinearRegression()\n",
                "model.fit(X_train_processed, y_train_raw)\n",
                "y_pred = model.predict(X_test_processed)\n",
                "\n",
                "rmse = np.sqrt(mean_squared_error(y_test_raw, y_pred))\n",
                "r2 = r2_score(y_test_raw, y_pred)\n",
                "\n",
                "print(\"Quick Linear Regression Test (to validate the pipeline):\")\n",
                "print(f\"  RMSE: {rmse:,.0f}\")\n",
                "print(f\"  R2:   {r2:.4f}\")\n",
                "print(\"\\nThe pipeline successfully preprocesses raw data for model consumption.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 8. Exercises\n",
                "\n",
                "### Exercise 1: Handle a Messy Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Exercise 1: Clean the following messy dataset\n",
                "\n",
                "np.random.seed(99)\n",
                "messy_data = pd.DataFrame({\n",
                "    'age': [25, np.nan, 30, 45, np.nan, 35, 28, np.nan, 50, 22],\n",
                "    'salary': [50000, 60000, np.nan, 80000, 70000, np.nan, 55000, 65000, np.nan, 45000],\n",
                "    'department': ['Sales', 'IT', 'IT', np.nan, 'HR', 'Sales', np.nan, 'IT', 'HR', 'Sales'],\n",
                "    'performance': ['Good', 'Excellent', 'Fair', 'Good', 'Poor', np.nan, 'Good', 'Excellent', 'Fair', 'Good']\n",
                "})\n",
                "\n",
                "print(\"Messy Dataset:\")\n",
                "print(messy_data)\n",
                "print(f\"\\nMissing values:\\n{messy_data.isnull().sum()}\")\n",
                "\n",
                "# TODO: \n",
                "# 1. Impute 'age' and 'salary' with the median\n",
                "# 2. Impute 'department' with the most frequent value\n",
                "# 3. Encode 'performance' with ordinal encoding (Poor=0, Fair=1, Good=2, Excellent=3)\n",
                "# 4. One-hot encode 'department'\n",
                "# 5. Print the cleaned dataframe\n",
                "\n",
                "# Your code here:\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Exercise 2: Compare Scaling Effects on KNN"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Exercise 2: Train a KNN classifier on the Iris dataset with and without scaling.\n",
                "# Compare the test accuracy.\n",
                "\n",
                "from sklearn.datasets import load_iris\n",
                "from sklearn.neighbors import KNeighborsClassifier\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "from sklearn.model_selection import train_test_split\n",
                "\n",
                "iris = load_iris()\n",
                "X, y = iris.data, iris.target\n",
                "\n",
                "# TODO:\n",
                "# 1. Split data into train/test (80/20)\n",
                "# 2. Train KNN (K=5) WITHOUT scaling and record accuracy\n",
                "# 3. Scale the features using StandardScaler (fit on train, transform both)\n",
                "# 4. Train KNN (K=5) WITH scaling and record accuracy\n",
                "# 5. Print both accuracies and comment on the difference\n",
                "\n",
                "# Your code here:\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Exercise 3: Build a Complete Preprocessing Pipeline"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Exercise 3: Using sklearn Pipeline and ColumnTransformer,\n",
                "# build a preprocessing pipeline for the Titanic-like dataset below.\n",
                "\n",
                "titanic_data = pd.DataFrame({\n",
                "    'pclass': [1, 3, 2, 1, 3, 2, 1, 3, 2, 3],\n",
                "    'sex': ['male', 'female', 'female', 'male', 'male', 'female', 'female', 'male', 'male', 'female'],\n",
                "    'age': [22, np.nan, 35, 45, np.nan, 28, 58, 19, np.nan, 30],\n",
                "    'fare': [7.25, 71.28, 8.05, 52.0, np.nan, 13.0, 26.55, 8.05, 11.5, np.nan],\n",
                "    'embarked': ['S', 'C', 'S', np.nan, 'S', 'Q', 'S', 'S', 'C', 'Q'],\n",
                "    'survived': [0, 1, 1, 1, 0, 1, 1, 0, 0, 1]\n",
                "})\n",
                "\n",
                "print(\"Titanic-like Dataset:\")\n",
                "print(titanic_data)\n",
                "\n",
                "# TODO:\n",
                "# 1. Separate features (X) and target (y = 'survived')\n",
                "# 2. Identify numerical and categorical columns\n",
                "# 3. Build a ColumnTransformer with appropriate pipelines for each\n",
                "# 4. Fit and transform the data\n",
                "# 5. Print the processed feature matrix\n",
                "\n",
                "# Your code here:\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 9. Summary and Further Reading\n",
                "\n",
                "### What We Covered\n",
                "\n",
                "- **Data Inspection**: Always start by understanding shape, types, distributions, and missing values.\n",
                "- **Missing Values**: Impute using mean/median (numerical) or mode (categorical). Drop only when appropriate.\n",
                "- **Encoding**: Use label/ordinal encoding for ordered categories, one-hot encoding for nominal categories.\n",
                "- **Scaling**: StandardScaler (most common), MinMaxScaler (for bounded ranges), RobustScaler (for outlier-heavy data).\n",
                "- **Feature Selection**: Use correlation analysis and variance thresholds to identify relevance.\n",
                "- **Pipelines**: Use `Pipeline` and `ColumnTransformer` to build reproducible, leak-free preprocessing workflows.\n",
                "\n",
                "### Recommended Reading\n",
                "\n",
                "- [Scikit-learn Preprocessing Guide](https://scikit-learn.org/stable/modules/preprocessing.html)\n",
                "- [Scikit-learn Pipelines](https://scikit-learn.org/stable/modules/compose.html)\n",
                "- Chapter 2 of Aurélien Géron, *Hands-On Machine Learning* (end-to-end ML project with full preprocessing)\n",
                "\n",
                "### Next Module\n",
                "\n",
                "In **Module 4: Supervised Learning — Regression**, we will apply these preprocessing techniques to build regression models, covering linear regression, polynomial regression, regularization, and more.\n",
                "\n",
                "---"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.9.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}