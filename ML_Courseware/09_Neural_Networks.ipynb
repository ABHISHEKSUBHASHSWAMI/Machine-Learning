{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Module 9: Neural Networks and Deep Learning Introduction\n",
                "\n",
                "---\n",
                "\n",
                "Neural networks are the foundation of deep learning. This module builds your understanding from the ground up — starting with a single perceptron, progressing to multi-layer networks, and concluding with practical implementations using Keras/TensorFlow.\n",
                "\n",
                "---\n",
                "\n",
                "## Table of Contents\n",
                "\n",
                "1. [The Perceptron](#1.-The-Perceptron)\n",
                "2. [Activation Functions](#2.-Activation-Functions)\n",
                "3. [Multi-Layer Perceptron (MLP) from Scratch](#3.-Multi-Layer-Perceptron-from-Scratch)\n",
                "4. [Neural Networks with Scikit-learn](#4.-Neural-Networks-with-Scikit-learn)\n",
                "5. [Introduction to Keras/TensorFlow](#5.-Introduction-to-Keras/TensorFlow)\n",
                "6. [Convolutional Neural Networks — Overview](#6.-Convolutional-Neural-Networks)\n",
                "7. [Exercises](#7.-Exercises)\n",
                "8. [Summary and Further Reading](#8.-Summary-and-Further-Reading)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "from sklearn.datasets import load_breast_cancer, load_digits, make_moons\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "from sklearn.metrics import accuracy_score, classification_report\n",
                "\n",
                "plt.style.use('seaborn-v0_8-whitegrid')\n",
                "np.random.seed(42)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 1. The Perceptron\n",
                "\n",
                "A perceptron is the simplest neural network — a single neuron. It takes weighted inputs, sums them, adds a bias, and passes the result through an activation function.\n",
                "\n",
                "$$z = w_1 x_1 + w_2 x_2 + \\ldots + w_n x_n + b$$\n",
                "$$\\hat{y} = \\text{activation}(z)$$"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Implement a perceptron from scratch\n",
                "class Perceptron:\n",
                "    \"\"\"A simple perceptron classifier.\"\"\"\n",
                "    \n",
                "    def __init__(self, learning_rate=0.01, n_iterations=100):\n",
                "        self.lr = learning_rate\n",
                "        self.n_iter = n_iterations\n",
                "        self.weights = None\n",
                "        self.bias = None\n",
                "        self.errors = []\n",
                "    \n",
                "    def fit(self, X, y):\n",
                "        n_features = X.shape[1]\n",
                "        self.weights = np.zeros(n_features)\n",
                "        self.bias = 0.0\n",
                "        \n",
                "        for _ in range(self.n_iter):\n",
                "            errors = 0\n",
                "            for xi, yi in zip(X, y):\n",
                "                prediction = self.predict_single(xi)\n",
                "                update = self.lr * (yi - prediction)\n",
                "                self.weights += update * xi\n",
                "                self.bias += update\n",
                "                errors += int(update != 0.0)\n",
                "            self.errors.append(errors)\n",
                "        return self\n",
                "    \n",
                "    def predict_single(self, x):\n",
                "        return 1 if np.dot(x, self.weights) + self.bias >= 0 else 0\n",
                "    \n",
                "    def predict(self, X):\n",
                "        return np.array([self.predict_single(xi) for xi in X])\n",
                "\n",
                "# Test on linearly separable data\n",
                "from sklearn.datasets import make_classification\n",
                "X_lin, y_lin = make_classification(n_samples=200, n_features=2, n_informative=2,\n",
                "                                    n_redundant=0, n_clusters_per_class=1, random_state=42)\n",
                "\n",
                "ppn = Perceptron(learning_rate=0.1, n_iterations=50)\n",
                "ppn.fit(X_lin, y_lin)\n",
                "\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "# Convergence\n",
                "axes[0].plot(ppn.errors, 'o-', color='#FF5722', linewidth=2, markersize=4)\n",
                "axes[0].set_xlabel('Epoch', fontsize=13)\n",
                "axes[0].set_ylabel('Number of Misclassifications', fontsize=13)\n",
                "axes[0].set_title('Perceptron Convergence', fontsize=14, fontweight='bold')\n",
                "\n",
                "# Decision boundary\n",
                "from matplotlib.colors import ListedColormap\n",
                "h = 0.02\n",
                "x_min, x_max = X_lin[:, 0].min() - 0.5, X_lin[:, 0].max() + 0.5\n",
                "y_min, y_max = X_lin[:, 1].min() - 0.5, X_lin[:, 1].max() + 0.5\n",
                "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
                "Z = ppn.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
                "axes[1].contourf(xx, yy, Z, alpha=0.3, cmap=ListedColormap(['#BBDEFB', '#FFCCBC']))\n",
                "axes[1].scatter(X_lin[:, 0], X_lin[:, 1], c=y_lin, cmap=ListedColormap(['#1565C0', '#E64A19']),\n",
                "                s=30, edgecolors='white', linewidth=0.5)\n",
                "axes[1].set_title('Perceptron Decision Boundary', fontsize=14, fontweight='bold')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(f\"Perceptron accuracy: {accuracy_score(y_lin, ppn.predict(X_lin)):.4f}\")\n",
                "print(\"The perceptron converges when data is linearly separable.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 2. Activation Functions\n",
                "\n",
                "Activation functions introduce non-linearity, allowing neural networks to learn complex patterns."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Common activation functions\n",
                "z = np.linspace(-5, 5, 200)\n",
                "\n",
                "activations = {\n",
                "    'Sigmoid': (1 / (1 + np.exp(-z)), 'Output range: (0, 1). Used in output layer for binary classification.'),\n",
                "    'Tanh': (np.tanh(z), 'Output range: (-1, 1). Zero-centered, often better than sigmoid for hidden layers.'),\n",
                "    'ReLU': (np.maximum(0, z), 'Output range: [0, inf). Most popular for hidden layers. Fast, avoids vanishing gradient.'),\n",
                "    'Leaky ReLU': (np.where(z > 0, z, 0.01 * z), 'Fixes the \"dying ReLU\" problem by allowing small gradients for negative inputs.'),\n",
                "}\n",
                "\n",
                "fig, axes = plt.subplots(1, 4, figsize=(20, 4))\n",
                "colors = ['#2196F3', '#FF5722', '#4CAF50', '#9C27B0']\n",
                "\n",
                "for idx, (name, (values, desc)) in enumerate(activations.items()):\n",
                "    ax = axes[idx]\n",
                "    ax.plot(z, values, linewidth=2.5, color=colors[idx])\n",
                "    ax.axhline(y=0, color='gray', linewidth=0.5, linestyle='--')\n",
                "    ax.axvline(x=0, color='gray', linewidth=0.5, linestyle='--')\n",
                "    ax.set_title(name, fontsize=13, fontweight='bold')\n",
                "    ax.set_xlabel('z')\n",
                "    ax.set_ylabel('f(z)')\n",
                "\n",
                "plt.suptitle('Common Activation Functions', fontsize=15, fontweight='bold')\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "for name, (_, desc) in activations.items():\n",
                "    print(f\"{name}: {desc}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 3. Multi-Layer Perceptron (MLP) from Scratch\n",
                "\n",
                "Here we implement a simple 2-layer neural network from scratch to understand forward propagation, loss computation, and backpropagation."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Simple 2-layer neural network from scratch\n",
                "class SimpleNeuralNetwork:\n",
                "    \"\"\"A 2-layer neural network: input -> hidden (sigmoid) -> output (sigmoid).\"\"\"\n",
                "    \n",
                "    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.1):\n",
                "        self.lr = learning_rate\n",
                "        # Initialize weights with small random values\n",
                "        self.W1 = np.random.randn(input_size, hidden_size) * 0.5\n",
                "        self.b1 = np.zeros((1, hidden_size))\n",
                "        self.W2 = np.random.randn(hidden_size, output_size) * 0.5\n",
                "        self.b2 = np.zeros((1, output_size))\n",
                "        self.losses = []\n",
                "    \n",
                "    def sigmoid(self, z):\n",
                "        return 1 / (1 + np.exp(-np.clip(z, -500, 500)))\n",
                "    \n",
                "    def sigmoid_derivative(self, a):\n",
                "        return a * (1 - a)\n",
                "    \n",
                "    def forward(self, X):\n",
                "        self.z1 = X @ self.W1 + self.b1\n",
                "        self.a1 = self.sigmoid(self.z1)  # hidden layer output\n",
                "        self.z2 = self.a1 @ self.W2 + self.b2\n",
                "        self.a2 = self.sigmoid(self.z2)  # final output\n",
                "        return self.a2\n",
                "    \n",
                "    def backward(self, X, y):\n",
                "        m = X.shape[0]\n",
                "        \n",
                "        # Output layer error\n",
                "        dz2 = self.a2 - y.reshape(-1, 1)  # derivative of BCE loss * sigmoid\n",
                "        dW2 = (self.a1.T @ dz2) / m\n",
                "        db2 = np.sum(dz2, axis=0, keepdims=True) / m\n",
                "        \n",
                "        # Hidden layer error\n",
                "        dz1 = (dz2 @ self.W2.T) * self.sigmoid_derivative(self.a1)\n",
                "        dW1 = (X.T @ dz1) / m\n",
                "        db1 = np.sum(dz1, axis=0, keepdims=True) / m\n",
                "        \n",
                "        # Update weights\n",
                "        self.W2 -= self.lr * dW2\n",
                "        self.b2 -= self.lr * db2\n",
                "        self.W1 -= self.lr * dW1\n",
                "        self.b1 -= self.lr * db1\n",
                "    \n",
                "    def compute_loss(self, y):\n",
                "        m = y.shape[0]\n",
                "        y = y.reshape(-1, 1)\n",
                "        return -np.mean(y * np.log(self.a2 + 1e-8) + (1 - y) * np.log(1 - self.a2 + 1e-8))\n",
                "    \n",
                "    def fit(self, X, y, epochs=500):\n",
                "        for epoch in range(epochs):\n",
                "            self.forward(X)\n",
                "            loss = self.compute_loss(y)\n",
                "            self.losses.append(loss)\n",
                "            self.backward(X, y)\n",
                "        return self\n",
                "    \n",
                "    def predict(self, X):\n",
                "        return (self.forward(X) >= 0.5).astype(int).ravel()\n",
                "\n",
                "# Train on Moons data\n",
                "X_moons, y_moons = make_moons(n_samples=300, noise=0.2, random_state=42)\n",
                "X_train_m, X_test_m, y_train_m, y_test_m = train_test_split(X_moons, y_moons, test_size=0.2, random_state=42)\n",
                "\n",
                "nn = SimpleNeuralNetwork(input_size=2, hidden_size=16, output_size=1, learning_rate=1.0)\n",
                "nn.fit(X_train_m, y_train_m, epochs=1000)\n",
                "\n",
                "print(f\"Custom Neural Network — Moons Dataset\")\n",
                "print(f\"  Train accuracy: {accuracy_score(y_train_m, nn.predict(X_train_m)):.4f}\")\n",
                "print(f\"  Test accuracy:  {accuracy_score(y_test_m, nn.predict(X_test_m)):.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize loss curve and decision boundary\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "# Loss curve\n",
                "axes[0].plot(nn.losses, linewidth=2, color='#FF5722')\n",
                "axes[0].set_xlabel('Epoch', fontsize=13)\n",
                "axes[0].set_ylabel('Binary Cross-Entropy Loss', fontsize=13)\n",
                "axes[0].set_title('Training Loss Convergence', fontsize=14, fontweight='bold')\n",
                "\n",
                "# Decision boundary\n",
                "h = 0.02\n",
                "x_min, x_max = X_moons[:, 0].min() - 0.5, X_moons[:, 0].max() + 0.5\n",
                "y_min, y_max = X_moons[:, 1].min() - 0.5, X_moons[:, 1].max() + 0.5\n",
                "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
                "Z = nn.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
                "axes[1].contourf(xx, yy, Z, alpha=0.3, cmap=ListedColormap(['#BBDEFB', '#FFCCBC']))\n",
                "axes[1].scatter(X_moons[:, 0], X_moons[:, 1], c=y_moons,\n",
                "                cmap=ListedColormap(['#1565C0', '#E64A19']), s=30, edgecolors='white', linewidth=0.5)\n",
                "axes[1].set_title('Neural Network Decision Boundary', fontsize=14, fontweight='bold')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(\"Unlike the perceptron, the MLP can learn non-linear decision boundaries.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 4. Neural Networks with Scikit-learn\n",
                "\n",
                "Scikit-learn provides `MLPClassifier` and `MLPRegressor` for quick neural network experiments without the need for deep learning frameworks."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.neural_network import MLPClassifier\n",
                "\n",
                "# Compare different architectures on Breast Cancer dataset\n",
                "cancer = load_breast_cancer()\n",
                "X_train_c, X_test_c, y_train_c, y_test_c = train_test_split(\n",
                "    cancer.data, cancer.target, test_size=0.2, random_state=42, stratify=cancer.target\n",
                ")\n",
                "scaler = StandardScaler()\n",
                "X_train_cs = scaler.fit_transform(X_train_c)\n",
                "X_test_cs = scaler.transform(X_test_c)\n",
                "\n",
                "architectures = {\n",
                "    '1 layer (50)': (50,),\n",
                "    '2 layers (50, 25)': (50, 25),\n",
                "    '3 layers (100, 50, 25)': (100, 50, 25),\n",
                "}\n",
                "\n",
                "print(\"Scikit-learn MLPClassifier — Architecture Comparison\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "mlp_results = []\n",
                "for name, arch in architectures.items():\n",
                "    mlp = MLPClassifier(hidden_layer_sizes=arch, max_iter=1000,\n",
                "                        random_state=42, early_stopping=True, validation_fraction=0.1)\n",
                "    mlp.fit(X_train_cs, y_train_c)\n",
                "    train_acc = mlp.score(X_train_cs, y_train_c)\n",
                "    test_acc = mlp.score(X_test_cs, y_test_c)\n",
                "    mlp_results.append({'Architecture': name, 'Train': train_acc, 'Test': test_acc,\n",
                "                        'Iterations': mlp.n_iter_})\n",
                "    print(f\"  {name:>25s}: Train={train_acc:.4f}  Test={test_acc:.4f}  Epochs={mlp.n_iter_}\")\n",
                "\n",
                "# Plot loss curve for the best architecture\n",
                "mlp_best = MLPClassifier(hidden_layer_sizes=(100, 50, 25), max_iter=1000,\n",
                "                          random_state=42)\n",
                "mlp_best.fit(X_train_cs, y_train_c)\n",
                "\n",
                "fig, ax = plt.subplots(figsize=(10, 5))\n",
                "ax.plot(mlp_best.loss_curve_, linewidth=2, color='#2196F3')\n",
                "ax.set_xlabel('Iteration', fontsize=13)\n",
                "ax.set_ylabel('Loss', fontsize=13)\n",
                "ax.set_title('MLP Training Loss Curve', fontsize=14, fontweight='bold')\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 5. Introduction to Keras/TensorFlow\n",
                "\n",
                "For production-grade deep learning, we use frameworks like **TensorFlow** with the **Keras** API. Keras provides a clean, intuitive interface for building neural networks.\n",
                "\n",
                "Note: TensorFlow must be installed (`pip install tensorflow`). If not available, this section will demonstrate the concepts with code that can be run once installed."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "try:\n",
                "    import tensorflow as tf\n",
                "    from tensorflow import keras\n",
                "    from tensorflow.keras import layers\n",
                "    \n",
                "    print(f\"TensorFlow version: {tf.__version__}\")\n",
                "    \n",
                "    # Build a neural network with Keras Sequential API\n",
                "    model = keras.Sequential([\n",
                "        layers.Dense(64, activation='relu', input_shape=(X_train_cs.shape[1],)),\n",
                "        layers.Dropout(0.3),\n",
                "        layers.Dense(32, activation='relu'),\n",
                "        layers.Dropout(0.2),\n",
                "        layers.Dense(1, activation='sigmoid')\n",
                "    ])\n",
                "    \n",
                "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
                "    model.summary()\n",
                "    \n",
                "    # Train the model\n",
                "    history = model.fit(\n",
                "        X_train_cs, y_train_c,\n",
                "        epochs=100,\n",
                "        batch_size=32,\n",
                "        validation_split=0.2,\n",
                "        verbose=0\n",
                "    )\n",
                "    \n",
                "    # Evaluate\n",
                "    test_loss, test_acc = model.evaluate(X_test_cs, y_test_c, verbose=0)\n",
                "    print(f\"\\nKeras Model Test Accuracy: {test_acc:.4f}\")\n",
                "    \n",
                "    # Plot training history\n",
                "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "    \n",
                "    axes[0].plot(history.history['loss'], label='Training Loss', color='#2196F3', linewidth=2)\n",
                "    axes[0].plot(history.history['val_loss'], label='Validation Loss', color='#FF5722', linewidth=2)\n",
                "    axes[0].set_xlabel('Epoch', fontsize=13)\n",
                "    axes[0].set_ylabel('Loss', fontsize=13)\n",
                "    axes[0].set_title('Loss Over Training', fontsize=14, fontweight='bold')\n",
                "    axes[0].legend(fontsize=11)\n",
                "    \n",
                "    axes[1].plot(history.history['accuracy'], label='Training Accuracy', color='#2196F3', linewidth=2)\n",
                "    axes[1].plot(history.history['val_accuracy'], label='Validation Accuracy', color='#FF5722', linewidth=2)\n",
                "    axes[1].set_xlabel('Epoch', fontsize=13)\n",
                "    axes[1].set_ylabel('Accuracy', fontsize=13)\n",
                "    axes[1].set_title('Accuracy Over Training', fontsize=14, fontweight='bold')\n",
                "    axes[1].legend(fontsize=11)\n",
                "    \n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "\n",
                "except ImportError:\n",
                "    print(\"TensorFlow is not installed. To install, run: pip install tensorflow\")\n",
                "    print(\"\\nBelow is the code that would be used (for reference):\")\n",
                "    print(\"\"\"\n",
                "    import tensorflow as tf\n",
                "    from tensorflow.keras import layers\n",
                "    \n",
                "    model = tf.keras.Sequential([\n",
                "        layers.Dense(64, activation='relu', input_shape=(n_features,)),\n",
                "        layers.Dropout(0.3),\n",
                "        layers.Dense(32, activation='relu'),\n",
                "        layers.Dropout(0.2),\n",
                "        layers.Dense(1, activation='sigmoid')\n",
                "    ])\n",
                "    \n",
                "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
                "    history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2)\n",
                "    \"\"\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 6. Convolutional Neural Networks — Overview\n",
                "\n",
                "**CNNs** are specialized neural networks designed for image data. They use convolutional layers that apply learned filters to detect features like edges, textures, and patterns.\n",
                "\n",
                "Key concepts:\n",
                "- **Convolutional Layer**: Applies filters that slide across the input to detect local features\n",
                "- **Pooling Layer**: Reduces spatial dimensions (e.g., max pooling)\n",
                "- **Flatten**: Converts 2D feature maps into a 1D vector for the dense layers"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize the digits dataset as images\n",
                "digits = load_digits()\n",
                "\n",
                "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
                "for i, ax in enumerate(axes.flat):\n",
                "    ax.imshow(digits.images[i], cmap='gray_r')\n",
                "    ax.set_title(f'Label: {digits.target[i]}', fontsize=11)\n",
                "    ax.axis('off')\n",
                "\n",
                "plt.suptitle('Handwritten Digits (8x8 pixels)', fontsize=14, fontweight='bold')\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(f\"Each digit image is {digits.images[0].shape} pixels.\")\n",
                "print(f\"When flattened, each image becomes a vector of {digits.data.shape[1]} features.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# CNN with Keras on Digits (if TensorFlow is available)\n",
                "try:\n",
                "    import tensorflow as tf\n",
                "    from tensorflow.keras import layers\n",
                "    \n",
                "    # Prepare data (reshape for CNN: samples, height, width, channels)\n",
                "    X_digits = digits.images.reshape(-1, 8, 8, 1) / 16.0  # normalize\n",
                "    y_digits = digits.target\n",
                "    \n",
                "    X_train_d, X_test_d, y_train_d, y_test_d = train_test_split(\n",
                "        X_digits, y_digits, test_size=0.2, random_state=42, stratify=y_digits\n",
                "    )\n",
                "    \n",
                "    cnn = tf.keras.Sequential([\n",
                "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=(8, 8, 1), padding='same'),\n",
                "        layers.MaxPooling2D((2, 2)),\n",
                "        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
                "        layers.Flatten(),\n",
                "        layers.Dense(64, activation='relu'),\n",
                "        layers.Dropout(0.3),\n",
                "        layers.Dense(10, activation='softmax')  # 10 classes\n",
                "    ])\n",
                "    \n",
                "    cnn.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
                "    \n",
                "    history_cnn = cnn.fit(X_train_d, y_train_d, epochs=30, batch_size=32,\n",
                "                          validation_split=0.15, verbose=0)\n",
                "    \n",
                "    test_loss_cnn, test_acc_cnn = cnn.evaluate(X_test_d, y_test_d, verbose=0)\n",
                "    print(f\"CNN Test Accuracy on Digits: {test_acc_cnn:.4f}\")\n",
                "    \n",
                "    # Plot training history\n",
                "    fig, ax = plt.subplots(figsize=(10, 5))\n",
                "    ax.plot(history_cnn.history['accuracy'], label='Train', color='#2196F3', linewidth=2)\n",
                "    ax.plot(history_cnn.history['val_accuracy'], label='Validation', color='#FF5722', linewidth=2)\n",
                "    ax.set_xlabel('Epoch', fontsize=13)\n",
                "    ax.set_ylabel('Accuracy', fontsize=13)\n",
                "    ax.set_title('CNN Training History (Digits)', fontsize=14, fontweight='bold')\n",
                "    ax.legend(fontsize=11)\n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "\n",
                "except ImportError:\n",
                "    print(\"TensorFlow is not installed. The CNN example requires TensorFlow.\")\n",
                "    print(\"Install with: pip install tensorflow\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 7. Exercises\n",
                "\n",
                "### Exercise 1: Hidden Layer Size Experiment"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Exercise 1: Using MLPClassifier on the Breast Cancer dataset:\n",
                "# 1. Try hidden_layer_sizes: (10,), (50,), (100,), (50, 25), (100, 50, 25)\n",
                "# 2. Record training and test accuracy for each\n",
                "# 3. Plot a bar chart comparing architectures\n",
                "# 4. Does adding more layers always help? Why or why not?\n",
                "\n",
                "# Your code here:\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Exercise 2: XOR Problem"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Exercise 2: The XOR problem cannot be solved by a single perceptron.\n",
                "# Show that a neural network can solve it:\n",
                "# \n",
                "# XOR truth table:\n",
                "# [0,0] -> 0, [0,1] -> 1, [1,0] -> 1, [1,1] -> 0\n",
                "#\n",
                "# 1. Create the XOR dataset\n",
                "# 2. Try to solve it with a perceptron (show it fails)\n",
                "# 3. Solve it with an MLPClassifier with 1 hidden layer\n",
                "# 4. Plot the decision boundary\n",
                "\n",
                "X_xor = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
                "y_xor = np.array([0, 1, 1, 0])\n",
                "\n",
                "# Your code here:\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Exercise 3: Digit Classification"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Exercise 3: Using the Digits dataset:\n",
                "# 1. Scale the data\n",
                "# 2. Train an MLPClassifier with architecture (128, 64, 32)\n",
                "# 3. Print accuracy and classification report\n",
                "# 4. Plot some misclassified digits with their true and predicted labels\n",
                "\n",
                "# Your code here:\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 8. Summary and Further Reading\n",
                "\n",
                "### What We Covered\n",
                "\n",
                "- **Perceptron**: The simplest neuron — linear classifier with step activation.\n",
                "- **Activation Functions**: Sigmoid, Tanh, ReLU, Leaky ReLU — what they do and when to use them.\n",
                "- **MLP from Scratch**: Forward propagation, backpropagation, and gradient descent.\n",
                "- **Scikit-learn MLPClassifier**: Quick neural network prototyping.\n",
                "- **Keras/TensorFlow**: Building, training, and evaluating deep learning models.\n",
                "- **CNNs**: Convolutional layers for image data.\n",
                "\n",
                "### Recommended Reading\n",
                "\n",
                "- [Keras Documentation](https://keras.io/)\n",
                "- [TensorFlow Tutorials](https://www.tensorflow.org/tutorials)\n",
                "- Chapter 10-14 of Aurélien Géron, *Hands-On Machine Learning* (Neural Networks)\n",
                "- Michael Nielsen, *Neural Networks and Deep Learning* (free online book)\n",
                "\n",
                "### Next Module\n",
                "\n",
                "In **Module 10: Advanced Topics and Best Practices**, we will cover practical deployment considerations, handling imbalanced data, and working with real-world ML pipelines.\n",
                "\n",
                "---"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.9.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}